## æµ‹è¯•å·¥å…·å®‰è£…

1. å®‰è£…kubectl

   ```shell 
   # ç¬¬ä¸€æ­¥
   curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
   # ç¬¬äºŒæ­¥
   sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
   # éªŒè¯ç‰ˆæœ¬å·
   kubectl version --client
   ```

2. å®‰è£…k3d

   å‚æ•° `https://k3d.io/stable/#releases`
   
   å®‰è£…çš„k3dé»˜è®¤åˆ›å»ºçš„é›†ç¾¤çš„å¯¹åº”çš„ç‰ˆæœ¬ä¸€èˆ¬ä¸æ˜¯æœ€æ–°çš„å¯¹åº”çš„k8s APIçš„ç‰ˆæœ¬ã€‚å¯ä»¥ç”¨é…ç½®æ–‡ä»¶æ¥æŒ‡å®šæœ€æ–°çš„k3sé•œåƒï¼Œæ¥å¯¹åº”k8sç‰ˆæœ¬ã€‚
   
   ```yaml
   # k3d å¯åŠ¨çš„é…ç½®æ–‡ä»¶, å®šä¹‰ä½¿ç”¨æœ€æ–°çš„k3sç‰ˆæœ¬[k8sç‰ˆæœ¬]
   apiVersion: k3d.io/v1alpha5 # ä½¿ç”¨æœ€æ–°çš„APIç‰ˆæœ¬ä»¥èŽ·å¾—æ‰€æœ‰åŠŸèƒ½
   kind: Simple
   metadata:
     name: my-cluster # å¯ä»¥å®šä¹‰é›†ç¾¤åç§°ï¼Œä¹Ÿå¯ä»¥ä¸æŒ‡å®šï¼Œåœ¨k3d create clusteræ—¶æŒ‡å®š
   image: rancher/k3s:latest # åœ¨è¿™é‡ŒæŒ‡å®šä½ æƒ³è¦çš„â€œé»˜è®¤â€K3sé•œåƒç‰ˆæœ¬
   ```
   
   ```shell
   ## ä»¥ä¸‹è¾“å…¥æ˜¯é™æ€çš„ï¼Œæ˜¯è·Ÿç€k3dçš„ç‰ˆæœ¬èµ°çš„
   leite@leite-company ~> k3d version
   k3d version v5.8.3
   k3s version v1.31.5-k3s1 (default)
   ## è¾“å‡ºä»¥ä¸‹è¯´æ˜Žå·²æ­£ç¡®åº”ç”¨äº†æœ€æ–°çš„k3sç‰ˆæœ¬
   leite@leite-company ~ [1]> kubectl version
   Client Version: v1.34.1
   Kustomize Version: v5.7.1
   Server Version: v1.34.1+k3s1
   ```
   
   

## Kubernetes Components

The components of a Kubernetes cluster:

![image-20250815091851689](./images/image-20250815091851689.png)

## Pod

* Pods are the smallest deployable units of computing that you can create and manage in Kubernetes

* You don't need to run multiple containers to provide replication (for resilience or capacity)

* Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an
  environment for running container(s). A Pod persists until it is deleted.

* Modifying the pod template or switching to a new pod template has no direct effect on the Pods that already exist.
  > åªä¼šæ ¹æ®æ–°çš„æ›´æ–°çš„æ¨¡æ¿é‡æ–°åˆ›å»ºä¸€ä¸ªpod

* The name of a Pod must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostname.
  For best compatibility, the name should follow the more restrictive rules for a DNS label
  
* `kubectl describe pod` å‘½ä»¤ã€‚å®ƒä¼šå‘Šè¯‰ä½  Pod å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿçš„è¯¦ç»†äº‹ä»¶è®°å½•ã€‚

* æ€Žä¹ˆçŸ¥é“æˆ‘çš„podæœ‰æ²¡æœ‰å¯åŠ¨æˆåŠŸï¼Ÿ

  > 1. å®è§‚æ£€æŸ¥ï¼š`kubectl get pods` è¿˜å¯ä»¥å¢žåŠ `--watch` è¿˜å®žæ—¶è§‚å¯Ÿ
  > 2. è¯¦ç»†è¯Šæ–­ï¼š`kubectl describe pod [pod-name]`
  > 3. æ·±å…¥åº”ç”¨å†…éƒ¨ï¼š`kubectl logs [pod-name]` ã€å¯ä»¥ç”¨--previousé€‰æ‹©æ¥æŸ¥çœ‹ä¸Šä¸€æ¬¡çš„æ—¥å¿—ï¼Œè¿˜å¯ä»¥ç”¨-fã€‘


* ç¡®è®¤ Pod å†…çš„åº”ç”¨æ˜¯å¦çœŸçš„æ­£å¸¸å·¥ä½œ

  > è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå®ƒä¼šåœ¨ä½ çš„æœ¬åœ° `8084` ç«¯å£å’Œ Deployment ä¸­çš„ä¸€ä¸ª Pod çš„ `8084` ç«¯å£ä¹‹é—´å»ºç«‹ä¸€ä¸ªä¸´æ—¶çš„ã€ç›´æŽ¥çš„é€šé“ï¼š
  >
  > ```shell
  > kubectl port-forward deployment/springboot3-deployment 8084:8084
  > ```
  >
  > > åœ¨ä½ çš„**æœ¬åœ°è®¡ç®—æœº**å’Œé›†ç¾¤å†…çš„ **Pod** ä¹‹é—´ï¼Œé€šè¿‡ Kubernetes API Server å»ºç«‹äº†ä¸€æ¡**ä¸´æ—¶çš„ã€åŠ å¯†çš„ã€ç‚¹å¯¹ç‚¹çš„é€šä¿¡éš§é“**ã€‚
  > >
  > > **æ²¡æœ‰è´Ÿè½½å‡è¡¡**ï¼š`port-forward` ä¸ä¼šåœ¨å¤šä¸ªå‰¯æœ¬ï¼ˆreplicasï¼‰ä¹‹é—´è½®è¯¢æˆ–åˆ†å‘æµé‡ã€‚æ‰€æœ‰è¯·æ±‚éƒ½ä¼šè¢«é€åˆ°åŒä¸€ä¸ª Pod å®žä¾‹ä¸Šã€‚
  > >
  > > **ä¼šè¯æ˜¯â€œç²˜æ€§â€çš„**ï¼šåœ¨ä½ æŒ‰ä¸‹ `Ctrl+C` ç»“æŸ `port-forward` å‘½ä»¤ä¹‹å‰ï¼Œè¿™ä¸ªéš§é“ä¼šä¸€ç›´è¿žæŽ¥åˆ°æœ€åˆé€‰å®šçš„é‚£ä¸ª Podã€‚
  > >
  > > **æ²¡æœ‰è‡ªåŠ¨æ•…éšœè½¬ç§»**ï¼šå¦‚æžœåœ¨ `port-forward` è¿è¡ŒæœŸé—´ï¼Œå®ƒè¿žæŽ¥çš„é‚£ä¸ª Pod æ°å¥½â€œåäº†â€å¹¶è¢« Kubernetes é‡å¯ï¼Œä½ çš„ `port-forward` è¿žæŽ¥ä¼š**ä¸­æ–­**ï¼Œå‘½ä»¤ä¼šæŠ¥é”™å¹¶é€€å‡ºã€‚
  >
  > è¿˜å¯ä»¥åœ¨åœ¨é›†ç¾¤å†…éƒ¨æµ‹è¯• ï¼š
  >
  > **å¯åŠ¨ä¸€ä¸ªä¸´æ—¶çš„æµ‹è¯• Pod**ï¼šæˆ‘ä»¬å¯ä»¥è¿è¡Œä¸€ä¸ªåŒ…å« `curl` ç­‰ç½‘ç»œå·¥å…·çš„ä¸´æ—¶ Podã€‚
  >
  > ```shell
  > # è¿è¡Œä¸€ä¸ªä¸´æ—¶çš„ busybox Podï¼Œå¹¶åœ¨ç»“æŸåŽè‡ªåŠ¨åˆ é™¤
  > kubectl run my-test-pod --image=busybox -it --rm -- sh
  > ## kubectl run my-debug-pod --image=curlimages/curl -i --tty --rm -- /bin/sh è¿™ä¸ªä¹Ÿå¯ä»¥
  > ```
  >
  > **åœ¨ä¸´æ—¶ Pod å†…é€šè¿‡ Service åç§°è®¿é—®**ï¼šKubernetes è‡ªå¸¦äº† DNS æœåŠ¡ï¼Œä½ å¯ä»¥ç›´æŽ¥é€šè¿‡ Service çš„åç§°æ¥è®¿é—®å®ƒã€‚
  >
  > ```shell
  > # å‡è®¾ä½ å·²ç»åœ¨ my-test-pod çš„ shell ä¸­
  > # è¯­æ³•: wget -qO- http://[service-name]:[service-port]
  > wget -qO- http://springboot3-service:80
  > ```
  >
  > å¦‚æžœè¿”å›žäº†åº”ç”¨çš„æ­£ç¡®å“åº”ï¼Œè¯´æ˜Ž Service çš„æœåŠ¡å‘çŽ°å’Œç«¯å£è½¬å‘éƒ½æ˜¯æ­£å¸¸çš„ã€‚

* ä»€ä¹ˆæ—¶å€™ä¼šå•ç‹¬å®šä¹‰å’Œä½¿ç”¨ Pod

  > **åœºæ™¯ç¤ºä¾‹**ï¼šä½ æƒ³æµ‹è¯•ä¸€ä¸‹é›†ç¾¤å†…éƒ¨çš„ç½‘ç»œæ˜¯å¦é€šç•…ï¼Œæˆ–è€…æƒ³çœ‹æŸä¸ª `Service` æ˜¯å¦èƒ½è¢«è®¿é—®åˆ°ã€‚
  >
  > **æ“ä½œ**ï¼šä½ å¯ä»¥å¿«é€Ÿåˆ›å»ºä¸€ä¸ªåŒ…å«ç½‘ç»œå·¥å…·ï¼ˆå¦‚ `curl`, `ping`, `dig`ï¼‰çš„ Podï¼Œç„¶åŽé€šè¿‡ `kubectl exec` è¿›å…¥è¿™ä¸ª Pod è¿›è¡Œè°ƒè¯•ã€‚è°ƒè¯•ç»“æŸåŽï¼Œç›´æŽ¥åˆ é™¤è¿™ä¸ª Pod å³å¯ï¼Œä¸ç•™ä»»ä½•ç—•è¿¹ã€‚
  >
  > **ç¤ºä¾‹ YAML (**`debug-pod.yaml`**)ï¼š**
  >
  > ```yaml
  > apiVersion: v1
  > kind: Pod
  > metadata:
  > name: curl-pod
  > spec:
  >  containers:
  >       - name: my-curl # æˆ‘ä»¬ç”¨ä¸€ä¸ªåŒ…å« curl çš„é•œåƒï¼Œå¹¶è®©å®ƒä¸€ç›´è¿è¡Œï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½ exec è¿›åŽ»
  >         image: curlimages/curl:latest
  >     command: ["sleep", "3600"] # è®©å®¹å™¨ä¿æŒè¿è¡Œï¼Œå¦åˆ™å®ƒä¼šç«‹å³é€€å‡º
  > ```
  >
  > **ä½¿ç”¨å‘½ä»¤**:
  >
  >   ```shell
  >   # åˆ›å»º Pod
  >   kubectl apply -f debug-pod.yaml
  > 
  >   # è¿›å…¥ Pod å†…éƒ¨æ‰§è¡Œå‘½ä»¤
  >   kubectl exec -it curl-pod -- sh
  > 
  >   # (åœ¨ Pod å†…éƒ¨)
  >   # curl [your-service-name].[namespace].svc.cluster.local
  >   # exit
  > 
  >   # è°ƒè¯•å®Œæ¯•åŽåˆ é™¤ Pod
  >   kubectl delete pod curl-pod 
  >   ```

* Pods that are part of a DaemonSet tolerate being run on an unschedulable Node. DaemonSets typically provide node-local
  services that should run on the Node even if it is being drained of workload applications.

  > åœ¨ Kubernetes ä¸­ï¼Œè¿™äº›â€œå¿…é¡»å®‰è£…åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šâ€çš„åŽå°æœåŠ¡ï¼Œå°±æ˜¯é€šè¿‡ DaemonSet æ¥éƒ¨ç½²çš„ã€‚å¸¸è§çš„ä¾‹å­æœ‰ï¼šæ—¥å¿—æ”¶é›†å™¨ï¼ŒèŠ‚ç‚¹ç›‘æŽ§å™¨ï¼Œç½‘ç»œæ’ä»¶ï¼Œå­˜å‚¨æ’ä»¶

##  Deployment

* Deploymentï¼šè´Ÿè´£ç®¡ç†å’Œç»´æŠ¤ä½ çš„åº”ç”¨å®žä¾‹ï¼ˆPodï¼‰ã€‚å®ƒä¼šç¡®ä¿æŒ‡å®šæ•°é‡çš„ Nginx Pod æ­£åœ¨è¿è¡Œã€‚å¦‚æžœæŸä¸ª Pod æŒ‚æŽ‰äº†ï¼ŒDeployment
  ä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªæ–°çš„æ¥æ›¿ä»£å®ƒ
  
* åœ¨ Deploymentï¼ˆä»¥åŠ ReplicaSet, StatefulSet, Job, CronJob ç­‰è¿™ç±»æŽ§åˆ¶å™¨ï¼‰çš„ Pod æ¨¡æ¿ï¼ˆspec.templateï¼‰ä¸­ï¼Œmetadata.name è¿™ä¸ªå­—æ®µæ˜¯ä¸èƒ½è®¾ç½®çš„ã€‚å¦‚æžœä½ å°è¯•è®¾ç½®å®ƒï¼ŒKubernetes API Server ä¼šæ‹’ç»ä½ çš„è¯·æ±‚ã€‚

* Deployment è¦èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼ˆç‰¹åˆ«æ˜¯è¿è¡Œå¤šä¸ªå‰¯æœ¬ã€è¿›è¡Œæ»šåŠ¨æ›´æ–°å’Œè‡ªæˆ‘ä¿®å¤ï¼‰ï¼Œå…¶åº•å±‚çš„ Pod å¿…é¡»é€šè¿‡ç±»ä¼¼ `generateName` çš„æœºåˆ¶æ¥åˆ›å»ºï¼Œä»¥ä¿è¯æ¯ä¸ª Pod åç§°çš„å”¯ä¸€æ€§

* The server may generate a name when generateName is provided instead of name in a resource create request. When generateName is used, the provided value is used as a name prefix, which server appends a generated suffix to.

  > Kubernetes v1.31ä»¥åŽä¼šé‡è¯•8æ¬¡ä»¥ä½¿ç”Ÿæˆå”¯ä¸€çš„åå­—
  
* ä¸€ä¸ª Deployment å®žé™…ä¸Šå¹¶ä¸ç›´æŽ¥ç®¡ç† Podï¼Œå®ƒçš„å·¥ä½œæµç¨‹æ˜¯è¿™æ ·çš„ï¼š

  1. **Deployment**: ä½ åˆ›å»ºäº†ä¸€ä¸ª Deployment èµ„æºï¼Œå®ƒçš„åç§°æ˜¯å›ºå®šçš„ï¼ˆæ¯”å¦‚ `nginx-deployment`ï¼‰ã€‚è¿™ä¸ª Deployment è´Ÿè´£ç®¡ç†â€œç‰ˆæœ¬â€ã€‚
  
  2. **ReplicaSet**: Deployment ä¼šæ ¹æ®è‡ªå·±çš„ Pod æ¨¡æ¿ï¼Œåˆ›å»ºä¸€ä¸ª **ReplicaSet** èµ„æºã€‚è¿™ä¸ª ReplicaSet çš„åç§°æ˜¯**åŠ¨æ€ç”Ÿæˆçš„**ï¼Œé€šå¸¸æ˜¯ `[Deploymentåç§°]-[Podæ¨¡æ¿çš„å“ˆå¸Œå€¼]`ï¼Œä¾‹å¦‚ `nginx-deployment-66b6c48dd5`ã€‚è¿™ä¸ªå“ˆå¸Œå€¼ç¡®ä¿äº†æ¯æ¬¡ä½ æ›´æ–° Deployment çš„ Pod æ¨¡æ¿æ—¶ï¼ˆæ¯”å¦‚æ›´æ¢é•œåƒç‰ˆæœ¬ï¼‰ï¼Œéƒ½ä¼šåˆ›å»ºä¸€ä¸ªå…¨æ–°çš„ã€ä¸åŒåç§°çš„ ReplicaSetã€‚
  
     > kubectl get rs
     >
     > kubectl get replicateSet 
  
  3. **Pod**: ReplicaSet çš„ä»»åŠ¡å¾ˆç®€å•ï¼Œå°±æ˜¯ç¡®ä¿æœ‰æŒ‡å®šæ•°é‡çš„ã€ç¬¦åˆå…¶æ¨¡æ¿çš„ Pod æ­£åœ¨è¿è¡Œã€‚å®ƒä¼šæ ¹æ®è‡ªå·±çš„åç§°ä½œä¸º**å‰ç¼€**ï¼ŒåŽ»åˆ›å»º Podã€‚æ‰€ä»¥ï¼Œæœ€ç»ˆ Pod çš„åç§°ä¹Ÿæ˜¯**åŠ¨æ€ç”Ÿæˆçš„**ï¼Œæ ¼å¼é€šå¸¸æ˜¯ `[ReplicaSetåç§°]-[éšæœºåŽç¼€]`ï¼Œä¾‹å¦‚ `nginx-deployment-66b6c48dd5-x7p9m`ã€‚

## Service

* Serviceï¼šè´Ÿè´£ä¸ºä¸€ç»„ Pod æä¾›ä¸€ä¸ªç¨³å®šã€ç»Ÿä¸€çš„è®¿é—®å…¥å£ã€‚å› ä¸º Pod æ˜¯â€œçŸ­æš‚â€çš„ï¼Œå®ƒä»¬çš„ IP åœ°å€ä¼šå˜åŒ–ã€‚Service æä¾›äº†ä¸€ä¸ªå›ºå®šçš„
  IP åœ°å€å’Œ DNS åç§°ï¼Œä½¿å¾—å…¶ä»–åº”ç”¨æˆ–å¤–éƒ¨ç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°è®¿é—®åˆ°ä½ çš„ Nginx æœåŠ¡ï¼Œè€Œæ— éœ€å…³å¿ƒåŽç«¯å…·ä½“æ˜¯å“ªä¸ª Pod åœ¨æä¾›æœåŠ¡ã€‚

  > Service çš„ IP åœ°å€ (`ClusterIP`) å’Œ DNS åç§°çš„â€œå›ºå®šâ€ï¼Œæ˜¯ç›¸å¯¹äºŽ **Service è¿™ä¸ª API å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸ** è€Œè¨€çš„ã€‚
  >
  > ç®€å•æ¥è¯´ï¼Œåªè¦ä½ ä¸åˆ é™¤è¿™ä¸ª Service å¯¹è±¡ï¼ˆ`kubectl delete service my-service`ï¼‰ï¼Œå®ƒçš„ `ClusterIP` å’Œ DNS åç§°å°±**ä¸ä¼šæ”¹å˜**ã€‚
  >
  >  FQDN (Fully Qualified Domain Name)ï¼š [service-name].[namespace-name].svc.[cluster-domain].
  >
  > * `[cluster-domain]` é›†ç¾¤åŸŸåæ˜¯é›†ç¾¤çº§åˆ«çš„é…ç½®ï¼Œé€šå¸¸æ˜¯å›ºå®šçš„ã€‚æœ€å¯é çš„æŸ¥æ‰¾æ–¹æ³•æ˜¯è¿›å…¥ä»»æ„ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„ Podï¼ŒæŸ¥çœ‹å®ƒçš„ DNS é…ç½®æ–‡ä»¶ `/etc/resolv.conf`
  >
  >   ```shell
  >   # 1. é¦–å…ˆï¼Œéšä¾¿æ‰¾ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„ Pod
  >   kubectl get pods
  >   
  >   # å‡è®¾æˆ‘ä»¬æ‰¾åˆ°äº†ä¸€ä¸ªå« nginx-deployment-6bcfd6f857-klmno çš„ Pod
  >   
  >   # 2. ä½¿ç”¨ kubectl exec è¿›å…¥è¯¥ Pod å¹¶æŸ¥çœ‹ resolv.conf æ–‡ä»¶
  >   kubectl exec -it nginx-deployment-6bcfd6f857-klmno -- cat /etc/resolv.conf
  >   ## åœ¨ä¸Šé¢è¿™ä¸ªå‘½ä»¤ä¸­ï¼Œcat å‰é¢çš„`--` èµ·åˆ°åˆ†éš”çš„ä½œç”¨ï¼Œå‘Šè¯‰kubectlçš„å‘½ä»¤å·²ç»ç»“æŸã€‚ä¸Ždocker execä¸åŒï¼Œdocker exec ä¸éœ€è¦è¿™ä¸ª                                   
  >   ```
  >
  >   è¾“å‡ºä¸­`search default.svc.cluster.local svc.cluster.local cluster.local`  æœ€åŽè¿™ä¸ª`cluster.local` å°±æ˜¯è¿™ä¸ªå€¼
  >
  > * è¿˜å¯ä»¥åœ¨é›†ç¾¤å†…ç›´æŽ¥æŸ¥è¯¢
  >
  >   å¯åŠ¨ä¸€ä¸ªä¸´æ—¶çš„è°ƒè¯• Pod
  >
  >   ```shell
  >   # è¿è¡Œä¸€ä¸ªä¸´æ—¶çš„ busybox Podï¼Œå¹¶è¿›å…¥å…¶ shell çŽ¯å¢ƒ
  >   kubectl run dns-test -it --rm --image=busybox:1.28 -- sh
  >   ```
  >
  >   åœ¨ Pod å†…éƒ¨ä½¿ç”¨ `nslookup` è¿›è¡ŒæŸ¥è¯¢
  >
  >   ```shell
  >   # (å› ä¸ºæˆ‘ä»¬åœ¨ default å‘½åç©ºé—´é‡Œ)
  >   nslookup springboot3-service
  >   ```

* ä½ å¯ä»¥æŠŠ app: nginx ç†è§£ä¸ºä½ å’Œ Kubernetes çš„ä¸€ä¸ªçº¦å®šï¼šä½ ç»™ä¸€ç»„ Pod è´´ä¸Šè¿™ä¸ªç‹¬ç‰¹çš„â€œåç‰Œâ€ï¼Œç„¶åŽå‘Šè¯‰ Deployment å’Œ Service
  æŒ‰ç…§è¿™ä¸ªâ€œåç‰Œâ€åŽ»è®¤é¢†å’ŒæŸ¥æ‰¾å®ƒä»¬
  
* Service `type` æ˜¯ `ClusterIP`ã€é»˜è®¤å€¼ã€‘æ—¶ipä¸ç›´æŽ¥æš´éœ²åˆ°é›†ç¾¤å¤–éƒ¨ï¼Œåªèƒ½è¢«é›†ç¾¤å†…çš„ Ingress æŽ§åˆ¶å™¨æ‰¾åˆ°ã€‚typeä¸ºloadBalanceræ—¶, ç«¯å£ä¼šæš´éœ²åˆ°é›†ç¾¤å¤–ã€‚ã€åœ¨k3dä¸­æµ‹è¯•æ—¶ï¼ŒæŠŠserviceçš„typeè®¾ç½®ä¸ºloadBalanceå¹¶ä¸ç”Ÿæ•ˆã€‘
  
  > `LoadBalancer` ç±»åž‹æ˜¯ `NodePort` çš„æ‰©å±•ã€‚å®ƒä¼šå‘åº•å±‚äº‘å¹³å°ï¼ˆå¦‚ AWS, GCP, Azureï¼‰è¯·æ±‚ä¸€ä¸ªå¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨ï¼Œå¹¶å°†è¿™ä¸ªè´Ÿè½½å‡è¡¡å™¨çš„ IP åœ°å€ä½œä¸º Service çš„å¤–éƒ¨è®¿é—®å…¥å£ã€‚
  >
  > - **ä½œç”¨**ï¼šè¿™æ˜¯å°†æœåŠ¡æš´éœ²åˆ°å…¬ç½‘çš„ **æ ‡å‡†æ–¹å¼**ã€‚äº‘æœåŠ¡æä¾›å•†ä¼šä¸ºä½ åˆ›å»ºä¸€ä¸ªè´Ÿè½½å‡è¡¡å™¨ï¼Œå¹¶å°†æµé‡å¯¼å‘ä½ æ‰€æœ‰èŠ‚ç‚¹çš„ `NodePort`ã€‚
  > - **ä½¿ç”¨åœºæ™¯**ï¼šé€‚ç”¨äºŽç”Ÿäº§çŽ¯å¢ƒï¼Œå½“ä½ éœ€è¦ä¸€ä¸ªç¨³å®šã€é«˜å¯ç”¨çš„å…¬ç½‘ IP æ¥æš´éœ²ä½ çš„æœåŠ¡æ—¶ã€‚

## Config file

* kubectl é»˜è®¤ä¼šåœ¨ä½ çš„ç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„ .kube æ–‡ä»¶å¤¹ä¸­å¯»æ‰¾åä¸º config çš„æ–‡ä»¶ã€‚

  > åœ¨ Linux å’Œ macOS ä¸Šï¼Œè·¯å¾„é€šå¸¸æ˜¯ ~/.kube/configã€‚

* Most often, you provide the information to kubectl in a file known as a manifest. By convention, manifests are YAML (you could also use JSON format).

* YAML æ–‡ä»¶åœ¨ä¸¤ç§æ¨¡å¼ä¸‹çš„â€œè§’è‰²â€
    * åœ¨ kubectl create -f (å‘½ä»¤å¼) ä¸­ï¼šYAML æ–‡ä»¶æ˜¯ä¸€ä¸ªä¸€æ¬¡æ€§çš„æ¨¡æ¿ã€‚ä½ å‘½ä»¤ Kubernetesï¼šâ€œæŒ‰ç…§è¿™ä¸ªæ¨¡æ¿ï¼Œç»™æˆ‘åˆ›å»ºä¸€ä¸ªå¯¹è±¡â€ã€‚åˆ›å»ºå®ŒæˆåŽï¼Œè¿™ä¸ª
      YAML æ–‡ä»¶å’Œé›†ç¾¤ä¸­çš„é‚£ä¸ªå¯¹è±¡ä¹‹é—´ï¼Œå°±æ²¡æœ‰å¿…ç„¶çš„è”ç³»äº†ã€‚Kubernetes ä¸ä¼šâ€œè®°ä½â€ä½ æ˜¯ç”¨å“ªä¸ªæ–‡ä»¶åˆ›å»ºçš„å®ƒã€‚
    * åœ¨ kubectl apply -f (å£°æ˜Žå¼) ä¸­ï¼šYAML æ–‡ä»¶æ˜¯å¯¹è±¡çš„**â€œæœŸæœ›çŠ¶æ€â€çš„å£°æ˜Ž**ã€‚ä½ å‘Šè¯‰ Kubernetesï¼šâ€œè¯·ç¡®ä¿é›†ç¾¤ä¸­æœ‰ä¸€ä¸ªä¸Žè¿™ä¸ª
      YAML æ–‡ä»¶æè¿°çš„çŠ¶æ€ç›¸åŒ¹é…çš„å¯¹è±¡â€ã€‚Kubernetes ä¸ä»…ä¼šåˆ›å»ºè¿™ä¸ªå¯¹è±¡ï¼Œè¿˜ä¼šè®°å½•ä¸‹è¿™ä¸ªâ€œæœŸæœ›çŠ¶æ€â€ï¼Œä»¥ä¾¿äºŽæœªæ¥çš„æ¯”è¾ƒå’Œæ›´æ–°ã€‚

* ä»…ä»…ä¿®æ”¹å¹¶ä¿å­˜åœ¨æœ¬åœ° configs/ ç›®å½•ä¸‹çš„ YAML æ–‡ä»¶ï¼Œå¹¶ä¸ä¼šå¯¹é›†ç¾¤äº§ç”Ÿä»»ä½•å½±å“ã€‚ Kubernetes é›†ç¾¤å®Œå…¨ä¸çŸ¥é“ä½ æœ¬åœ°æ–‡ä»¶çš„å˜åŒ–ã€‚ä½ å¿…é¡»é€šè¿‡
  kubectl apply è¿™ä¸ªåŠ¨ä½œï¼Œæ˜Žç¡®åœ°å‘Šè¯‰ Kubernetesï¼š"è¯·æŒ‰ç…§æˆ‘æœ€æ–°çš„é…ç½®æ–‡ä»¶ï¼ŒåŽ»åŒæ­¥é›†ç¾¤çš„çŠ¶æ€ã€‚"

  ```shell
   kubectl diff -f configs/
  ```

  Our previous example (replicas): The change from kubectl scale was NOT retained because the replicas field was "owned"
  by your YAML file. apply enforced your file's value.

* The note's meaning (LoadBalancer example): Changes from other controllers (like adding a clusterIP) ARE retained, because those fields are not "owned" by your YAML file. The patch mechanism surgically updates only the fields you explicitly manage in your file.

* Starting with Kubernetes v1.25, the API server offers server side field validation that detects unrecognized or duplicate fields in an object. It provides all the functionality of kubectl --validate on the server side.

  > **æœåŠ¡ç«¯è¯•è¿è¡Œ (Server-side Dry Run)**
  >
  > ```shell
  > kubectl apply -f [your-manifest].yaml --dry-run=server
  > ```
  >
  > å¦‚æžœæ–‡ä»¶æœ‰é”™è¯¯ï¼Œå®ƒä¼šåƒä¸Šé¢çš„ä¾‹å­ä¸€æ ·æŠ¥é”™ã€‚å¦‚æžœæ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼Œå®ƒä¼šè¿”å›žä¸€ä¸ªæˆåŠŸçš„æç¤ºï¼ˆä½†ä¸ä¼šçœŸçš„åˆ›å»ºèµ„æºï¼‰ã€‚**æ€»ä¹‹ï¼Œ--dry-run=server æ˜¯ä¸€ä¸ªéžå¸¸å®‰å…¨çš„éªŒè¯å·¥å…·ã€‚** å®ƒçš„è®¾è®¡åˆè¡·å°±æ˜¯ä¸ºäº†è®©æ‚¨åœ¨çœŸæ­£éƒ¨ç½²åˆ°é›†ç¾¤ä¹‹å‰ï¼Œç™¾åˆ†ä¹‹ç™¾ç¡®è®¤æ‚¨çš„é…ç½®æ¸…å•æ˜¯æœ‰æ•ˆä¸”è¢«é›†ç¾¤æ‰€æŽ¥å—çš„ï¼Œè€Œæ— éœ€æ‹…å¿ƒä¼šæ„å¤–åˆ›å»ºæˆ–ä¿®æ”¹ä»»ä½•ä¸œè¥¿ã€‚

## Object

Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.

* Keep in mind that label Key must be unique for a given object
  
* Names of resources need to be unique within a namespace, but not across namespaces.
  
* When you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as
  some basic information about the object (such as a name).

* Almost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status.

* The status describes the current state of the object, supplied and updated by the Kubernetes system and its components.

  > statuså¯ä»¥ç†è§£ä¸ºâ€œçœ‹èµ·æ¥æ˜¯ä»€ä¹ˆæ ·â€ï¼Œè€Œstateæ˜¯â€œå®žé™…æ˜¯ä»€ä¹ˆæ ·å­çš„â€

* Each object in your cluster has a Name that is unique for that type of resource. Every Kubernetes object also has a UID that is unique across your whole cluster.

  > For example, you can only have one Pod named `myapp-1234` within the same namespace, but you can have one Pod and one Deployment that are each named `myapp-1234`.

* **Kubernetes çš„ä¸–ç•Œè§‚æ˜¯å»ºç«‹åœ¨å®ƒè‡ªå·±çš„ API å¯¹è±¡ä¸Šçš„**ã€‚å®ƒé€šè¿‡ Kubelet ç­‰ç»„ä»¶æ¥è§‚æµ‹å¤–éƒ¨ç‰©ç†ä¸–ç•Œçš„çŠ¶æ€ï¼Œå¹¶å°½åŠ›ä½¿å…¶ä¸Žå†…éƒ¨çš„å£°æ˜Žå¼çŠ¶æ€ä¿æŒä¸€è‡´ã€‚ä½†å¦‚æžœå¤–éƒ¨ä¸–ç•Œå‘ç”Ÿäº†å®ƒæ— æ³•è§‚æµ‹åˆ°çš„å‰§çƒˆå˜åŒ–ï¼ˆæ¯”å¦‚ä¸€ä¸ªèŠ‚ç‚¹è¢«å·å·æ›¿æ¢äº†ï¼‰ï¼Œè€Œå†…éƒ¨çš„é€»è¾‘å¯¹è±¡æ²¡æœ‰è¢«ç›¸åº”æ›´æ–°ï¼Œå°±ä¼šå¯¼è‡´è¿™ç§â€œèº«ä»½æ··æ·†â€å’ŒçŠ¶æ€ä¸ä¸€è‡´ï¼Œä»Žè€Œå¼•å‘å„ç§éš¾ä»¥æŽ’æŸ¥çš„è¯¡å¼‚é—®é¢˜ã€‚

  > åœ¨ç‰©ç†/è™šæ‹Ÿå±‚é¢é”€æ¯ä¸€ä¸ªèŠ‚ç‚¹ä¹‹å‰ï¼Œ**åŠ¡å¿…å…ˆåœ¨ Kubernetes ä¸­å°†å…¶åˆ é™¤**ã€‚
  >
  > æ­£ç¡®çš„æ“ä½œæµç¨‹åº”è¯¥æ˜¯ï¼š
  >
  > 1. **æ ‡è®°èŠ‚ç‚¹ä¸å¯è°ƒåº¦**ï¼š
  >
  >    ```shell
  >    kubectl cordon worker-01
  >    ```
  >
  >    è¿™èƒ½é˜²æ­¢æ–°çš„ Pod è¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚
  >
  > 2. **é©±é€èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰ Pod**ï¼š
  >
  >    ```shell
  >    kubectl drain worker-01 --ignore-daemonsets
  >    ```
  >
  >    è¿™ä¼šå®‰å…¨åœ°å°†è¯¥èŠ‚ç‚¹ä¸ŠçŽ°æœ‰çš„ Pod è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹ã€‚`--ignore-daemonsets` æ˜¯å› ä¸º DaemonSet ç®¡ç†çš„ Pod ä¸éœ€è¦è¢«é©±é€ã€‚
  >
  > 3. **ä»Ž Kubernetes ä¸­åˆ é™¤èŠ‚ç‚¹å¯¹è±¡**ï¼š
  >
  >    ```shell
  >    kubectl delete node worker-01
  >    ```
  >
  >    è¿™ä¸€æ­¥å°±æ˜¯â€œé”€æ¯å­¦ç±å¡â€ï¼Œå½»åº•æ¸…é™¤å®ƒåœ¨ Kubernetes ä¸­çš„æ‰€æœ‰è®°å½•ã€‚
  >
  > 4. **é”€æ¯ç‰©ç†/è™šæ‹Ÿæœº**ï¼š çŽ°åœ¨ï¼Œä½ å¯ä»¥å®‰å…¨åœ°åŽ»ä½ çš„äº‘å¹³å°æˆ–è™šæ‹ŸåŒ–å¹³å°åˆ é™¤è¿™å°æœåŠ¡å™¨äº†ã€‚

* A client-provided string that refers to an object in a [resource](https://kubernetes.io/docs/reference/using-api/api-concepts/#standard-api-terminology) URL, such as `/api/v1/pods/some-name`.

  Only one object of a given kind can have a given name at a time.  Names must be unique across **all API versions** of the same resource. API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name. In other words, API version is irrelevant in this context.

## Kubernetes API

* There are two mechanisms that Kubernetes uses to publish these API specifications

  > 1. The Discovery API
  > 2. The Kubernetes OpenAPI Document

* é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»æ˜Žç™½ Discovery API çš„ç›®çš„ã€‚æ— è®ºæ˜¯ kubectlã€Rancher UI è¿˜æ˜¯ä»»ä½•å…¶ä»–ä¸Ž Kubernetes é›†ç¾¤äº¤äº’çš„å®¢æˆ·ç«¯ï¼Œå®ƒä»¬åœ¨æ‰§è¡Œæ“ä½œä¹‹å‰ï¼Œéƒ½éœ€è¦å…ˆçŸ¥é“ï¼š
  
  * â€œè¿™ä¸ªé›†ç¾¤é‡Œæœ‰å“ªäº› API Groupï¼Ÿâ€: (ä¾‹å¦‚ apps, batch, networking.k8s.io ç­‰)ã€‚
  * â€œæ¯ä¸ª Group ä¸‹æœ‰å“ªäº›ç‰ˆæœ¬ï¼Ÿâ€ : (ä¾‹å¦‚ apps group ä¸‹æœ‰v1)â€œ
  * æ¯ä¸ª Group/Version ä¸‹æœ‰å“ªäº›èµ„æº (Resource)ï¼Ÿâ€ : (ä¾‹å¦‚ apps/v1 ä¸‹æœ‰ deployments, statefulsets, daemonsets ç­‰)
  * â€œè¿™äº›èµ„æºæ”¯æŒå“ªäº›æ“ä½œ (Verb)ï¼Ÿâ€ : (ä¾‹å¦‚ deployments æ”¯æŒ create, get, list, delete ç­‰)
  
* Unaggregated Discovery (éžèšåˆå‘çŽ°)
  Unaggregated Discovery æŒ‡çš„æ˜¯ å•ä¸ª API æœåŠ¡å™¨è‡ªèº« æä¾›çš„ã€å…³äºŽ å®ƒè‡ªå·±æ‰€èƒ½æœåŠ¡çš„ API çš„å‘çŽ°ä¿¡æ¯ã€‚

* Aggregated Discovery (èšåˆå‘çŽ°)
  Aggregated Discovery æ­£æ˜¯ Kubernetes API Aggregation Layer (èšåˆå±‚) çš„å¼ºå¤§ä¹‹å¤„ã€‚å®ƒæä¾›äº†ä¸€ä¸ª ç»Ÿä¸€çš„ã€èšåˆåŽ çš„è§†å›¾ã€‚
  å½“å®¢æˆ·ç«¯ï¼ˆå¦‚ kubectlï¼‰æŸ¥è¯¢ä¸» kube-apiserver çš„å‘çŽ°ç«¯ç‚¹æ—¶ï¼Œèšåˆå±‚ä¸ä»…ä¼šè¿”å›ž kube-apiserver è‡ªå·±çš„ API ä¿¡æ¯ï¼Œè¿˜ä¼šæ™ºèƒ½åœ°å°†æ‰€æœ‰å·²æ³¨å†Œçš„æ‰©å±•
  API æœåŠ¡å™¨ï¼ˆé€šè¿‡ APIService å¯¹è±¡æ³¨å†Œï¼‰çš„å‘çŽ°ä¿¡æ¯ä¹Ÿä¸€å¹¶åŒ…å«è¿›æ¥å¹¶è¿”å›žã€‚
  
* Kubernetes offers stable support for aggregated discovery, publishing all resources supported by a cluster through two
  endpoints (/api and /apis).
  > * /api: åˆ—å‡ºæ ¸å¿ƒ API Group (åªæœ‰ v1)ã€‚ã€**æ ¸å¿ƒ API (Core API)** æˆ–ç§°ä¸º**åŽ†å²é—ç•™ API (Legacy API)**ã€‘
  > * /apis: åˆ—å‡ºæ‰€æœ‰éžæ ¸å¿ƒçš„ API Group (å¦‚ apps, batch, apiextensions.k8s.io ç­‰)ã€‚ã€åˆ†ç»„ API (Grouped API)ã€‘
      >   ä¸ºä»€ä¹ˆä¼šæœ‰ä¸¤ä¸ªç«¯ç‚¹ï¼š æœ€åˆçš„è®¾è®¡: åœ¨ Kubernetes çš„æ—©æœŸï¼Œæ‰€æœ‰çš„ API èµ„æºå¯¹è±¡ï¼ˆå¦‚ Pod, Service, Node,
      ReplicationController ç­‰ï¼‰éƒ½è¢«æ”¾åœ¨ä¸€ä¸ªæ²¡æœ‰åå­—çš„ API Group é‡Œï¼Œè¿™ä¸ª Group å°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„â€œæ ¸å¿ƒç»„ (Core Group)
      â€ã€‚ç”±äºŽå®ƒæ²¡æœ‰åå­—ï¼Œä¸ºäº†è®¿é—®å®ƒï¼ŒAPI Server å°±æä¾›äº† /api/v1 è¿™ä¸ªç‰¹æ®Šçš„ç«¯ç‚¹ã€‚åœ¨å½“æ—¶ï¼Œè¿™å°±æ˜¯ Kubernetes çš„å…¨éƒ¨ APIã€‚
      å‘çŽ°æ‰©å±•æ€§é—®é¢˜: éšç€é¡¹ç›®çš„å‘å±•ï¼Œå¼€å‘è€…ä»¬å¾ˆå¿«æ„è¯†åˆ°ï¼ŒæŠŠæ‰€æœ‰ä¸œè¥¿éƒ½å¡žè¿›ä¸€ä¸ªæ²¡æœ‰åˆ†ç»„çš„ API é‡Œæ˜¯æ— æ³•æ‰©å±•çš„ã€‚å¦‚æžœæˆ‘æƒ³æ·»åŠ ä¸€ç»„æ–°çš„
      API ç”¨äºŽå¤„ç†â€œæ‰¹å¤„ç†ä»»åŠ¡â€ï¼Œæˆ–è€…å¦ä¸€ç»„ API ç”¨äºŽå¤„ç†â€œç½‘ç»œç­–ç•¥â€ï¼ŒæŠŠå®ƒä»¬éƒ½å †åœ¨æ ¸å¿ƒç»„é‡Œä¼šå˜å¾—éžå¸¸æ··ä¹±ã€‚ â€œå‘½åç»„â€çš„è¯žç”Ÿ:
      ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒKubernetes å¼•å…¥äº†â€œAPI Groupï¼ˆå‘½åç»„ï¼‰â€çš„æ¦‚å¿µã€‚è¿™å…è®¸å¼€å‘è€…æ ¹æ®åŠŸèƒ½é¢†åŸŸå°† API èµ„æºè¿›è¡Œé€»è¾‘åˆ†ç»„ã€‚ä¾‹å¦‚ï¼š
      apps ç»„ï¼šåŒ…å« Deployment, StatefulSet, DaemonSet ç­‰ã€‚batch ç»„ï¼šåŒ…å« Job, CronJob ç­‰ã€‚ networking.k8s.io ç»„ï¼šåŒ…å«
      Ingress, NetworkPolicy ç­‰ã€‚ æ‰€æœ‰è¿™äº›â€œå‘½åç»„â€çš„ API éƒ½é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„å‰ç¼€ /apis æ¥è®¿é—®ï¼Œä¾‹å¦‚
      /apis/apps/v1ï¼Œ/apis/batch/v1ã€‚
  
* æ‰§è¡Œè¿™ä¸ªå‘½ä»¤ï¼Œä½ ä¼šçœ‹åˆ° kubectl æ­£åœ¨å‘ apiserver å‘å‡ºä¸€ç³»åˆ—çš„ GET è¯·æ±‚æ¥å‘çŽ°èµ„æº

  ```shell
  kubectl get pods --v=8
  ```

* æŸ¥è¯¢æ‰€æœ‰å¯ç”¨api

  ```shell
  kubectl api-versions
  ```

* ç›´æŽ¥è®¿é—® API ServeråŽ»æŸ¥è¯¢æœ‰å“ªäº›æœ‰ç”¨api-versions

  `kubectl` å‘½ä»¤å®žé™…ä¸Šæ˜¯åœ¨åŽå°å‘ Kubernetes API Server å‘é€ HTTP è¯·æ±‚ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰‹åŠ¨æ¨¡æ‹Ÿè¿™ä¸ªè¿‡ç¨‹æ¥æŽ¢ç´¢ APIã€‚ä¸ºäº†å®‰å…¨åœ°è®¿é—® API Serverï¼Œæœ€ç®€å•çš„æ–¹å¼æ˜¯ä½¿ç”¨ `kubectl proxy`ã€‚

  åœ¨ä¸€ä¸ªç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œè®©è¿™ä¸ªç»ˆç«¯ä¿æŒè¿è¡Œã€‚

  ```shell
  kubectl proxy
  ```

  æ‰“å¼€å¦ä¸€ä¸ªç»ˆç«¯ï¼Œä½¿ç”¨ `curl` è¿›è¡ŒæŸ¥è¯¢ã€‚

  ```shell
  curl http://127.0.0.1:8001/api
  ## /apis
  curl http://127.0.0.1:8001/apis
  ```

* æŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„ API èµ„æº (`api-resources`)

  ```shell
  kubectl api-resources
  # --api-group="" è¡¨ç¤ºæŸ¥è¯¢æ ¸å¿ƒç»„
  kubectl api-resources --api-group=""
  ```

* Without indicating the resource type using the Accept header, the default response for the /api and /apis endpoint is
  an unaggregated discovery document.
  
* the kubectl tool fetches and caches the API specification for enabling command-line completion and other features. The
  two supported mechanisms are as follows:
  
    * Discovery API å°±åƒæ˜¯è¿™æœ¬ä¹¦çš„ â€œç›®å½•â€ã€‚
    * OpenAPI Document å°±åƒæ˜¯è¿™æœ¬ä¹¦ â€œæ­£æ–‡å†…å®¹ä¸­æ‰€æœ‰åè¯çš„è¯¦ç»†è§£é‡Šå’Œè¯­æ³•ç»“æž„è¯´æ˜Žâ€

## k3d æµ‹è¯•ç›¸å…³

* åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œé€šå¸¸ä¼šæœ‰å¤šä¸ª Master èŠ‚ç‚¹ï¼ˆåœ¨ k3d/k3s é‡Œè¢«ç§°ä¸º Server èŠ‚ç‚¹ï¼‰æ¥ç¡®ä¿é«˜å¯ç”¨æ€§ã€‚ä½ ä¸ä¼šç›´æŽ¥è¿žæŽ¥åˆ°æŸä¸€ä¸ª Master èŠ‚ç‚¹ï¼Œå› ä¸ºå¦‚æžœé‚£ä¸ªèŠ‚ç‚¹å®•æœºäº†ï¼Œä½ å°±æ— æ³•è®¿é—®é›†ç¾¤äº†ã€‚æ­£ç¡®çš„åšæ³•æ˜¯è¿žæŽ¥åˆ°ä¸€ä¸ª**è´Ÿè½½å‡è¡¡å™¨ (Load Balancer)**ï¼Œç”±å®ƒæ¥å°†ä½ çš„è¯·æ±‚è½¬å‘ç»™åŽé¢å¥åº·çš„ Master èŠ‚ç‚¹ã€‚

  k3d åœ¨æœ¬åœ°ç”¨ Docker å®¹å™¨å·§å¦™åœ°å¤çŽ°äº†è¿™å¥—æž¶æž„ï¼š

  1. **`k3d-my-cluster-server-0` å®¹å™¨**: è¿™æ˜¯çœŸæ­£çš„ K3s Serverï¼Œå®ƒåœ¨**å®¹å™¨å†…éƒ¨**è¿è¡Œç€ Kubernetes API Serverï¼Œç›‘å¬ç€ `6443` ç«¯å£ã€‚è¿™ä¸ªå®¹å™¨æ²¡æœ‰ç›´æŽ¥æš´éœ²ç«¯å£åˆ°å®¿ä¸»æœºï¼Œæ‰€ä»¥ä½ ä»Žå¤–éƒ¨æ— æ³•ç›´æŽ¥è®¿é—®å®ƒã€‚
  2. **`k3d-my-cluster-serverlb` å®¹å™¨**: è¿™æ˜¯ä¸€ä¸ªåŸºäºŽ NGINX çš„åå‘ä»£ç†/è´Ÿè½½å‡è¡¡å™¨ã€‚k3d å¯åŠ¨å®ƒï¼Œå¹¶è®©å®ƒç›‘å¬å®¿ä¸»æœºçš„ä¸€ä¸ªç«¯å£ï¼Œç„¶åŽå°†æµé‡è½¬å‘ç»™åŽç«¯çš„ K3s Server å®¹å™¨ã€‚

* ä½ çš„ kubectl å¹¶ä¸æ˜¯ç›´æŽ¥å’Œ K3s Server å®¹å™¨é€šä¿¡ã€‚å®ƒåœ¨å’Œä¸€ä¸ªä½œä¸ºè´Ÿè½½å‡è¡¡å™¨çš„ä»£ç†å®¹å™¨ (k3d-my-cluster-serverlb) é€šä¿¡ã€‚è¿™ä¸ªä»£ç†å®¹å™¨è´Ÿè´£å°†ä½ çš„è¯·æ±‚å®‰å…¨åœ°è½¬å‘ç»™çœŸæ­£çš„ K3s Server å®¹å™¨ã€‚39753 æ˜¯ k3d ä¸ºè¿™ä¸ªè´Ÿè½½å‡è¡¡å™¨éšæœºé€‰æ‹©çš„ã€æš´éœ²åœ¨ä½ å®¿ä¸»æœºä¸Šçš„ç«¯å£ã€‚

* åœ¨k3dé‡Œæµ‹è¯•æ—¶ï¼Œè®¾ç½®type=LoadBalanceræ—¶æ²¡æœ‰ç”¨ï¼Œå³ä½¿è®¾ç½®k3d cluster create my-cluster -p "8080:80@loadbalancer"ï¼Œéœ€è¦æ˜ å°„type=NodePort çš„ç«¯å£ï¼Œå¦‚8080:30080

* åœ¨ç”¨k3dåšæµ‹è¯•æ—¶ï¼Œé›†ç¾¤èŠ‚ç‚¹ "çœ‹" ä¸åˆ°ä½ æœ¬åœ°æœºå™¨ä¸Šçš„ Docker é•œåƒ

  > 1. ä½¿ç”¨ `k3d image import` å‘½ä»¤
  >
  >    `k3d image import springboot3:v1.0.10 -c my-cluster`
  >
  > 2. **ä¿®æ”¹ä½ çš„ Deployment YAML æ–‡ä»¶**
  >
  >    imagePullPolicy: IfNotPresent # <-- å…³é”®ï¼æ·»åŠ è¿™ä¸€è¡Œ
  >
  > åœ¨ç”Ÿäº§çŽ¯å¢ƒæˆ–æ›´å¤æ‚çš„å¼€å‘çŽ¯å¢ƒä¸­ï¼Œæœ€ä½³å®žè·µæ˜¯æ­å»ºä¸€ä¸ªé•œåƒä»“åº“ï¼ˆRegistryï¼‰ï¼Œæ¯”å¦‚ Harborã€Nexusï¼Œæˆ–è€…ç›´æŽ¥ä½¿ç”¨ Docker Hubã€é˜¿é‡Œäº‘ ACR ç­‰ã€‚

## Label


* Labels are key/value pairs. Valid label keys have two segments: an optional prefix and name, separated by a slash (/).

  > Valid label value:
  >
  > - must be 63 characters or less (can be empty),
  > - unless empty, must begin and end with an alphanumeric character (`[a-z0-9A-Z]`),
  > - could contain dashes (`-`), underscores (`_`), dots (`.`), and alphanumerics between.

* The API currently supports two types of selectors: equality-based and set-based. 

* If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler, kube-controller-manager, kube-apiserver, kubectl, or other third-party automation) which add labels to end-user objects must specify a prefix.

* the comma separator acts as a logical AND (&&) operator.

* selector: { component: redis } æ˜¯æ—§ç‰ˆçš„ã€ç®€æ´çš„å†™æ³•ã€‚

  selector: { matchLabels: { component: redis } } æ˜¯æ–°ç‰ˆçš„ã€æ›´ç»“æž„åŒ–ã€æ›´æŽ¨èçš„å†™æ³•ã€‚

  Kubernetes API åœ¨å¤„ç†ç¬¬ä¸€ç§å†™æ³•æ—¶ï¼Œä¼šè‡ªåŠ¨å°†å…¶ç†è§£ä¸ºç¬¬äºŒç§å†™æ³•ã€‚

* Newer resources, such as Job, Deployment, ReplicaSet, and DaemonSet, support set-based requirements as well.

  > ```yaml
  > selector:
  >   matchLabels:
  >     component: redis
  >   matchExpressions:
  >     - { key: tier, operator: In, values: [cache] }
  >     - { key: environment, operator: NotIn, values: [dev] }
  > 
  > ```

* set-based requirements åº”ç”¨ç”¨å¼•å·åŒ…èµ·æ¥

  > kubectl get pods -l 'environment in (production),tier in (frontend)'

* kubectl get pods -l environment=production,tier=frontend

* kubectl get pods -Lapp -Ltier -Lrole

  > â€˜-Lâ€™ å‚æ•°ä¸æ˜¯è¿‡æ»¤ä½œç”¨ï¼Œè€Œæ˜¯åœ¨æœ€ç»ˆçš„æŸ¥è¯¢ç»“æžœä¸­ä»¥åˆ—çš„å½¢å¼æ˜¾ç¤º

* æ›´æ–°label

  > ```shell
  > kubectl label pods -l app=nginx tier=fe
  > ```
  >
  > This first filters all pods with the label "app=nginx", and then labels them with the "tier=fe"ã€‚é™¤äº†ç”¨-l app=nginxæ ‡ç­¾æ¥è¿‡æ»¤ï¼Œè¿˜å¯ä»¥ç”¨podçš„åå­—æ¥è¿‡æ»¤éœ€è¦æ“ä½œçš„pods
  >
  > é»˜è®¤æƒ…å†µä¸‹ï¼Œå½“å·²ç»å­˜åœ¨tieræ ‡ç­¾æ—¶ï¼Œä¸ä¼šæ›´æ–°æˆåŠŸã€‚å¯ä»¥åŠ å…¥`kubectl label --overwrite pods`è¿™ä¸ªå‚æ•°

## Namespace


* Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.)

* For a production cluster, consider not using the default namespace. Instead, make other namespaces and use those.

* Kubernetes starts with four initial namespaces:

  > 1. default
  > 2. kube-node-lease
  > 3. kube-public
  > 4. kube-system

* Avoid creating namespaces with the prefix kube-, since it is reserved for Kubernetes system namespaces.

* kubectl get namespace

* To set the namespace for a current request, use the --namespace flag.

  > ```shell
  > kubectl run nginx --image=nginx --namespace=[insert-namespace-name-here]
  > kubectl get pods --namespace=[insert-namespace-name-here]
  > ```

* You can permanently save the namespace for all subsequent kubectl commands in that context.

  > ```shell
  > kubectl config set-context --current --namespace=[insert-namespace-name-here]
  > # Validate it
  > kubectl config view --minify | grep namespace:
  > ```

* Not all objects are in a namespace

  > However namespace resources are not themselves in a namespace. And low-level resources, such as nodes and persistentVolumes, are not in any namespace.
  >
  > ```shell
  > # In a namespace
  > kubectl api-resources --namespaced=true
  > 
  > # Not in a namespace
  > kubectl api-resources --namespaced=false
  > ```

* The Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on all namespaces. The value of the label is the namespace name

  > kubectl describe namespaces kube-system

* The keys and the values in the map must be strings. In other words, you cannot use numeric, boolean, list or other types for either the keys or the values.

## Annotations


* Annotations are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (/). 

  > The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (`[a-z0-9A-Z]`) with dashes (`-`), underscores (`_`), dots (`.`), and alphanumerics between.

* Shared labels and annotations share a common prefix: app.kubernetes.io. Labels without a prefix are private to users. The shared prefix ensures that shared labels do not interfere with custom user labels.

  > **Shared Labels** æ˜¯ä¸€å¥— **å®˜æ–¹æŽ¨èçš„ã€æ ‡å‡†åŒ–çš„æ ‡ç­¾**ã€‚å®ƒä»¬ä½¿ç”¨ `app.kubernetes.io/` è¿™ä¸ªç»Ÿä¸€çš„å‰ç¼€ï¼Œç›®çš„æ˜¯ä¸ºäº†è®©ä¸åŒçš„å·¥å…·ã€å›¢é˜Ÿå’Œç”¨æˆ·èƒ½å¤Ÿç”¨ä¸€ç§é€šç”¨çš„æ–¹å¼æ¥æè¿°å’Œè¯†åˆ«åœ¨ Kubernetes ä¸­è¿è¡Œçš„åº”ç”¨ç¨‹åº

* The metadata is organized around the concept of an application. Kubernetes is not a platform as a service (PaaS) and doesn't have or enforce a formal notion of an application. Instead, applications are informal and described with metadata. The definition of what an application contains is loose.

## Field selectors


* Field selectors are essentially resource filters. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the kubectl queries kubectl get pods and kubectl get pods --field-selector "" equivalent.

* You can use the =, ==, and != operators with field selectors (= and == mean the same thing). 

  > kubectl get services --all-namespaces --field-selector metadata.namespace!=default

* As with label and other selectors, field selectors can be chained together as a comma-separated list. 

  > kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always

## Finalizer


* Finalizer æ˜¯ä¸€ä¸ªå­˜åœ¨äºŽèµ„æºå¯¹è±¡ metadata ä¸­çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚

* è¿™ä¸ª Finalizer ç¡®ä¿äº†å½“ä½ åˆ é™¤è¿™ä¸ª Service æ—¶ï¼ŒKubernetes ä¼šå…ˆè°ƒç”¨äº‘å¹³å°çš„ API åŽ»åˆ é™¤é‚£ä¸ªçœŸå®žçš„ã€ä¼šäº§ç”Ÿè´¹ç”¨çš„è´Ÿè½½å‡è¡¡å™¨ï¼Œç„¶åŽå†åˆ é™¤ Service å¯¹è±¡æœ¬èº«ã€‚å¦‚æžœæ²¡æœ‰è¿™ä¸ªæœºåˆ¶ï¼Œä½ å¯èƒ½ä¼šç•™ä¸‹å¾ˆå¤šæ— äººç®¡ç†çš„â€œåƒµå°¸â€äº‘èµ„æºã€‚

  > **Finalizer**: `service.kubernetes.io/load-balancer-cleanup` (åœ¨ä¸€äº›äº‘åŽ‚å•†çš„å®žçŽ°ä¸­)

* ä¸ºä»€ä¹ˆèµ„æºä¼šå¡åœ¨ Terminating çŠ¶æ€ï¼ŸðŸš¨
  è¿™æ˜¯ä½ åœ¨å®žè·µä¸­ä¸€å®šä¼šé‡åˆ°çš„ç»å…¸é—®é¢˜ã€‚å½“ä¸€ä¸ªèµ„æºé•¿æ—¶é—´å¤„äºŽ Terminating çŠ¶æ€æ—¶ï¼Œå‡ ä¹Ž 100% æ˜¯ Finalizer å¯¼è‡´çš„ã€‚

  > **åŽŸå› **ï¼šè´Ÿè´£æ¸…ç†å¹¶ç§»é™¤é‚£ä¸ª Finalizer çš„æŽ§åˆ¶å™¨**æ— æ³•å®Œæˆå®ƒçš„å·¥ä½œ**ã€‚

* Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.

  > **Marked for deletion (æ ‡è®°ä¸ºåˆ é™¤)**: èµ„æºæœ‰äº† `deletionTimestamp`ï¼Œå¤„äºŽ `Terminating` çŠ¶æ€ã€‚å®ƒå¯¹å¤–å·²ç»â€œæ­»äº¡â€ï¼ˆæ¯”å¦‚ Pod ä¸å†æŽ¥æ”¶æµé‡ï¼‰ï¼Œä½†å®ƒçš„â€œå°¸ä½“â€ï¼ˆåœ¨ etcd ä¸­çš„è®°å½•ï¼‰è¿˜åœ¨ã€‚
  >
  > **Fully deleted (å½»åº•åˆ é™¤)**: èµ„æºçš„è®°å½•ä»Ž etcd ä¸­è¢«å½»åº•æŠ¹é™¤ï¼Œå®ƒä¸å¤å­˜åœ¨äº†ã€‚
  >
  > **Specific conditions are met (ç‰¹å®šæ¡ä»¶è¢«æ»¡è¶³)**: è¿™ä¸ªâ€œç‰¹å®šæ¡ä»¶â€éžå¸¸æ˜Žç¡®ï¼Œ**æŒ‡çš„å°±æ˜¯** `metadata.finalizers` **åˆ—è¡¨å˜ä¸ºç©º**ã€‚
  >
  > é‚£ä¹ˆè°æ¥æ¸…ç©ºè¿™ä¸ªåˆ—è¡¨å‘¢ï¼Ÿç­”æ¡ˆæ˜¯**æŽ§åˆ¶å™¨ (Controller)**ã€‚
  >
  > - æ¯ä¸ª Finalizer å­—ç¬¦ä¸²éƒ½å¯¹åº”ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„æŽ§åˆ¶å™¨ã€‚
  > - è¿™ä¸ªæŽ§åˆ¶å™¨ä¸€ç›´åœ¨ç›‘æŽ§ï¼Œå½“å®ƒå‘çŽ°è‡ªå·±è´Ÿè´£çš„èµ„æºå‡ºçŽ°äº† `deletionTimestamp` æ—¶ï¼Œå®ƒå°±çŸ¥é“è¯¥å¹²æ´»äº†ï¼ˆæ‰§è¡Œæ¸…ç†ä»»åŠ¡ï¼‰ã€‚
  > - æ¸…ç†ä»»åŠ¡å®ŒæˆåŽï¼ˆæ¯”å¦‚äº‘ç¡¬ç›˜è¢«åˆ äº†ï¼Œæ•°æ®åº“å¤‡ä»½å¥½äº†ï¼‰ï¼ŒæŽ§åˆ¶å™¨å°±ä¼šå‘èµ·ä¸€ä¸ª API è¯·æ±‚ï¼ŒæŠŠè‡ªå·±è´Ÿè´£çš„é‚£ä¸ª Finalizer å­—ç¬¦ä¸²ä»Žåˆ—è¡¨ä¸­**ç§»é™¤**ã€‚
  > - å½“æ‰€æœ‰æŽ§åˆ¶å™¨éƒ½å®Œæˆäº†è‡ªå·±çš„ä»»åŠ¡ï¼Œ`finalizers` åˆ—è¡¨å°±å˜ç©ºäº†ã€‚
  >
  > å®ƒå®žé™…ä¸Šæ˜¯ä¸€ä¸ª**å­—ç¬¦ä¸²**ã€‚è¿™äº›å­—ç¬¦ä¸²å­˜åœ¨äºŽä¸€ä¸ªåˆ—è¡¨é‡Œï¼Œä½ç½®åœ¨ `metadata.finalizers`
  >
  > å®ƒä»¬åƒå¸¦æœ‰åç©ºé—´çš„é”®ä¸€æ ·ï¼Œæ˜¯ç‹¬ä¸€æ— äºŒçš„æ ‡è¯†ç¬¦
  >
  > ç®€å•æ¥è¯´ï¼šä½ å¯ä»¥æŠŠå®ƒç†è§£ä¸ºâ€œå¸¦æœ‰å”¯ä¸€å‰ç¼€çš„ç‰¹æ®Šæ ‡ç­¾â€ã€‚
  >
  > ```yaml
  > metadata:
  >   finalizers:
  >   - kubernetes.io/pv-protection  # ä¸€ä¸ªéµå¾ª "namespaced key" æ ¼å¼çš„å­—ç¬¦ä¸²
  >   - another.tool.com/do-backup    # å¦ä¸€ä¸ªéµå¾ªåŒæ ·æ ¼å¼çš„å­—ç¬¦ä¸²
  > ```

* Custom finalizer names must be publicly qualified finalizer names, such as example.com/finalizer-name. Kubernetes enforces this format; the API server rejects writes to objects where the change does not use qualified finalizer names for any custom finalizer.

* Dependent objects also have an ownerReferences.blockOwnerDeletion field that takes a boolean value and controls whether specific dependents can block garbage collection from deleting their owner object. Kubernetes automatically sets this field to true if a controller (for example, the Deployment controller) sets the value of the metadata.ownerReferences field. You can also set the value of the blockOwnerDeletion field manually to control which dependents block garbage collection.

  > å…³ç³»é“¾: Deployment -> ReplicaSet -> Podã€‚
  >
  > åˆ é™¤é“¾: åˆ é™¤ Deployment -> åˆ é™¤ ReplicaSet -> åˆ é™¤ Podã€‚
  >
  > blockOwnerDeletion: true: æ˜¯ä¸€ä¸ª â€œåˆ¹è½¦â€ã€‚Dependent å¯¹è±¡å¯¹ Owner è¯´ï¼šâ€œåˆ«åˆ æˆ‘è€æ¿ï¼Œé™¤éžæˆ‘å…ˆèµ°ï¼â€
  >
  > kubectl delete deployment è§¦å‘çš„æ˜¯ä¸€ä¸ªâ€œæœ‰åºè§£æ•£â€ï¼Œè€Œéžâ€œæ–©é¦–è¡ŒåŠ¨ã€ç›´æŽ¥åˆ é™¤deploymentã€‘â€

* In foreground deletion, it adds the foreground finalizer so that the controller must delete dependent resources that also have ownerReferences.blockOwnerDeletion=true before it deletes the owner.

* kubectl delete deployment my-app --cascade=orphan

  > **ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**
  >
  > 1. `Deployment` **å¯¹è±¡è¢«ç«‹å³åˆ é™¤**ï¼š`my-app` è¿™ä¸ª `Deployment` èµ„æºçž¬é—´å°±æ¶ˆå¤±äº†ã€‚
  > 2. `ReplicaSet` **å’Œ** `Pod` **å®Œå¥½æ— æŸ**ï¼šä½ ä¼šæƒŠè®¶åœ°å‘çŽ°ï¼Œ`ReplicaSet` å’Œæ‰€æœ‰çš„ `Pod` ä¾ç„¶åœ¨è¿è¡Œï¼
  > 3. `ReplicaSet` **æˆä¸ºå­¤å„¿**ï¼šå¦‚æžœä½ æŸ¥çœ‹é‚£ä¸ªå¹¸å­˜çš„ `ReplicaSet` çš„ YAML (`kubectl get rs [rs-name] -o yaml`)ï¼Œä½ ä¼šå‘çŽ°å®ƒ `metadata` é‡Œçš„ `ownerReferences` å­—æ®µ**å·²ç»ä¸è§äº†**ã€‚å®ƒä¸å†å±žäºŽä»»ä½•äººï¼Œå˜æˆäº†ä¸€ä¸ªç‹¬ç«‹çš„ã€æ²¡äººç®¡ç†çš„ `ReplicaSet`ã€‚

## K3d è¿è¡Œï¼ˆ **PLG æ ˆ**: Promtail + Loki + Grafanaï¼‰

1. å‡†å¤‡å·¥ä½œ: å®‰è£…helm

   ```shell
   sudo snap install helm --classic
   ```

2. ç¬¬ä¸€æ­¥ï¼šç†è§£æž¶æž„

   åœ¨åŠ¨æ‰‹ä¹‹å‰ï¼Œå…ˆçœ‹ä¸€çœ¼æˆ‘ä»¬å°†è¦æ­å»ºçš„æž¶æž„ï¼š

   1. **Promtail (æ¬è¿å·¥)**ï¼šå®ƒä»¥ **DaemonSet** çš„å½¢å¼è¿è¡Œï¼Œæ„å‘³ç€ä½  k3d çš„æ¯ä¸€ä¸ªâ€œèŠ‚ç‚¹å®¹å™¨â€é‡Œéƒ½ä¼šè‡ªåŠ¨è¿è¡Œä¸€ä¸ª Promtailã€‚å®ƒè´Ÿè´£åŽ» `/var/log/pods`ï¼ˆå°±æ˜¯ä½ åˆšæ‰è¿›åŽ»çš„é‚£ä¸ªç›®å½•ï¼‰æŠ“å–æ—¥å¿—ï¼Œå¹¶æ‰“ä¸Šæ ‡ç­¾ï¼ˆPodåã€Namespaceç­‰ï¼‰ã€‚
   2. **Loki (ä»“åº“)**ï¼šå®ƒæ˜¯æ ¸å¿ƒå­˜å‚¨ï¼Œè´Ÿè´£æŽ¥æ”¶ Promtail å‘æ¥çš„æµï¼Œå¹¶è¿›è¡ŒåŽ‹ç¼©å­˜å‚¨ã€‚
   3. **Grafana (ä»ªè¡¨ç›˜)**ï¼šå¯è§†åŒ–çš„ Web ç•Œé¢ï¼Œæˆ‘ä»¬åœ¨è¿™é‡ŒæŸ¥è¯¢å’Œçœ‹å›¾ã€‚

3. ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Helm å®‰è£… Loki-Stack
   ä¸ºäº†ç®€åŒ–æµç¨‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å®˜æ–¹çš„ loki-stack Chartï¼Œå®ƒä¼šæŠŠä¸Šé¢ä¸‰ä¸ªç»„ä»¶æ‰“åŒ…ä¸€èµ·è£…å¥½ã€‚

   **1. æ·»åŠ  Grafana ä»“åº“** åœ¨ä½ çš„ç»ˆç«¯æ‰§è¡Œï¼š

   ```shell
   helm repo add grafana https://grafana.github.io/helm-charts
   helm repo update
   ```

   **2. åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ Namespace** æŠŠç›‘æŽ§ç›¸å…³çš„èµ„æºéš”ç¦»å‡ºæ¥æ˜¯ä¸ªå¥½ä¹ æƒ¯ï¼š

   ```shell
   kubectl create namespace logging
   ```

   **3. å®‰è£… PLG æ ˆ** æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ã€‚ *æ³¨æ„ï¼šæˆ‘ä»¬æ˜¾å¼å¼€å¯äº† Grafanaï¼Œå› ä¸ºè¿™ä¸ª Chart é»˜è®¤å¯èƒ½ä¸å®‰è£…å®ƒã€‚*

   ```shell
   helm upgrade --install loki grafana/loki-stack \
     --namespace logging \
     --set grafana.enabled=true \
     --set promtail.enabled=true \
     --set loki.persistence.enabled=false
   ```

   *(æ³¨ï¼šä¸ºäº† k3d å®žéªŒæ–¹ä¾¿ï¼Œæˆ‘å…³é—­äº† persistence æŒä¹…åŒ–å­˜å‚¨ã€‚å¦‚æžœä½ é‡å¯ k3d é›†ç¾¤ï¼ŒLoki é‡Œçš„æ—§æ—¥å¿—ä¼šä¸¢å¤±ï¼Œä½†å¯¹å®žéªŒæ¥è¯´è¶³å¤Ÿäº†ä¸”æ›´è½»é‡ã€‚)*

4. ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å®‰è£…

   ç­‰å¾…å‡ åˆ†é’Ÿï¼ŒæŸ¥çœ‹ Pod æ˜¯å¦éƒ½è·‘èµ·æ¥äº†ï¼š

   ```shell
   kubectl get pods -n logging
   ```

   ä½ åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„åˆ—è¡¨ï¼š

   - `loki-0` (æˆ–è€… `loki-promtail-...`)
   - `loki-grafana-...`

   å¦‚æžœçŠ¶æ€éƒ½æ˜¯ `Running`ï¼Œæ­å–œä½ ï¼Œç³»ç»Ÿå·²ç»ç”±å®ˆè½¬æ”»äº†ï¼

5. ç¬¬å››æ­¥ï¼šèŽ·å– Grafana å¯†ç 

   Grafana é»˜è®¤ç”Ÿæˆçš„ `admin` å¯†ç ä¿å­˜åœ¨ K8s çš„ Secret é‡Œã€‚æˆ‘ä»¬éœ€è¦æŠŠå®ƒè§£å¯†å‡ºæ¥ã€‚

   æ‰§è¡Œè¿™æ¡å‘½ä»¤ï¼ˆè¿™æ˜¯è¿ç»´äººå‘˜å¿…å¤‡çš„â€œé»‘å®¢â€æŠ€èƒ½ï¼‰ï¼š

   ```shell
   # èŽ·å– secretï¼Œæå– admin-password å­—æ®µï¼Œå¹¶ base64 è§£ç 
   kubectl get secret --namespace logging loki-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
   ```

   ç»ˆç«¯ä¼šè¾“å‡ºä¸€ä¸²å­—ç¬¦ï¼Œ**å¤åˆ¶å®ƒ**ï¼Œè¿™å°±æ˜¯ä½ çš„ç™»å½•å¯†ç ã€‚

6. ç¬¬äº”æ­¥ï¼šè®¿é—® Grafana (Port Forwarding)

   å› ä¸ºä½ åœ¨ k3d é‡Œé¢ï¼Œå¤–éƒ¨æ— æ³•ç›´æŽ¥è®¿é—® ClusterIPã€‚æˆ‘ä»¬éœ€è¦ç”¨ `port-forward` æŠŠ Grafana çš„ç«¯å£æ˜ å°„åˆ°ä½ ç‰©ç†æœºçš„ `localhost`

   ```shell
   # å°† k8s å†…éƒ¨çš„ 80 ç«¯å£æ˜ å°„åˆ°ä½ ç”µè„‘çš„ 3000 ç«¯å£
   kubectl port-forward --namespace logging service/loki-grafana 3000:80
   ```

   *æ³¨æ„ï¼šè¿™ä¸ªå‘½ä»¤ä¼šå ç”¨ç»ˆç«¯ï¼Œä¸è¦å…³é—­å®ƒã€‚*

7. ç¬¬å…­æ­¥ï¼šè§è¯å¥‡è¿¹çš„æ—¶åˆ»

   1. æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—®ï¼š`http://localhost:3000`
   2. **ç”¨æˆ·å**ï¼š`admin`
   3. **å¯†ç **ï¼šåˆšæ‰å¤åˆ¶çš„é‚£ä¸²å­—ç¬¦ã€‚
   4. è¿›å…¥é¦–é¡µåŽï¼Œç‚¹å‡»å·¦ä¾§èœå•çš„ **"Explore" (æŒ‡å—é’ˆå›¾æ ‡)**ã€‚
   5. åœ¨é¡¶éƒ¨çš„ä¸‹æ‹‰æ¡†ä¸­ï¼Œç¡®ä¿é€‰æ‹©äº† **"Loki"** ä½œä¸ºæ•°æ®æºã€‚

   **çŽ°åœ¨ï¼Œæˆ‘ä»¬æ¥æŸ¥ä½ çš„ Spring Boot æ—¥å¿—ï¼**

8. é«˜çº§æŸ¥è¯¢

   ```shell
   ## ~ ä¸ºæ­£åˆ™
   {namespace="default"} | json | level="ERROR"
   {namespace="default"} | json | message != "exception"
   {namespace="default"} | json | message !~ "(?i).*exception.*"
   {namespace="default"} | json | message = "exception"
   {namespace="default"} | json | message =~ "(?i).*exception.*"
   {namespace="default"} |=
   {namespace="default"} |~ 
   {namespace="default"} !=
   {namespace="default"} !~
   
   ```

   


## å…¶å®ƒ

* `kind: Ingress` ä¼šæš´éœ²ä¸€ä¸ª IP åœ°å€å—ï¼Ÿ

  > ä¸ä¼šï¼Œ`Ingress` èµ„æºæœ¬èº«ä¸ä¼šã€‚ä¸€ä¸ª `kind: Ingress` çš„ YAML æ–‡ä»¶ï¼Œå®ƒä»…ä»…æ˜¯**ä¸€å¥—è§„åˆ™**çš„é›†åˆï¼Œå°±åƒä¸€å¼ å†™ç€â€œ`a.com` çš„æµé‡è¯·èµ°Aé—¨ï¼Œ`b.com` çš„æµé‡è¯·èµ°Bé—¨â€çš„è¯´æ˜Žä¹¦ã€‚è¿™å¼ è¯´æ˜Žä¹¦æœ¬èº«å¹¶æ²¡æœ‰åœ°å€ï¼Œå®ƒéœ€è¦è¢«äººï¼ˆä¹Ÿå°±æ˜¯ `Ingress Controller`ï¼‰åŽ»é˜…è¯»å’Œæ‰§è¡Œã€‚
  >
  > çœŸæ­£æš´éœ² IP åœ°å€çš„ï¼Œæ˜¯ `Ingress Controller` çš„ `Service`ï¼
  >
  > å›žé¡¾ä¸€ä¸‹ `Ingress Controller` æ˜¯å¦‚ä½•è¢«å®‰è£…çš„ï¼š
  >
  > 1. **Ingress Controller æ˜¯ä¸€ä¸ªéœ€è¦è¢«**`å®‰è£…`**åˆ°é›†ç¾¤ä¸­çš„åº”ç”¨**ï¼Œå®ƒä¸æ˜¯ K8s è‡ªå¸¦çš„ã€‚
  > 2. **å®‰è£…çš„æœ¬è´¨**æ˜¯åº”ç”¨ä¸€å¥—åŒ…å«äº† `Deployment`ã€`Service`ã€`RBAC` ç­‰èµ„æºçš„ YAML æ–‡ä»¶ã€‚
  > 3. **K3s ç”¨æˆ·æœ€å¹¸ç¦**ï¼Œå› ä¸º K3s å·²ç»å†…ç½®äº† **Traefik**ï¼Œæ— éœ€æ‰‹åŠ¨å®‰è£…ã€‚ä½ åªéœ€è¦ç›´æŽ¥åœ¨ `Ingress` ä¸­ä½¿ç”¨ `ingressClassName: "traefik"` å³å¯ã€‚
  > 4. åœ¨**æ ‡å‡† K8s çŽ¯å¢ƒ**ä¸­ï¼Œæœ€å¸¸ç”¨çš„é€‰æ‹©æ˜¯ **NGINX Ingress Controller**ï¼Œå¯ä»¥é€šè¿‡å®˜æ–¹ `kubectl apply` å‘½ä»¤æˆ– `Helm` Chart æ¥å®‰è£…ã€‚
  > 5. **`IngressClass` èµ„æºæ˜¯åœ¨å®‰è£… Controller çš„è¿‡ç¨‹ä¸­è¢«è‡ªåŠ¨åˆ›å»ºçš„**ã€‚å®ƒåƒä¸€ä¸ªâ€œå‘Šç¤ºç‰Œâ€ï¼Œå‘Šè¯‰æ•´ä¸ªé›†ç¾¤ï¼šâ€œå˜¿ï¼Œæˆ‘è¿™é‡Œæœ‰ä¸€ä¸ªåä¸º `nginx` (æˆ– `traefik`) çš„ Controllerï¼Œä½ ä»¬è°éœ€è¦å¤„ç† Ingress è§„åˆ™ï¼Œå°±é€šè¿‡ `ingressClassName` æ¥æ‰¾æˆ‘ï¼â€
  >
  > çŽ°åœ¨ï¼Œ`Ingress` (è§„åˆ™)ã€`Ingress Controller` (æ‰§è¡Œè€…) å’Œ `IngressClass` (è”ç³»æ–¹å¼) è¿™ä¸‰è€…ä¹‹
  >
  > å®ƒé€šå¸¸åŒ…å«ä¸€ä¸ª `Deployment` (è¿è¡Œ Controller çš„ Pods) å’Œä¸€ä¸ª `Service` (æŠŠè¿™äº› Pods æš´éœ²å‡ºåŽ»)ã€‚è¿™ä¸ª `Service` çš„ç±»åž‹é€šå¸¸æ˜¯ `LoadBalancer` æˆ– `NodePort`ã€‚
  >
  > - å½“ `Service` çš„ç±»åž‹æ˜¯ `LoadBalancer` æ—¶ï¼Œäº‘æœåŠ¡å•†ä¼šä¸º**è¿™ä¸ª Service**åˆ†é…ä¸€ä¸ª**å…¬ç½‘ IP åœ°å€**ã€‚
  > - å½“ `Service` çš„ç±»åž‹æ˜¯ `NodePort` æ—¶ï¼Œä½ å¯ä»¥é€šè¿‡**ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹çš„ IP** + `NodePort` ç«¯å£æ¥è®¿é—®ã€‚
  >
  > é‚£ä¸ªå®è´µçš„ã€å”¯ä¸€çš„ã€å¯¹å¤–æœåŠ¡çš„å…¬ç½‘ IP åœ°å€ï¼Œæ˜¯å±žäºŽ **Ingress Controller çš„ Service** çš„ï¼Œè€Œä¸æ˜¯å±žäºŽä½ åˆ›å»ºçš„æŸä¸€ä¸ª `Ingress` è§„åˆ™å¯¹è±¡çš„ã€‚
  >
  > ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹åˆ°è¿™ä¸ª IP åœ°å€ï¼š
  >
  > ```shell
  > # æŸ¥çœ‹ Ingress Controller çš„ Service
  > # æ³¨æ„å‘½åç©ºé—´ï¼Œå¦‚æžœä½ æ˜¯ç”¨ helm è£…çš„ nginx-ingressï¼Œé‚£å°±åœ¨ ingress-nginx å‘½åç©ºé—´
  > # å¦‚æžœæ˜¯ k3s è‡ªå¸¦çš„ traefikï¼Œé‚£å°±åœ¨ kube-system
  > kubectl get service -n ingress-nginx 
  > 
  > # ä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡º
  > # NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                      AGE
  > # ingress-nginx-controller   LoadBalancer   10.43.151.108   203.0.113.55     80:32168/TCP,443:30256/TCP   10m
  > #                                                          ^^^^^^^^^^^^
  > # å°±æ˜¯è¿™ä¸ª IPï¼
  > ```


* æ•…éšœæŽ’æŸ¥ä¸‰æ­¥æ³•

  > ç¬¬ 1 æ­¥ï¼šç¡®è®¤ Pods æ˜¯å¦å¥åº·è¿è¡Œ
  >
  > ```shell
  > kubectl get pods -o wide
  > ```
  >
  > ç¬¬ 2 æ­¥ï¼šç¡®è®¤ Service æ˜¯å¦æ­£ç¡®å…³è”äº† Pods
  >
  > ```shell
  > kubectl describe service nginx-service
  > ```
  >
  > **æ­£ç¡®çš„çŠ¶æ€**: `Endpoints` åŽé¢åº”è¯¥åˆ—å‡ºäº†ä¸€ä¸ªæˆ–å¤šä¸ª IP åœ°å€å’Œç«¯å£ï¼Œè¿™äº› IP åº”è¯¥ä¸Žä½ åœ¨ä¸Šä¸€æ­¥ä¸­çœ‹åˆ°çš„ Pod IP å®Œå…¨ä¸€è‡´ã€‚`Endpoints:         10.42.0.5:80,10.42.0.6:80`
  >
  > ç¬¬ 3 æ­¥ï¼šç¡®è®¤ k3d èŠ‚ç‚¹çš„ç«¯å£æ˜ å°„ (æœ€å¯èƒ½çš„åŽŸå› )

* æœ€å¿«ã€æœ€ç›´æŽ¥çš„ç»•è¿‡ç½‘ç»œé—®é¢˜çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨ä¸Šæ¬¡è®¨è®ºä¸­ä¹Ÿæåˆ°äº†ã€‚å®ƒä¸ä¾èµ–ä»»ä½•ç«¯å£æ˜ å°„ï¼Œè€Œæ˜¯ç›´æŽ¥åœ¨ä½ çš„ç”µè„‘å’Œ Service ä¹‹é—´å»ºç«‹ä¸€æ¡éš§é“ã€‚

  > ```shell
  > # åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ
  > kubectl port-forward service/nginx-service 8080:80
  > ```

* MetalLB (å¼ºçƒˆæŽ¨è) è¿™æ˜¯åœ¨è‡ªå»ºé›†ç¾¤ï¼ˆBare-Metalï¼‰ä¸­å®žçŽ° type: LoadBalancer çš„æœ€ä½³å®žè·µæ–¹æ¡ˆã€‚MetalLB æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå®ƒèƒ½ä¸ºä½ çš„é›†ç¾¤æ¨¡æ‹Ÿäº‘æœåŠ¡å•†çš„è´Ÿè½½å‡è¡¡å™¨åŠŸèƒ½ã€‚

* ä½¿ç”¨ kubectl explain å‘½ä»¤ï¼šè¿™æ˜¯ä¸€ä¸ªéžå¸¸æœ‰ç”¨çš„å‘½ä»¤ï¼Œå¯ä»¥å¸®åŠ©ä½ äº†è§£ä»»ä½• Kubernetes èµ„æºçš„ç»“æž„å’Œå­—æ®µã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä½ æƒ³çŸ¥é“ Deployment çš„ apiVersion

  > å¦‚ 
  >
  > ```shell
  > kubectl explain Deployment
  > # GROUP:      apps
  > # KIND:       Deployment
  > # VERSION:    v1
  > 
  > ```
  >
  > æ‰€ä»¥å®šä¹‰Deploymentæ—¶ä¸ºï¼š
  >
  > ```
  > apiVersion: apps/v1
  > kind: Deployment
  > ```

* ä¸€ä¸ªå®Œæ•´çš„åº”ç”¨[ç³»ç»Ÿ]ï¼Œä¸€èˆ¬åªæœ‰ä¸€ä¸ªtypeä¸ºloadbalancerçš„service?

  > * å¯¹äºŽä¸€ä¸ªå®Œæ•´çš„ã€çŽ°ä»£åŒ–çš„åº”ç”¨ç³»ç»Ÿï¼ˆç‰¹åˆ«æ˜¯åŸºäºŽå¾®æœåŠ¡æž¶æž„çš„ Web åº”ç”¨ï¼‰ï¼Œé€šå¸¸æœ€ä½³å®žè·µå°±æ˜¯åªä½¿ç”¨ä¸€ä¸ª Type=LoadBalancer çš„ Serviceã€‚æ ‡å‡†çš„åº”ç”¨æš´éœ²æž¶æž„ï¼šâ€œLoadBalancer + Ingress Controllerâ€
  >
  > * å¦‚æžœä½ çš„åº”ç”¨ç³»ç»ŸåŒ…å«ä¸€äº›éž HTTP/HTTPS çš„æœåŠ¡ï¼Œæ¯”å¦‚ï¼š
  >
  >   * ä¸€ä¸ªéœ€è¦ç›´æŽ¥æš´éœ²ç»™å¤–éƒ¨å®¢æˆ·ç«¯çš„ æ•°æ®åº“ (å¦‚ PostgreSQL)ã€‚
  >   * ä¸€ä¸ª MQTT æ¶ˆæ¯ä»£ç†æœåŠ¡ã€‚
  >   * ä¸€ä¸ª SFTP æ–‡ä»¶æœåŠ¡ã€‚
  >
  >   è¿™äº›æœåŠ¡å·¥ä½œåœ¨ TCP/UDP å±‚ï¼ŒIngress Controllerï¼ˆé€šå¸¸ä¸º HTTP è®¾è®¡ï¼‰æ— æ³•å¤„ç†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ºè¿™äº›ç‰¹å®šçš„æœåŠ¡å†é¢å¤–åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ Type=LoadBalancer Service æ˜¯å®Œå…¨åˆç†çš„ã€‚

* In case of a Node, it is implicitly assumed that an instance using the same name will have the same state (e.g.network settings, root disk contents) and attributes like node labels.

  > è¿™é‡Œçš„instanceæ˜¯æŒ‡è™šæ‹Ÿæœºæˆ–è€…ç‰©ç†æœºã€‚ Kubernetes
  > è®¤â€˜åâ€™ä¸è®¤â€˜äººâ€™ã€‚å®ƒæŠŠèŠ‚ç‚¹åç§°å½“ä½œèº«ä»½è¯å·ã€‚å¦‚æžœä¸€ä¸ªæ–°äººæ‹¿äº†æ—§äººçš„èº«ä»½è¯å·æ¥æŠ¥åˆ°ï¼Œç³»ç»Ÿä¼šæŠŠä»–å½“æˆæ—§äººï¼Œä½†è¿™ä¸ªæ–°äººçš„èƒ½åŠ›å’ŒèƒŒæ™¯ï¼ˆç£ç›˜å†…å®¹ã€ç¡¬ä»¶å±žæ€§ï¼‰æ˜¯å…¨æ–°çš„ã€‚è¿™ç§èº«ä»½ä¸Žå®žé™…èƒ½åŠ›çš„ä¸åŒ¹é…ï¼Œæ­£æ˜¯å¾ˆå¤šè¯¡å¼‚é—®é¢˜çš„æ ¹æºã€‚è¯·åŠ¡å¿…ç¡®ä¿åœ¨æ›¿æ¢èŠ‚ç‚¹æ—¶ï¼Œå…ˆâ€˜æ³¨é”€â€™æ—§çš„èº«ä»½ä¿¡æ¯ï¼ˆkubectl
  > delete nodeï¼‰ï¼Œå†è®©æ–°äººç”¨è‡ªå·±çš„èº«ä»½æ³¨å†Œã€‚

* Register the node with the given list of taints

  > å¯ä»¥æŠŠ Taint (æ±¡ç‚¹) æƒ³è±¡æˆèŠ‚ç‚¹ï¼ˆNodeï¼‰ä¸Šçš„ä¸€ä¸ªâ€œæŽ’æ–¥æ ‡ç­¾â€æˆ–è€…â€œè°¢ç»å…¥å†…â€çš„ç‰Œå­ã€‚ ä¸€æ—¦ä¸€ä¸ªèŠ‚ç‚¹è¢«æ‰“ä¸Šäº†æŸä¸ª
  > Taintï¼ŒKubernetes çš„è°ƒåº¦å™¨ï¼ˆSchedulerï¼‰é»˜è®¤å°±ä¸ä¼šæŠŠä»»ä½• Pod è°ƒåº¦åˆ°è¿™ä¸ªèŠ‚ç‚¹ä¸Šã€‚è¿™å°±å¥½åƒä¸€ä¸ªæˆ¿é—´é—¨å£æŒ‚ç€â€œè¯·å‹¿æ‰“æ‰°â€çš„ç‰Œå­ï¼Œæ­£å¸¸æƒ…å†µä¸‹ï¼Œæ²¡æœ‰äººä¼šè¿›åŽ»ã€‚

## Container

### Container hooks

- PostStart

  However, if the `PostStart` hook takes too long to execute or if it hangs, it can prevent the container from transitioning to a `running` state.

- PreStop

  `PreStop` hooks are not executed asynchronously from the signal to stop the Container; the hook must complete its execution before the TERM signal can be sent. 

> **Hook å¤±è´¥çš„å½±å“**ï¼š
>
> - `postStart` Hook å¤±è´¥ï¼šå¦‚æžœ `postStart` Hook æ‰§è¡Œå¤±è´¥ï¼Œå®¹å™¨å°†æ— æ³•è¿›å…¥ `Running` çŠ¶æ€ï¼Œ`kubelet` ä¼šæ€æ­»å¹¶å°è¯•é‡å¯è¿™ä¸ªå®¹å™¨ï¼Œå¯¼è‡´ Pod è¿›å…¥ `CrashLoopBackOff` çŠ¶æ€ã€‚
> - `preStop` Hook å¤±è´¥ï¼š`preStop` Hook çš„å¤±è´¥ä¸ä¼šé˜»æ­¢å®¹å™¨çš„ç»ˆæ­¢ã€‚Kubernetes åœ¨å°è¯•æ‰§è¡Œ `preStop` Hook åŽï¼ˆæ— è®ºæˆåŠŸä¸Žå¦ï¼‰ï¼Œä»ç„¶ä¼šå‘å®¹å™¨çš„ä¸»è¿›ç¨‹å‘é€ `TERM` ä¿¡å·ã€‚
>
> å…¶å®žä¹Ÿå¯ä»¥æ€»ç»“ä¸ºè¿™ä¸¤ä¸ªhookåªè¦æœ‰ä¸€ä¸ªå¤±è´¥ï¼Œå®¹å™¨éƒ½ä¼šè¢«killed

### Hook handler implementations

- Exec

1. **æ‰§è¡ŒçŽ¯å¢ƒ**ï¼š`Exec` ç±»åž‹çš„ Hook Handler **å®Œå…¨åœ¨å®¹å™¨å†…éƒ¨æ‰§è¡Œ**ã€‚å®ƒå’Œä½ åœ¨å®¹å™¨å¯åŠ¨åŽä½¿ç”¨ `kubectl exec` æˆ– `docker exec` è¿›å…¥å®¹å™¨æ‰§è¡Œå‘½ä»¤çš„çŽ¯å¢ƒæ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚
2. **èµ„æºå½’å±ž**ï¼šå› æ­¤ï¼Œè¿™ä¸ªè„šæœ¬æˆ–å‘½ä»¤æ‰€æ¶ˆè€—çš„ **æ‰€æœ‰èµ„æºï¼ˆCPUã€å†…å­˜ç­‰ï¼‰éƒ½è®¡ç®—åœ¨è¯¥å®¹å™¨çš„è´¦ä¸Š**ã€‚å®ƒä¼šå—åˆ°ä¸ºè¯¥å®¹å™¨é…ç½®çš„ `resources.limits` å’Œ `resources.requests` çš„çº¦æŸã€‚

- HTTP
- Sleep

>  `httpGet`, `tcpSocket` ([deprecated](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.31/#lifecyclehandler-v1-core)) and `sleep` are executed by the kubelet process (è¯·æ±‚çš„æŽ¥æ”¶å’Œå¤„ç†å‘ç”Ÿåœ¨å®¹å™¨å†…ï¼Œå› æ­¤**å¤„ç†è¯¥è¯·æ±‚æ‰€æ¶ˆè€—çš„èµ„æºå½’å±žäºŽå®¹å™¨**), and `exec` is executed in the container.

é…ç½®preStop sample:

```yaml
spec:
  containers:
  - name: nginx
    image: nginx:1.25
    ports:
    - containerPort: 80
    lifecycle:
      preStop:
        ## æˆ–è€… httpGet:
        exec:   
        ....
```



### Hook delivery guarantees

è¿™äº›hookå¯èƒ½ä¼šè¿è¡Œå¤šæ¬¡ã€‚è¿™ä¸ªæ¦‚å¿µåœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­éžå¸¸å¸¸è§ï¼Œè¢«ç§°ä¸º **â€œè‡³å°‘ä¸€æ¬¡ (At-Least-Once)â€** æŠ•é€’è¯­ä¹‰

### Debugging Hook handlers

è¿™äº›Hook handlerså¦‚æžœæ‰§è¡Œå¤±è´¥ï¼Œå¯ä»¥æ‰§è¡Œåƒç±»ä¼¼çš„è¯­å¥æ¥æŸ¥çœ‹æ—¥å¿—`kubectl describe pod lifecycle-demo`

## Workloads

A workload is an application running on Kubernetes. Whether your workload is a single component or several that work together, on Kubernetes you run it inside a set of [*pods*](https://kubernetes.io/docs/concepts/workloads/pods/). In Kubernetes, a Pod represents a set of running [containers](https://kubernetes.io/docs/concepts/containers/) on your cluster.

several **built-in workload resources**:

* [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) and [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/) (replacing the legacy resource [ReplicationController](https://kubernetes.io/docs/reference/glossary/?all=true#term-replication-controller)).
* [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) 
* [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) defines Pods that provide facilities that are local to nodes. 
* [Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) and [CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/) provide different ways to define tasks that run to completion and then stop. You can use a [Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) to define a task that runs to completion, just once. You can use a [CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/) to run the same Job multiple times according a schedule.

In the wider Kubernetes ecosystem, you can find third-party workload resources that provide additional behaviors. Using a [custom resource definition](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/), you can add in a third-party workload resource if you want a specific behavior that's not part of Kubernetes' core. 

### Pods

*Pods* are the smallest deployable units of computing that you can create and manage in Kubernetes. ä½ å¯ä»¥æŠŠä¸€ä¸ª Pod æƒ³è±¡æˆä¸€å°ç‹¬ç«‹çš„â€œé€»è¾‘ä¸»æœºâ€æˆ–è™šæ‹Ÿæœºã€‚è¿™å°â€œä¸»æœºâ€æœ‰è‡ªå·±å”¯ä¸€çš„ IP åœ°å€ã€‚

Pods in a Kubernetes cluster are used in two main ways:

- **Pods that run a single container**. 

- **Pods that run multiple containers that need to work together**. 

  You should use this pattern only in specific instances in which your containers are tightly coupled. You don't need to run multiple containers to provide replication (for resilience or capacity);

#### Using Pods

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80
```

Pods are generally not created directly and are created using workload resources. Instead, create them using workload resources such as [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) or [Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/). If your Pods need to track state, consider the [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) resource.

#### Working with Pods

Pods are designed as relatively ephemeral, disposable entities. The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is *evicted* for lack of resources, or the node fails.

> Note:
>
> Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for running container(s). A Pod persists until it is deleted.

The name of a Pod must be a valid [DNS subdomain](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names) value, but this can produce unexpected results for the Pod hostname. For best compatibility, the name should follow the more restrictive rules for a [DNS label](https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names).

##### Pod OS

1. å¯¹ `.spec.os.name` å­—æ®µçš„ç†è§£

   åœ¨ Pod çš„ YAML ä¸­è®¾ç½® `.spec.os.name` å­—æ®µ**å¹¶ä¸ä¼šå½±å“ `kube-scheduler` (è°ƒåº¦å™¨) çš„å®žé™…è°ƒåº¦å†³ç­–**ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æœ‰ä¸¤ç‚¹ï¼š

   1. **å£°æ˜Žä¸Žè¯†åˆ« (Declaration & Identification)**: å®ƒæ˜¯ä¸€ä¸ªæ˜Žç¡®çš„å…ƒæ•°æ®å­—æ®µï¼Œç”¨æ¥**å£°æ˜Ž**è¿™ä¸ª Pod å†…çš„å®¹å™¨æ˜¯ä¸ºå“ªä¸ªæ“ä½œç³»ç»Ÿæž„å»ºçš„ï¼ˆç›®å‰æ˜¯ `linux` æˆ– `windows`ï¼‰ã€‚è¿™ä½¿å¾—é›†ç¾¤ä¸­çš„å…¶ä»–ç»„ä»¶æˆ–å·¥å…·ï¼ˆæ¯”å¦‚ç›‘æŽ§ç³»ç»Ÿã€å®‰å…¨ç­–ç•¥å·¥å…·ï¼‰èƒ½å¤Ÿè½»æ¾è¯†åˆ« Pod çš„æ“ä½œç³»ç»Ÿç±»åž‹ã€‚
   2. **ç­–ç•¥åº”ç”¨ (Policy Enforcement)**: å¦‚æ–‡æ¡£ä¸­æåˆ°çš„ï¼Œ`Pod Security Standards` (Pod å®‰å…¨æ ‡å‡†) ä¼šåˆ©ç”¨è¿™ä¸ªå­—æ®µã€‚ä¾‹å¦‚ï¼ŒæŸäº›å®‰å…¨ç­–ç•¥åªé€‚ç”¨äºŽ Linux çŽ¯å¢ƒï¼ˆæ¯”å¦‚ä¸Ž `seccomp` æˆ– `AppArmor` ç›¸å…³çš„ç­–ç•¥ï¼‰ï¼Œåœ¨ Windows èŠ‚ç‚¹ä¸Šå¼ºåˆ¶æ‰§è¡Œè¿™äº›ç­–ç•¥æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚é€šè¿‡è¯»å– `.spec.os.name`, ç³»ç»Ÿå¯ä»¥æ™ºèƒ½åœ°é¿å…åœ¨ä¸ç›¸å…³çš„æ“ä½œç³»ç»Ÿä¸Šåº”ç”¨è¿™äº›ç­–ç•¥ã€‚
   3. **é¢å‘æœªæ¥ (Future-proofing)**: ç¤¾åŒºå¯èƒ½åœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­èµ‹äºˆè¿™ä¸ªå­—æ®µæ›´å¤šçš„åŠŸèƒ½ï¼Œç”šè‡³å¯èƒ½ç›´æŽ¥å½±å“è°ƒåº¦ã€‚ä½†å°±ç›®å‰è€Œè¨€ï¼Œå®ƒæ›´å¤šçš„æ˜¯ä¸€ä¸ªæè¿°æ€§ã€ä¾›å…¶ä»–ç»„ä»¶æ¶ˆè´¹çš„å­—æ®µã€‚

2. `kubernetes.io/os` Label æ‰“åœ¨å“ªä¸ªèµ„æºä¸Šï¼Ÿ

   `kubernetes.io/os` æ˜¯ä¸€ä¸ª **Node Label** (èŠ‚ç‚¹æ ‡ç­¾)ã€‚å®ƒè¢«æ‰“åœ¨ **Node (èŠ‚ç‚¹)** èµ„æºä¸Šã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥æŸ¥çœ‹ä½ é›†ç¾¤ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„æ ‡ç­¾:

   ```shell
   kubectl get nodes --show-labels
   ```

   > `kubernetes.io/os` è¿™ä¸ªæ ‡ç­¾ä¸»è¦æ˜¯**ç”± `kubelet` è‡ªåŠ¨æ·»åŠ **çš„ã€‚
   >
   > kubectl describe node [your-node-name] çœ‹åˆ°ç›¸åº”çš„System Info

3. åˆ°åº•æ˜¯ä»€ä¹ˆå†³å®šäº† Pod åˆ†é…åˆ°å¯¹åº”çš„æ“ä½œç³»ç»Ÿï¼Ÿ

   çœŸæ­£å†³å®š Pod è¢«è°ƒåº¦åˆ°ç‰¹å®šæ“ä½œç³»ç»ŸèŠ‚ç‚¹ä¸Šçš„æœºåˆ¶ï¼Œæ˜¯ **Pod Spec (Pod è§„çº¦) ä¸­çš„è°ƒåº¦çº¦æŸ**ä¸Ž **Node (èŠ‚ç‚¹) ä¸Šçš„æ ‡ç­¾**ä¹‹é—´çš„åŒ¹é…ã€‚

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: my-windows-pod
   spec:
     # æ­¥éª¤ 1: å£°æ˜Ž Pod çš„æ“ä½œç³»ç»Ÿç±»åž‹
     # è¿™æœ¬èº«ä¸å½±å“è°ƒåº¦ï¼Œä½†æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ï¼Œä¹Ÿä¸ºäº†ç¬¦åˆå®‰å…¨ç­–ç•¥ç­‰ã€‚
     os:
       name: windows
    ...
     nodeSelector:
       kubernetes.io/os: windows
   ```

ä¸ºäº†æ–¹ä¾¿ä½ è®°å¿†ï¼Œæˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªç®€å•çš„ç±»æ¯”ï¼š

| å­—æ®µ/æœºåˆ¶                         | åŠŸèƒ½                   | å¥½æ¯”æ˜¯...                                                    |
| --------------------------------- | ---------------------- | ------------------------------------------------------------ |
| **`.spec.os.name`**               | **å£°æ˜Ž (Declaration)** | åŒ…è£¹ä¸Šçš„â€œå†…å«ç‰©å“â€æ¸…å•ï¼Œå†™ç€â€œWindows è½¯ä»¶â€ã€‚                 |
| **Node Label `kubernetes.io/os`** | **å±žæ€§ (Attribute)**   | æ¯ä¸ªæˆ¿é—¨ä¸Šçš„æ ‡ç­¾ï¼Œå†™ç€â€œæœ¬æˆ·ä½¿ç”¨ Windows ç³»ç»Ÿâ€æˆ–â€œæœ¬æˆ·ä½¿ç”¨ Linux ç³»ç»Ÿâ€ã€‚ |
| **Pod `nodeSelector`**            | **æŒ‡ä»¤ (Instruction)** | å¿«é€’å•ä¸Šçš„â€œæŠ•é€’è¦æ±‚â€ï¼Œæ˜Žç¡®æŒ‡ç¤ºï¼šâ€œå¿…é¡»æŠ•é€’åˆ°ä½¿ç”¨ Windows ç³»ç»Ÿçš„ä½æˆ·â€ã€‚ |

##### Pods and controllers

You can use workload resources to create and manage multiple Pods for you.

Here are some examples of workload resources that manage one or more Pods:

- [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
- [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)
- [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset)

##### Pod templates

PodTemplates are specifications for creating Pods, and are included in workload resources such as [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), [Jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/), and [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/).  The `PodTemplate` is part of the desired state of whatever workload resource you used to run your app.

the sample:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: hello
spec:
  template:
    # This is the pod template
    spec:
      containers:
      - name: hello
        image: busybox:1.28
        command: ['sh', '-c', 'echo "Hello, Kubernetes!" && sleep 3600']
      restartPolicy: OnFailure
    # The pod template ends here
```

#### Pod update and replacement

Kubernetes doesn't prevent you from managing Pods directly. è™½ç„¶ Kubernetes **å…è®¸** ä½ ç›´æŽ¥æ“ä½œ Podï¼Œä½†è¿™é€šå¸¸æ˜¯ä¸€ç§**åæ¨¡å¼ï¼ˆanti-patternï¼‰**ï¼Œä¸»è¦ç”¨äºŽè°ƒè¯•æˆ–ç´§æ€¥æƒ…å†µã€‚

ç”¨edit äº¤äº’å¼åŽ»æ›´æ–°ï¼š`kubectl edit pod [pod-name]` ï¼›è¿˜å¯ä»¥ç”¨patchåŽ»æ›´æ–° `kubectl patch pod my-test-pod -p '{"spec":{"activeDeadlineSeconds":60}}'`

#### Resource sharing and communication

podå†…çš„containerså…±ç”¨ä¸€ä¸ªip, ä¸åŒçš„containerå¦‚æžœæƒ³è¦exposeç«¯å£ï¼Œåªèƒ½æ˜¯ä¸åŒçš„ã€‚åŒä¸€podé‡Œé¢çš„containerç”¨localhost+ç«¯å£è¿›è¡Œé€šè®¯

#### Static Pods

æˆ‘ä»¬å¹³æ—¶ç”¨ `kubectl apply -f my-pod.yaml` åˆ›å»ºçš„ Podï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º**æ ‡å‡† Pod** æˆ– **API Server ç®¡ç†çš„ Pod**ã€‚å®ƒä»¬çš„ç”Ÿå‘½å‘¨æœŸå®Œå…¨ç”± Kubernetes çš„æŽ§åˆ¶å¹³é¢ï¼ˆç‰¹åˆ«æ˜¯ API Serverï¼‰æ¥ç®¡ç†ã€‚

è€Œ**é™æ€ Pod**åˆ™å®Œå…¨ä¸åŒã€‚

- **å®šä¹‰**ï¼šé™æ€ Pod æ˜¯ç›´æŽ¥ç”±ç‰¹å®šèŠ‚ç‚¹ä¸Šçš„ **Kubelet** å®ˆæŠ¤è¿›ç¨‹ç®¡ç†çš„ Podï¼Œå®ƒä¸é€šè¿‡ API Server è¿›è¡Œç®¡ç†ã€‚
- **æ¥æº**ï¼šKubelet ä¼šç›‘è§†å…¶æ‰€åœ¨èŠ‚ç‚¹ä¸Šçš„ä¸€ä¸ªç‰¹å®šç›®å½•ï¼ˆé€šå¸¸æ˜¯ `/etc/kubernetes/manifests`ï¼‰ã€‚ä»»ä½•æ”¾åœ¨è¿™ä¸ªç›®å½•ä¸‹çš„æ ‡å‡† Pod å®šä¹‰ YAML/JSON æ–‡ä»¶ï¼Œéƒ½ä¼šè¢« Kubelet è‡ªåŠ¨è¯†åˆ«å¹¶åˆ›å»ºä¸ºé™æ€ Podã€‚
- **ç”Ÿå‘½å‘¨æœŸ**ï¼š
  - **åˆ›å»º**ï¼šå°† Pod çš„ YAML æ–‡ä»¶æ”¾å…¥ Kubelet çš„ç›‘è§†ç›®å½•ã€‚
  - **åˆ é™¤**ï¼šä»Žè¯¥ç›®å½•ä¸­åˆ é™¤ Pod çš„ YAML æ–‡ä»¶ã€‚
  - **æ›´æ–°**ï¼šä¿®æ”¹è¯¥ç›®å½•ä¸­çš„ Pod YAML æ–‡ä»¶ï¼ˆKubelet ä¼šè‡ªåŠ¨åœæ­¢æ—§çš„ Podï¼Œå¹¶æ ¹æ®æ–°æ–‡ä»¶å¯åŠ¨æ–°çš„ Podï¼‰ã€‚

**é•œåƒ Pod (Mirror Pod)**
çŽ°åœ¨æˆ‘ä»¬å›žåˆ°äº†ä½ é—®é¢˜çš„æ ¸å¿ƒã€‚æ—¢ç„¶é™æ€ Pod ä¸å— API Server ç®¡ç†ï¼Œé‚£æˆ‘ä»¬æ‰§è¡Œ kubectl get pods æ—¶ï¼Œèƒ½çœ‹åˆ°å®ƒä»¬å—ï¼Ÿå¦‚æžœçœ‹ä¸åˆ°ï¼Œé‚£é›†ç¾¤ç®¡ç†å‘˜å°±æ— æ³•æ„ŸçŸ¥åˆ°è¿™äº›å…³é”®ç»„ä»¶çš„å­˜åœ¨ï¼Œè¿™ä¼šç»™ç›‘æŽ§å’Œç®¡ç†å¸¦æ¥éº»çƒ¦ã€‚

* **å®šä¹‰**ï¼šå½“ Kubelet åœ¨èŠ‚ç‚¹ä¸ŠæˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªé™æ€ Pod åŽï¼Œå®ƒä¼š**è‡ªåŠ¨åœ°**ã€**ä¸»åŠ¨åœ°**åœ¨ API Server ä¸Šä¸ºè¿™ä¸ªé™æ€ Pod åˆ›å»ºä¸€ä¸ªå¯¹åº”çš„ã€**åªè¯»çš„**å¯¹è±¡ã€‚è¿™ä¸ªåœ¨ API Server ä¸Šçš„å¯¹è±¡å°±å«åšâ€œé•œåƒ Podâ€ã€‚
* **ç›®çš„**ï¼šå®ƒçš„å”¯ä¸€ç›®çš„å°±æ˜¯è®©è¿™ä¸ªé™æ€ Pod åœ¨ Kubernetes çš„ API ä¸­**å¯è§ (Visible)**ã€‚

#### Pods with multiple containers

For example, you might have a container that acts as a web server for files in a shared volume, and a separate [sidecar container](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/) that updates those files from a remote source, as in the following diagram:

![image-20250919162927803](./images/image-20250919162927803.png)



#### Pod Lifecycle

 Pods follow a defined lifecycle, starting in the `Pending` [phase](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase), moving through `Running` if at least one of its primary containers starts OK, and then through either the `Succeeded` or `Failed` phases depending on whether any container in the Pod terminated in failure.

##### Pod lifetime

Pods are only [scheduled](https://kubernetes.io/docs/concepts/scheduling-eviction/) once in their lifetime; assigning a Pod to a specific node is called *binding*, and the process of selecting which node to use is called *scheduling*. 

You can use [Pod Scheduling Readiness](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/) to delay scheduling for a Pod until all its *scheduling gates* are removed. For example, you might want to define a set of Pods but only trigger scheduling once all the Pods have been created.

> å¯ä»¥å®šä¹‰ä¸åŒçš„è°ƒåº¦é—¨ï¼Œåœ¨çœŸçš„è¢«scheduleä¹‹å‰éœ€è¦ç›¸å°±çš„controller è¢«è¿™äº›è°ƒåº¦é—¨éƒ½åˆ é™¤

##### Pods and fault recovery

ä¸ä¼šæŠŠé‚£ä¸ªæ—§çš„ã€å¤±è´¥çš„ Pod å®žä¾‹ï¼ˆidentified by a UIDï¼‰æ‹¿èµ·æ¥ï¼Œæ‹æ‹ç°å°˜ï¼Œç„¶åŽæ”¾åˆ°ä¸€ä¸ªæ–°èŠ‚ç‚¹ä¸Šè®©å®ƒç»§ç»­è¿è¡Œã€‚æˆ‘ä»¬æ˜¯ç›´æŽ¥æ”¾å¼ƒæ—§çš„ï¼Œç„¶åŽç”±åƒ Deployment è¿™æ ·çš„æŽ§åˆ¶å™¨åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„æ›¿ä»£å“ï¼Œè¿™ä¸ªæ›¿ä»£å“å†ç”±è°ƒåº¦å™¨æ‰¾ä¸€ä¸ªæ–°å®¶ã€‚

nodeæ•…éšœ vs Pod æ•…éšœï¼š

| ç‰¹æ€§               | åœºæ™¯ 1: èŠ‚ç‚¹å®•æœº (Node Failure)        | åœºæ™¯ 2: Pod æ•…éšœ (Pod Failure on Healthy Node) |
| ------------------ | -------------------------------------- | ---------------------------------------------- |
| **ä¸»è¦å¤„ç†è€…**     | `kube-controller-manager` (åœ¨æŽ§åˆ¶å¹³é¢) | `kubelet` (åœ¨å·¥ä½œèŠ‚ç‚¹ä¸Š)                       |
| **å¤„ç†å¯¹è±¡**       | æ•´ä¸ª **Pod å¯¹è±¡**                      | Pod å†…éƒ¨çš„**å®¹å™¨ (Container)**                 |
| **ç»“æžœ**           | æ—§ Pod è¢«é©±é€/åˆ é™¤ï¼›**åˆ›å»ºå…¨æ–°çš„ Pod** | **åœ¨åŒä¸€ä¸ª Pod å†…é‡å¯å®¹å™¨**                    |
| **Pod UID**        | æ›¿ä»£å“çš„ UID æ˜¯**æ–°çš„**                | Pod çš„ UID **ä¿æŒä¸å˜**                        |
| **Pod IP åœ°å€**    | æ›¿ä»£å“çš„ IP åœ°å€æ˜¯**æ–°çš„**             | Pod çš„ IP åœ°å€**ä¿æŒä¸å˜**                     |
| **æ‰€åœ¨èŠ‚ç‚¹**       | æ›¿ä»£å“è¢«è°ƒåº¦åˆ°**æ–°çš„å¥åº·èŠ‚ç‚¹**ä¸Š       | ä»ç„¶åœ¨**åŽŸæ¥çš„èŠ‚ç‚¹**ä¸Š                         |
| **æ¢å¤é€Ÿåº¦**       | è¾ƒæ…¢ (åˆ†é’Ÿçº§åˆ«ï¼Œæœ‰ 5 åˆ†é’Ÿç­‰å¾…æœŸ)       | éžå¸¸å¿« (ç§’çº§)                                  |
| **`kubectl` è¡¨çŽ°** | æ—§ Pod æ¶ˆå¤±ï¼Œæ–° Pod å‡ºçŽ°               | åŒä¸€ä¸ª Pod çš„ `RESTARTS` è®¡æ•°å¢žåŠ               |

##### Associated lifetimes

When something is said to have the same lifetime as a Pod, such as a [volume](https://kubernetes.io/docs/concepts/storage/volumes/), that means that the thing exists as long as that specific Pod (with that exact UID) exists. 

![image-20250919162927803](./images/image-20250919162927803.png)

è¿™ä¸ªmulti-container Pod ä¸€æ—¦ç»“æŸç”Ÿå‘½ï¼Œé‚£ä¹ˆæ‰€å…³è”çš„Volumeä¹Ÿå°†ç»“æŸã€‚

##### Pod phase

Here are the possible values for `phase`:

| Value       | Description                                                  |
| :---------- | :----------------------------------------------------------- |
| `Pending`   | The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network. |
| `Running`   | The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting. |
| `Succeeded` | All containers in the Pod have terminated in success, and will not be restarted. |
| `Failed`    | All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting. |
| `Unknown`   | For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running. |

> Make sure not to confuse *Status*, a kubectl display field for user intuition, with the pod's `phase`.  When a pod is failing to start repeatedly, `CrashLoopBackOff` may appear in the `Status` field of some kubectl commands. Similarly, when a pod is being deleted, `Terminating` may appear in the `Status` field of some kubectl commands.

> **Kubernetes Pod ç»ˆæ­¢ç”Ÿå‘½å‘¨æœŸ (v1.27+) æ ¸å¿ƒçŸ¥è¯†ç‚¹**
>
> **1. æ ¸å¿ƒå˜åŒ–ï¼š**
>
> - è‡ª K8s v1.27 èµ·ï¼Œè¢«åˆ é™¤çš„ Pod ä¸ä¼šä»Ž `Terminating` çŠ¶æ€ç›´æŽ¥æ¶ˆå¤±ã€‚
> - å®ƒä¼šå…ˆæ ¹æ®å®¹å™¨çš„æœ€ç»ˆé€€å‡ºç ï¼Œè¿‡æ¸¡åˆ°ä¸€ä¸ªæ˜Žç¡®çš„**ç»ˆç«¯é˜¶æ®µ**ï¼š`Succeeded` (æ‰€æœ‰å®¹å™¨é€€å‡ºç ä¸º0) æˆ– `Failed` (è‡³å°‘ä¸€ä¸ªå®¹å™¨é€€å‡ºç éž0)ã€‚
> - **ç›®çš„**ï¼šæžå¤§å¢žå¼ºäº† Pod çš„**å¯è§‚æµ‹æ€§**ï¼Œæ–¹ä¾¿å‡†ç¡®è¿½è¸ªä¸€æ¬¡æ€§ä»»åŠ¡ï¼ˆå¦‚ Jobï¼‰çš„æœ€ç»ˆæˆè´¥ã€‚
>
> **2. â€œç»ˆç«¯é˜¶æ®µâ€åœç•™æ—¶é•¿ç”±è°å†³å®šï¼Ÿ**
>
> è¿™ä¸ªåœç•™æ—¶é—´ç”±ä¸¤ç§æœºåˆ¶æŽ§åˆ¶ï¼Œ**ä¼˜å…ˆçº§ä»Žé«˜åˆ°ä½Ž**ï¼š
>
> - **æœºåˆ¶ä¸€ (ç²¾ç¡®æŽ§åˆ¶ - æŽ¨è): `ttlSecondsAfterFinished`**
>   - **é…ç½®**: åœ¨ Pod æˆ– Job çš„ `spec` ä¸­è®¾ç½® `ttlSecondsAfterFinished: <ç§’æ•°>`ã€‚
>   - **è¡Œä¸º**: Pod åˆ°è¾¾ `Succeeded`/`Failed` çŠ¶æ€åŽï¼Œä¼š**ç²¾ç¡®åœ°**ç­‰å¾…æŒ‡å®šçš„ç§’æ•°ï¼Œç„¶åŽè¢«åžƒåœ¾å›žæ”¶æœºåˆ¶è‡ªåŠ¨åˆ é™¤ã€‚
>   - **ç¤ºä¾‹**: è®¾ç½®ä¸º `100` åˆ™ä¿ç•™100ç§’ï¼›è®¾ç½®ä¸º `0` åˆ™ä¼šç«‹å³æ¸…ç†ã€‚
> - **æœºåˆ¶äºŒ (é›†ç¾¤å…œåº• - ä¸ç²¾ç¡®): `terminated-pod-gc-threshold`**
>   - **è§¦å‘æ¡ä»¶**: ä»…å½“ Pod **æœªè®¾ç½®** `ttlSecondsAfterFinished` æ—¶æ­¤æœºåˆ¶æ‰ç”Ÿæ•ˆã€‚
>   - **è¡Œä¸º**: ç”±é›†ç¾¤æŽ§åˆ¶å¹³é¢ (`kube-controller-manager`) çš„å…¨å±€å‚æ•° `--terminated-pod-gc-threshold` æŽ§åˆ¶ã€‚åªæœ‰å½“é›†ç¾¤ä¸­å·²ç»ˆæ­¢çš„ Pod æ€»æ•°è¶…è¿‡æ­¤é˜ˆå€¼æ—¶ï¼Œæ‰ä¼šå¼€å§‹æ¸…ç†æœ€æ—§çš„ Podã€‚
>   - **ç»“è®º**: åœç•™æ—¶é—´**ä¸ç¡®å®š**ï¼Œå¯èƒ½éžå¸¸ä¹…ã€‚
>
> **3. æœ€ä½³å®žè·µï¼š** ä¸ºäº†å¯é¢„æµ‹åœ°ç®¡ç† Pod ç”Ÿå‘½å‘¨æœŸå¹¶ä¿æŒé›†ç¾¤æ•´æ´ï¼Œåº”å§‹ç»ˆä¸ºä½ çš„ä¸€æ¬¡æ€§ä»»åŠ¡ï¼ˆå°¤å…¶æ˜¯ `Job` èµ„æºï¼‰**æ˜Žç¡®è®¾ç½® `spec.ttlSecondsAfterFinished`**ã€‚

##### Container states

There are three possible container states: `Waiting`, `Running`, and `Terminated`.

To check the state of a Pod's containers, you can use `kubectl describe pod <name-of-pod>`.

**`Waiting`**

When you use `kubectl` to query a Pod with a container that is `Waiting`, you also see a Reason field to summarize why the container is in that state.

**`Running`**

When you use `kubectl` to query a Pod with a container that is `Running`, you also see information about when the container entered the `Running` state.

**`Terminated`**

When you use `kubectl` to query a Pod with a container that is `Terminated`, you see a reason, an exit code, and the start and finish time for that container's period of execution.

##### How Pods handle problems with containers

 Kubernetes å®¹å™¨å´©æºƒå¤„ç†æµç¨‹æ€»ç»“

| åºå·  | æè¿° (Description)                               | ç±»åž‹ (Type)                  | ä½ èƒ½çœ‹åˆ°çš„ (`kubectl`)                                  |
| ----- | ------------------------------------------------ | ---------------------------- | ------------------------------------------------------- |
| **1** | é¦–æ¬¡å´©æºƒ (Initial crash)                         | **äº‹ä»¶ (Event)**             | å‡ ä¹Žçœ‹ä¸åˆ°ï¼Œä¸€é—ªè€Œè¿‡                                    |
| **2** | é‡å¤å´©æºƒ (Repeated crashes)                      | **è¿‡ç¨‹ (Process)**           | Pod çŠ¶æ€ä¸ç¨³å®š, `RESTARTS` è®¡æ•°å¢žåŠ                      |
| **3** | `CrashLoopBackOff` çŠ¶æ€ (CrashLoopBackOff state) | **çŠ¶æ€/åŽŸå›  (State/Reason)** | æ˜Žç¡®çœ‹åˆ° `STATUS` åˆ—ä¸º `CrashLoopBackOff`               |
| **4** | é€€é¿é‡ç½® (Backoff reset)                         | **åŠ¨ä½œ (Action)**            | çœ‹ä¸åˆ°è¿™ä¸ªåŠ¨ä½œï¼Œä½†èƒ½çœ‹åˆ°ç»“æžœï¼šPod ç¨³å®šåœ¨ `Running` çŠ¶æ€ |

To investigate the root cause of a `CrashLoopBackOff` issue, a user can:

1. **Check logs**: Use `kubectl logs <name-of-pod>` to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes.
2. **Inspect events**: Use `kubectl describe pod <name-of-pod>` to see events for the Pod, which can provide hints about configuration or resource issues.
3. **Review configuration**: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available.
4. **Check resource limits**: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue.
5. **Debug application**: There might exist bugs or misconfigurations in the application code. Running this container image locally or in a development environment can help diagnose application specific issues.

###### Pod-level container restart policy

The `spec` of a Pod has a `restartPolicy` field with possible values Always, OnFailure, and Never. The default value is Always.

The `restartPolicy` for a Pod applies to [app containers](https://kubernetes.io/docs/reference/glossary/?all=true#term-app-container) in the Pod and to regular [init containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/). [Sidecar containers](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/) ignore the Pod-level `restartPolicy` field

After containers in a Pod exit, the kubelet restarts them with an exponential backoff delay (10s, 20s, 40s, â€¦), that is capped at 300 seconds (5 minutes). Once a container has executed for 10 minutes without any problems, the kubelet resets the restart backoff timer for that container.

##### Pod conditions

A Pod has a PodStatus, which has an array of PodConditions through which the Pod has or has not passed. Kubelet manages the following PodConditions:

- `PodScheduled`: the Pod has been scheduled to a node.
- `PodReadyToStartContainers`: (beta feature; enabled by [default](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-has-network)) the Pod sandbox has been successfully created and networking configured.
- `ContainersReady`: all containers in the Pod are ready.
- `Initialized`: all [init containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) have completed successfully.
- `Ready`: the Pod is able to serve requests and should be added to the load balancing pools of all matching Services.
- `DisruptionTarget`: the pod is about to be terminated due to a disruption (such as preemption, eviction or garbage-collection).
- `PodResizePending`: a pod resize was requested but cannot be applied. See [Pod resize status](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status).
- `PodResizeInProgress`: the pod is in the process of resizing. See [Pod resize status](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status).

ç”¨`kubectl describe pod [pod-name]` æ¥æŸ¥çœ‹ã€‚æ¯ä¸ªconditionçš„éƒ½æœ‰ä¸‹é¢çš„å­—æ®µçš„å€¼ï¼Œç”¨describe podæ¥æŸ¥çœ‹çš„æ—¶å€™ï¼Œåªæ˜¾ç¤º type, statusä¸¤ä¸ªæœ€é‡è¦çš„å­—æ®µï¼Œå¦‚æžœè¦è¯¦ç»†çš„çœ‹å¯ä»¥ç”¨`kubectl get pod [ä½ çš„podåç§°] -o yaml`

| Field name           | Description                                                  |
| :------------------- | :----------------------------------------------------------- |
| `type`               | Name of this Pod condition.                                  |
| `status`             | Indicates whether that condition is applicable, with possible values "`True`", "`False`", or "`Unknown`". |
| `lastProbeTime`      | Timestamp of when the Pod condition was last probed.         |
| `lastTransitionTime` | Timestamp for when the Pod last transitioned from one status to another. |
| `reason`             | Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition. |
| `message`            | Human-readable message indicating details about the last status transition. |

###### Pod readiness

Your application can inject extra feedback or signals into PodStatus: *Pod readiness*. To use this, set `readinessGates` in the Pod's `spec` to specify a list of additional conditions that the kubelet evaluates for Pod readiness.

example:

```yaml
kind: Pod
...
spec:
  readinessGates:
    - conditionType: "www.example.com/feature-1"
status:
  conditions:
    - type: Ready                              # a built in PodCondition
      status: "False"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
    - type: "www.example.com/feature-1"        # an extra PodCondition
      status: "False"
      lastProbeTime: null
      lastTransitionTime: 2018-01-01T00:00:00Z
  containerStatuses:
    - containerID: docker://abcd...
      ready: true
```

ä¸€ä¸ª Pod å¿…é¡»åŒæ—¶æ»¡è¶³æ‰€æœ‰ readinessGates æ¡ä»¶ å¹¶ä¸” å…¶è‡ªèº«çš„ readinessProbe æˆåŠŸï¼Œæ‰ä¼šè¢« Kubernetes æœ€ç»ˆæ ‡è®°ä¸º Ready çŠ¶æ€ï¼Œç„¶åŽ Service æ‰ä¼šå°†æµé‡è½¬å‘ç»™å®ƒã€‚readyå¥½äº†æ²¡æœ‰ï¼Œå¯ä»¥ç”¨è¿™ä¸ªæ¥æŸ¥è¯¢ã€‚ å…¶ä¸­çš„READY å­—æ®µ

```shell
leite@leite-company ~> kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
springboot3-deployment-559c8cc88b-l6sjg   0/1     Running   0          13s
springboot3-deployment-559c8cc88b-rr22r   0/1     Running   0          13s

```

##### Container probes

###### Check mechanisms

1. `exec` (æ‰§è¡Œå‘½ä»¤)

- **æ ¸å¿ƒæ€æƒ³**: åœ¨å®¹å™¨å†…éƒ¨æ‰§è¡Œä¸€ä¸ªä½ æŒ‡å®šçš„å‘½ä»¤ã€‚

- **æˆåŠŸæ ‡å‡†**: è¯¥å‘½ä»¤æ‰§è¡ŒåŽçš„é€€å‡ºç ï¼ˆExit Codeï¼‰ä¸º `0`ã€‚ä»»ä½•éž `0` çš„é€€å‡ºç éƒ½è¢«è®¤ä¸ºæ˜¯å¤±è´¥ã€‚

- **é€‚ç”¨åœºæ™¯**:

  - å½“ä½ çš„åº”ç”¨ç¨‹åºæ²¡æœ‰æä¾› HTTP å¥åº·æ£€æŸ¥æŽ¥å£æ—¶ã€‚
  - éœ€è¦æ£€æŸ¥åº”ç”¨å†…éƒ¨çš„ç‰¹å®šçŠ¶æ€ï¼Œä¾‹å¦‚ï¼šæ£€æŸ¥æŸä¸ªé‡è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (`cat /tmp/healthy`)ï¼Œæˆ–è€…è¿è¡Œä¸€ä¸ªè‡ªå®šä¹‰çš„å¥åº·æ£€æŸ¥è„šæœ¬ (`/usr/bin/check-health.sh`)ã€‚
  - éžå¸¸çµæ´»ï¼Œå¯ä»¥å®žçŽ°å¤æ‚çš„å¥åº·æ£€æŸ¥é€»è¾‘ã€‚

  *ç¤ºä¾‹*ï¼š

  YAML

  ```yaml
  livenessProbe:
    exec:
      command:
      - cat
      - /tmp/healthy
  ```

2. `httpGet` (HTTP GET è¯·æ±‚)

- **æ ¸å¿ƒæ€æƒ³**: å‘å®¹å™¨çš„ IP åœ°å€ã€æŒ‡å®šç«¯å£å’Œè·¯å¾„å‘é€ä¸€ä¸ª HTTP GET è¯·æ±‚ã€‚

- **æˆåŠŸæ ‡å‡†**: æ”¶åˆ°çš„ HTTP å“åº”çŠ¶æ€ç ï¼ˆStatus Codeï¼‰å¤§äºŽç­‰äºŽ `200` ä¸”å°äºŽ `400`ï¼ˆå³ `2xx` æˆ– `3xx` ç³»åˆ—ï¼‰ã€‚

- **é€‚ç”¨åœºæ™¯**:

  - **æœ€å¸¸ç”¨**çš„æ–¹å¼ï¼Œå‡ ä¹Žæ‰€æœ‰ Web åº”ç”¨æˆ–æä¾› HTTP API çš„æœåŠ¡éƒ½é€‚ç”¨ã€‚
  - é€šå¸¸åº”ç”¨ä¼šä¸“é—¨æä¾›ä¸€ä¸ªç”¨äºŽå¥åº·æ£€æŸ¥çš„ç«¯ç‚¹ï¼ˆendpointï¼‰ï¼Œæ¯”å¦‚ `/healthz` æˆ– `/status`ã€‚

  *ç¤ºä¾‹*ï¼š

  YAML

  ```yaml
  readinessProbe:
    httpGet:
      path: /healthz
      port: 8080
  ```

3. `tcpSocket` (TCP å¥—æŽ¥å­—)

- **æ ¸å¿ƒæ€æƒ³**: å°è¯•ä¸Žå®¹å™¨çš„æŒ‡å®šç«¯å£å»ºç«‹ä¸€ä¸ª TCP è¿žæŽ¥ã€‚

- **æˆåŠŸæ ‡å‡†**: TCP â€œä¸‰æ¬¡æ¡æ‰‹â€æˆåŠŸï¼Œå³ç«¯å£æ˜¯å¼€æ”¾çš„ã€‚åªè¦èƒ½æˆåŠŸå»ºç«‹è¿žæŽ¥ï¼Œå°±è¢«è®¤ä¸ºæ˜¯å¥åº·çš„ï¼Œå³ä½¿è¿žæŽ¥ç«‹å³è¢«å…³é—­ã€‚

- **é€‚ç”¨åœºæ™¯**:

  - é€‚ç”¨äºŽé‚£äº›ä¸æä¾› HTTP æŽ¥å£ï¼Œä½†ç›‘å¬ç‰¹å®š TCP ç«¯å£çš„æœåŠ¡ã€‚
  - ä¾‹å¦‚ï¼šæ•°æ®åº“ï¼ˆMySQL ç›‘å¬ `3306`ï¼‰ï¼Œç¼“å­˜æœåŠ¡ï¼ˆRedis ç›‘å¬ `6379`ï¼‰ï¼Œæˆ–è€…å…¶ä»–ä»»ä½•åŸºäºŽ TCP çš„åº”ç”¨ã€‚

  *ç¤ºä¾‹*ï¼š

  YAML

  ```yaml
  livenessProbe:
    tcpSocket:
      port: 3306
  ```

4. `grpc` (gRPC è¿œç¨‹è¿‡ç¨‹è°ƒç”¨)

- **æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨ gRPC åè®®æ‰§è¡Œä¸€ä¸ªè¿œç¨‹è¿‡ç¨‹è°ƒç”¨ã€‚è¿™æ˜¯æ¯”è¾ƒæ–°ä¸”ç‰¹å®šçš„ä¸€ç§æ–¹å¼ã€‚

- **æˆåŠŸæ ‡å‡†**: å“åº”çš„çŠ¶æ€æ˜¯ `SERVING`ã€‚è¿™è¦æ±‚ä½ çš„åº”ç”¨å¿…é¡»å®žçŽ° [gRPC Health Checking Protocol](https://www.google.com/search?q=https://github.com/grpc/grpc/blob/master/doc/health-checking.md)ã€‚

- **é€‚ç”¨åœºæ™¯**:

  - ä¸“é—¨ç”¨äºŽåŸºäºŽ gRPC æž„å»ºçš„å¾®æœåŠ¡ã€‚
  - å¦‚æžœä½ çš„æŠ€æœ¯æ ˆå¹¿æ³›ä½¿ç”¨ gRPCï¼Œè¿™æ˜¯ä¸€ç§æ¯” `httpGet` æ›´åŽŸç”Ÿã€æ›´é«˜æ•ˆçš„æ£€æŸ¥æ–¹å¼ã€‚

  *ç¤ºä¾‹*ï¼š

  YAML

  ```yaml
  ports:
  - name: grpc
    port: 9000
  livenessProbe:
    grpc:
      port: 9000
  ```

###### Probe outcome

Each probe has one of three results:

- `Success`

  The container passed the diagnostic.

- `Failure`

  The container failed the diagnostic.

- `Unknown`

  The diagnostic failed (no action should be taken, and the kubelet will make further checks).

###### Types of probe

* **livenessProbe**

* **readinessProbe**

  readiness probeè¿”å›žFailureå¹¶ä¸ä¼šå¯¼è‡´å®¹å™¨é‡å¯

* **startupProbe**

  Startup probes are useful for Pods that have containers that take a long time to come into service. 

  Indicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until it succeeds

  livenessProbe (å­˜æ´»æŽ¢é’ˆ) å’Œ readinessProbe (å°±ç»ªæŽ¢é’ˆ) åœ¨ startupProbe é¦–æ¬¡æˆåŠŸä¹‹å‰ï¼Œæ ¹æœ¬ä¸ä¼šå¼€å§‹æ‰§è¡Œã€‚

**ä¸ºä»€ä¹ˆéœ€è¦ `startupProbe`ï¼Ÿ**

æƒ³è±¡ä¸€ä¸ªåœºæ™¯ï¼šä½ æœ‰ä¸€ä¸ªå¤æ‚çš„ Java åº”ç”¨ï¼Œå®ƒå¯åŠ¨æ—¶éœ€è¦åŠ è½½å¤§é‡æ•°æ®ã€é¢„çƒ­ç¼“å­˜ã€å»ºç«‹æ•°æ®åº“è¿žæŽ¥æ± ç­‰ï¼Œæ•´ä¸ªè¿‡ç¨‹å¯èƒ½éœ€è¦2åˆ°3åˆ†é’Ÿã€‚

- **å¦‚æžœæ²¡æœ‰ `startupProbe`**ï¼šä½ å¯èƒ½ä¼šé…ç½®ä¸€ä¸ª `livenessProbe`ï¼Œè®©å®ƒæ¯10ç§’æ£€æŸ¥ä¸€æ¬¡åº”ç”¨çš„å¥åº·çŠ¶å†µã€‚ä½†ç”±äºŽåº”ç”¨å¯åŠ¨éœ€è¦180ç§’ï¼Œè¿™ä¸ª `livenessProbe` åœ¨å‰170ç§’å†…æ‰€æœ‰çš„æŽ¢æµ‹éƒ½ä¼šæ˜¯å¤±è´¥çš„ã€‚å¦‚æžœä½ çš„ `failureThreshold` (å¤±è´¥é˜ˆå€¼) è®¾ç½®ä¸º5ï¼Œé‚£ä¹ˆåœ¨ç¬¬50ç§’æ—¶ (10ç§’ * 5æ¬¡)ï¼Œkubelet å°±ä¼šè®¤ä¸ºä½ çš„åº”ç”¨å·²ç»æ­»äº†ï¼Œä»Žè€Œæ€æ­»å¹¶é‡å¯å®ƒã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šæ— é™å¾ªçŽ¯ï¼Œä½ çš„åº”ç”¨æ°¸è¿œä¹Ÿå¯åŠ¨ä¸èµ·æ¥ã€‚
- **æœ‰äº† `startupProbe`**ï¼šä½ å¯ä»¥ä¸“é—¨ä¸ºè¿™ä¸ªæ¼«é•¿çš„å¯åŠ¨è¿‡ç¨‹é…ç½®ä¸€ä¸ª `startupProbe`ã€‚ä¾‹å¦‚ï¼Œè®¾ç½®ä¸€ä¸ªå¾ˆé•¿çš„æŽ¢æµ‹å‘¨æœŸå’Œè¶³å¤Ÿé«˜çš„å¤±è´¥é˜ˆå€¼ï¼Œç»™å®ƒæ€»å…±5åˆ†é’Ÿçš„æ—¶é—´æ¥å®Œæˆå¯åŠ¨ã€‚
  - åœ¨è¿™5åˆ†é’Ÿå†…ï¼Œåªæœ‰ `startupProbe` åœ¨å·¥ä½œã€‚
  - `livenessProbe` å’Œ `readinessProbe` ä¼šä¸€ç›´â€œè¢–æ‰‹æ—è§‚â€ã€‚
  - ä¸€æ—¦ `startupProbe` æŽ¢æµ‹æˆåŠŸï¼Œå®ƒå°±â€œåŠŸæˆèº«é€€â€ï¼Œæ­¤åŽæ°¸è¿œä¸ä¼šå†æ‰§è¡Œã€‚
  - ç´§æŽ¥ç€ï¼Œ`livenessProbe` å’Œ `readinessProbe` å¼€å§‹æŽ¥ç®¡ï¼Œåˆ†åˆ«è´Ÿè´£ç›‘æŽ§å®¹å™¨åœ¨è¿è¡ŒæœŸé—´æ˜¯å¦å¥åº·ä»¥åŠæ˜¯å¦å‡†å¤‡å¥½æŽ¥æ”¶æµé‡ã€‚

##### Termination of Pods

 If the kubelet or the container runtime's management service is restarted while waiting for processes to terminate, the cluster retries from the start including the full original grace period.

###### Pod Termination Flow

If the `preStop` hook is still running after the grace period expires, the kubelet requests a small, one-off grace period extension of 2 seconds.

###### Forced Pod termination 

By default, all deletes are graceful within 30 seconds. The `kubectl delete` command supports the `--grace-period=<seconds>` option which allows you to override the default and specify your own value.

Using kubectl, You must specify an additional flag `--force` along with `--grace-period=0` in order to perform force deletions.

###### Pod shutdown and sidecar containers 

If your Pod includes one or more [sidecar containers](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/) (init containers with an Always restart policy), the kubelet will delay sending the TERM signal to these sidecar containers until the last main container has fully terminated. 

#### Init Containers

##### Understanding init containers

Init containers are exactly like regular containers, except:

- Init containers always run to completion.
- Each init container must complete successfully before the next one starts.

###### Differences from regular containers

Regular init containers (in other words: excluding sidecar containers) do not support the `lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` fields. 

sidecar containers continue running during a Pod's lifetime, and *do* support some probes. 

###### Differences from sidecar containers

Unlike [sidecar containers](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/), init containers are not continuously running alongside the main containers.

init containers do not support `lifecycle`, `livenessProbe`, `readinessProbe`, or `startupProbe` whereas sidecar containers support all these [probes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe) to control their lifecycle.

##### Detailed behavior

However, if the Pod `restartPolicy` is set to Always, the init containers use `restartPolicy` OnFailure.

> å³ä½¿podçš„restartPolicyæ˜¯always, ä½†å¯¹äºŽinit containersæ¥è¯´å…¶å®žç›¸å½“äºŽOnfaiure.

A Pod that is initializing is in the `Pending` state but should have a condition `Initialized` set to false.

If the Pod [restarts](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#pod-restart-reasons), or is restarted, all init containers must execute again.

Because init containers can be restarted, retried, or re-executed, init container code should be idempotent. 

However, Kubernetes prohibits `readinessProbe` from being used because init containers cannot define readiness distinct from completion.

However it is recommended to use `activeDeadlineSeconds` only if teams deploy their application as a Job, because `activeDeadlineSeconds` has an effect even after initContainer finished. 

###### Resource sharing within containers

1. æ ¸å¿ƒæ¦‚å¿µï¼š`Effective Init Request/Limit`

- **å®šä¹‰**: å®ƒæ˜¯ Kubernetes è®¡ç®—å‡ºçš„ä¸€ä¸ª**ä¸­é—´å€¼**ï¼Œä»£è¡¨ `init` é˜¶æ®µå¯¹**å•ä¸€èµ„æº**ï¼ˆå¦‚ `memory` æˆ– `cpu`ï¼‰çš„æœ€å¤§éœ€æ±‚ã€‚
- **è®¡ç®—æ–¹æ³•**: å–**æ‰€æœ‰ `init` å®¹å™¨**ä¸­ï¼Œå¯¹**åŒä¸€ç§èµ„æº**ï¼ˆ`cpu` æˆ– `memory`ï¼‰è®¾ç½®çš„ `request` æˆ– `limit` çš„**æœ€å¤§å€¼**ã€‚
  - `Effective Init Request` = `MAX(init_container_1_request, init_container_2_request, ...)`
  - `Effective Init Limit` = `MAX(init_container_1_limit, init_container_2_limit, ...)`

2. Pod æœ€ç»ˆèµ„æºè§„æ ¼çš„è®¡ç®—è§„åˆ™

   Pod å¯åŠ¨æ‰€éœ€çš„èµ„æºï¼Œå¿…é¡»åŒæ—¶æ»¡è¶³ `init` å®¹å™¨ï¼ˆè½®æµæ‰§è¡Œï¼‰å’Œ `main` å®¹å™¨ï¼ˆåŒæ—¶æ‰§è¡Œï¼‰çš„éœ€æ±‚ã€‚

- **Pod æ€»è¯·æ±‚ (Request)** = `MAX` ( **æ‰€æœ‰ä¸»å®¹å™¨è¯·æ±‚ä¹‹å’Œ** , **Effective Init Request** )
- **Pod æ€»é™åˆ¶ (Limit)** = `MAX` ( **æ‰€æœ‰ä¸»-å®¹å™¨é™åˆ¶ä¹‹å’Œ** , **Effective Init Limit** )

3. å…³é”®è¦ç‚¹ä¸Žè¾¹ç•Œæƒ…å†µ

- **ç‹¬ç«‹è®¡ç®—**: `cpu` å’Œ `memory` ä¸¤ç§èµ„æºçš„ `request` å’Œ `limit` æ˜¯å®Œå…¨åˆ†å¼€ç‹¬ç«‹è®¡ç®—çš„ã€‚
- **ä¸»å®¹å™¨ä¼˜å…ˆ**: å¦‚æžœä»»ä½•**ä¸€ä¸ªä¸»å®¹å™¨**æ²¡æœ‰è®¾ç½® `limit`ï¼Œé‚£ä¹ˆæ•´ä¸ª Pod çš„ `limit` å°±æ˜¯**æ— é™åˆ¶**çš„ã€‚`init` å®¹å™¨è®¾ç½®çš„ `limit` æ— æ³•çº¦æŸä¸»å®¹å™¨ã€‚
- **å½±å“ QoS**: æœªè®¾ç½® `limit` ä¼šå¯¼è‡´ Pod çš„ QoS ç­‰çº§é™ä¸º `Burstable`ï¼Œåœ¨èŠ‚ç‚¹èµ„æºç´§å¼ æ—¶æœ‰è¢«é©±é€çš„é£Žé™©ã€‚
- **è°ƒåº¦ä¾æ®**: Pod çš„æ€»è¯·æ±‚ (`Pod Total Request`) æ˜¯è°ƒåº¦å™¨ (`kube-scheduler`) åœ¨ä¸º Pod é€‰æ‹©èŠ‚ç‚¹æ—¶çš„é‡è¦ä¾æ®ã€‚

###### Pod restart reasons

| åœºæ™¯       | æ ¸å¿ƒè§¦å‘äº‹ä»¶     | Init è®°å½•ä¸¢å¤±çš„è§’è‰² | Pod æ˜¯å¦é‡å¯ | Init å®¹å™¨æ˜¯å¦é‡æ–°è¿è¡Œ       |
| ---------- | ---------------- | ------------------- | ------------ | --------------------------- |
| **ç¬¬ä¸€ç§** | ä¸»å®¹å™¨å…¨éƒ¨ç»ˆæ­¢   | é™„åŠ æ¡ä»¶            | **æ˜¯**       | **æ˜¯**ï¼ˆåœ¨ Pod é‡å¯æµç¨‹ä¸­ï¼‰ |
| **ç¬¬äºŒç§** | ä»… Init è®°å½•ä¸¢å¤± | å”¯ä¸€äº‹ä»¶            | **å¦**       | **å¦**                      |

#### Sidecar Containers

##### Sidecar containers in Kubernetes

These restartable *sidecar* containers are independent from other init containers and from the main application container(s) within the same pod. These can be started, stopped, or restarted without affecting the main application container and other init containers.

Example:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: alpine:latest
          command: ['sh', '-c', 'while true; do echo "logging" >> /opt/logs.txt; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /opt
      initContainers:
        - name: logshipper
          image: alpine:latest
          restartPolicy: Always
          command: ['sh', '-c', 'tail -F /opt/logs.txt']
          volumeMounts:
            - name: data
              mountPath: /opt
      volumes:
        - name: data
          emptyDir: {}
```

##### Sidecar containers and Pod lifecycle

If an init container is created with its `restartPolicy` set to `Always`, it will start and remain running during the entire life of the Pod.

After a sidecar-style init container is running (the kubelet has set the `started` status for that init container to true), the kubelet then starts the next init container from the ordered `.spec.initContainers` list. 

> å…·ä½“æ¥è¯´ï¼Œå®ƒçš„å®Œæ•´è·¯å¾„æ˜¯ pod.status.initContainerStatuses[].startedã€‚  sidecar containerå¯åŠ¨åŽå¯èƒ½ä¼šä¸€ç›´å¤„äºŽrunningçŠ¶æ€[è€Œä¸æ˜¯æ­£å¸¸çš„init containerä¸­çš„ç­‰å‰ä¸€ä¸ªinit containerçš„stateå˜ä¸ºterminatedå†å¯åŠ¨ä¸‹ä¸€ä¸ª]ï¼Œä½†å®ƒä¸ä¼šå½±å“ä¸‹ä¸€ä¸ªinit containerçš„æ­£å¸¸å¯åŠ¨ï¼Œå› ä¸ºè¿™ä¸ª`....started`çš„çŠ¶æ€å€¼ä¸ºtrue
>
> è¿™ä¸æ˜¯ä¸€ä¸ªä½ ä¼šç”¨ kubectl get pods ç›´æŽ¥çœ‹åˆ°çš„é¡¶å±‚çŠ¶æ€ï¼Œè€Œæ˜¯éœ€è¦æŸ¥çœ‹ Pod çš„è¯¦ç»† YAML æˆ– JSON æè¿°æ‰èƒ½æ‰¾åˆ°çš„å†…éƒ¨çŠ¶æ€.

###### Jobs with sidecar containers

If you define a Job that uses sidecar using Kubernetes-style init containers, the sidecar container in each Pod does not prevent the Job from completing after the main container has finished.

> Sidecar å®¹å™¨æœ¬èº«å°†ä¸å†æˆä¸ºåˆ¤æ–­ Pod æ˜¯å¦æˆåŠŸå®Œæˆçš„é˜»ç¢ã€‚

##### Differences from application containers

So exit codes different from `0` (`0` indicates successful exit), for sidecar containers are normal on Pod termination and should be generally ignored by the external tooling.

##### Differences from init containers

Sidecar containers run concurrently with the main application container. 

Sidecar containers can interact directly with the main application containers, because like init containers they always share the same network, and can optionally also share volumes (filesystems).

Init containers stop before the main containers start up, so init containers cannot exchange messages with the app container in a Pod. Any data passing is one-way (for example, an init container can put information inside an `emptyDir` volume).

#### Ephemeral Containers

A special type of container that runs temporarily in an existing [Pod](https://kubernetes.io/docs/concepts/workloads/pods/) to accomplish user-initiated actions such as troubleshooting. You use ephemeral containers to inspect services rather than to build applications.

> #### Note:
>
> Ephemeral containers are not supported by [static pods](https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/).

Ephemeral containers are created using a special `ephemeralcontainers` handler in the API rather than by adding them directly to `pod.spec`, so it's not possible to add an ephemeral container using `kubectl edit`.

å¯ä»¥ä½¿ç”¨ `kubectl debug` å‘½ä»¤æ¥é™„åŠ ä¸€ä¸ªä¸´æ—¶å®¹å™¨ï¼š

```bash
# è¯­æ³•: kubectl debug -it <pod-name> --image=<debug-image> --target=<app-container-name> -- <command>

# é™„åŠ ä¸€ä¸ª busybox å®¹å™¨ï¼Œå¹¶å¯åŠ¨ä¸€ä¸ªäº¤äº’å¼çš„ shell
kubectl debug -it my-app-pod --image=busybox --target=my-app-container

# è¿›å…¥ shell åŽï¼Œä½ å°±ä½äºŽ my-app-pod çš„ç½‘ç»œçŽ¯å¢ƒä¸­äº†
# ä½ å¯ä»¥...
# æ£€æŸ¥ç½‘ç»œè¿žæŽ¥
/ # ping google.com

# æ£€æŸ¥åº”ç”¨å®¹å™¨çš„ç«¯å£æ˜¯å¦åœ¨ç›‘å¬ (å‡è®¾åº”ç”¨è·‘åœ¨ 80 ç«¯å£)
/ # wget -qO- localhost:80

# æŸ¥çœ‹æ‰€æœ‰è¿›ç¨‹ (å¦‚æžœå¼€å¯äº†è¿›ç¨‹å…±äº«)
/ # ps aux
```

å½“ä½ æ‰§è¡Œ `kubectl debug` åŽï¼Œå¦‚æžœä½ åŽ»æŸ¥çœ‹ Pod çš„ YAML å®šä¹‰ï¼Œä½ ä¼šå‘çŽ°å¤šäº†ä¸€ä¸ª `ephemeralContainers` å­—æ®µï¼Œé‡Œé¢æè¿°äº†ä½ åˆšåˆšæ·»åŠ çš„ `busybox` å®¹å™¨ã€‚

#### Disruptions

This guide is for application owners who want to build highly available applications, and thus need to understand what types of disruptions can happen to Pods.

##### Voluntary and involuntary disruptions

We call these unavoidable cases *involuntary disruptions* to an application. Examples are:

- a hardware failure of the physical machine backing the node
- cluster administrator deletes VM (instance) by mistake
- cloud provider or hypervisor failure makes VM disappear
- a kernel panic
- the node disappears from the cluster due to cluster network partition
- eviction of a pod due to the node being [out-of-resources](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/).

We call other cases *voluntary disruptions*. These include both actions initiated by the application owner and those initiated by a Cluster Administrator. Typical application owner actions include:

- deleting the deployment or other controller that manages the pod
- updating a deployment's pod template causing a restart
- directly deleting a pod (e.g. by accident)

Cluster administrator actions include:

- [Draining a node](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/) for repair or upgrade.
- Draining a node from a cluster to scale the cluster down (learn about [Node Autoscaling](https://kubernetes.io/docs/concepts/cluster-administration/node-autoscaling/)).
- Removing a pod from a node to permit something else to fit on that node.

##### Dealing with disruptions

Here are some ways to mitigate involuntary disruptions:

- Ensure your pod [requests the resources](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/) it needs.
- Replicate your application if you need higher availability. (Learn about running replicated [stateless](https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/) and [stateful](https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/) applications.)
- For even higher availability when running replicated applications, spread applications across racks (using [anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)) or across zones (if using a [multi-zone cluster](https://kubernetes.io/docs/setup/multiple-zones).)

##### Pod disruption budgets

As an application owner, you can create a PodDisruptionBudget (PDB) for each application. A PDB limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions.

A PDB specifies the number of replicas that an application can tolerate having, relative to how many it is intended to have. For example, a Deployment which has a `.spec.replicas: 5` is supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.

It is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy) to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become [healthy](https://kubernetes.io/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod) before the drain can proceed.

example:

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-app-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: my-app
  # å…³é”®é…ç½®åœ¨è¿™é‡Œï¼
  unhealthyPodEvictionPolicy: AlwaysAllow
```

| ç­–ç•¥         | `IfHealthyBudget` (é»˜è®¤)                                     | `AlwaysAllow`                                                |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **ä¼˜ç‚¹**     | å¯¹åº”ç”¨æ›´â€œå®‰å…¨â€ï¼Œå°½æœ€å¤§åŠªåŠ›ä¿è¯å¥åº·å®žä¾‹çš„æ•°é‡ã€‚               | **ä¼˜å…ˆä¿éšœé›†ç¾¤è¿ç»´**ï¼Œä¸ä¼šå› ä¸ºå•ä¸ªåº”ç”¨çš„æ•…éšœè€Œé˜»å¡žèŠ‚ç‚¹ç»´æŠ¤ã€å‡çº§ç­‰é‡è¦æ“ä½œã€‚ |
| **ç¼ºç‚¹**     | **å¯èƒ½é˜»å¡žèŠ‚ç‚¹æŽ’ç©º**ï¼Œå¯¼è‡´é›†ç¾¤è¿ç»´å·¥ä½œæ— æ³•è¿›è¡Œã€‚             | å¦‚æžœåº”ç”¨çš„å¤§å¤šæ•°å®žä¾‹éƒ½ä¸å¥åº·ï¼Œ`drain` æ“ä½œå¯èƒ½ä¼šé©±é€æŽ‰æœ€åŽå‡ ä¸ªå¥åº·çš„å®žä¾‹ï¼Œå¯èƒ½å¯¼è‡´æœåŠ¡çŸ­æš‚ä¸­æ–­ã€‚ |
| **é€‚ç”¨åœºæ™¯** | æžå°‘æ•°æƒ…å†µä¸‹ï¼Œå¦‚æžœåº”ç”¨çš„å¥åº·æ¯”é›†ç¾¤çš„å¯ç»´æŠ¤æ€§æ›´é‡è¦ï¼Œä¸”åº”ç”¨æœ¬èº«éžå¸¸ç¨³å®šã€‚ | **ç»å¤§å¤šæ•°åœºæ™¯çš„æŽ¨èåšæ³•**ã€‚å®ƒéµå¾ªä¸€ä¸ªé‡è¦çš„è¿ç»´ç†å¿µï¼šåº”ç”¨çš„æ•…éšœåº”è¯¥ç”±åº”ç”¨è‡ªèº«è§£å†³ï¼ˆä¾‹å¦‚é€šè¿‡æŽ§åˆ¶å™¨é‡å»ºï¼‰ï¼Œè€Œä¸åº”è¯¥å½±å“åˆ°æ•´ä¸ªåŸºç¡€è®¾æ–½çš„ç®¡ç†ã€‚ |

##### Pod disruption conditions

[ä¸Ž Pod conditions å…³è”](#Pod conditions)

A dedicated Pod `DisruptionTarget` [condition](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions) is added to indicate that the Pod is about to be deleted due to a [disruption](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/). The `reason` field of the condition additionally indicates one of the following reasons for the Pod termination:

- `PreemptionByScheduler`

  Pod is due to be [preempted](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption) by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see [Pod priority preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/).

- `DeletionByTaintManager`

  Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within `kube-controller-manager`) due to a `NoExecute` taint that the Pod does not tolerate; see [taint](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)-based evictions.

- `EvictionByEvictionAPI`

  Pod has been marked for [eviction using the Kubernetes API](https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/) .

- `DeletionByPodGC`

  Pod, that is bound to a no longer existing Node, is due to be deleted by [Pod garbage collection](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection).

- `TerminationByKubelet`

  Pod has been terminated by the kubelet, because of either [node pressure eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/), the [graceful node shutdown](https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown), or preemption for [system critical pods](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)

When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's [Pod failure policy](https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy).

#### Pod hostname

##### Default Pod hostname

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox-1
spec:
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    name: busybox
```

The Pod created by this manifest will have its hostname and fully qualified domain name (FQDN) set to `busybox-1`.

##### Hostname with pod's hostname and subdomain fields

The Pod spec includes an optional `hostname` field. When set, this value takes precedence over the Pod's `metadata.name` as the hostname (observed from within the Pod). 

When both hostname and subdomain are set, the cluster's DNS server will create A and/or AAAA records based on these fields.

##### Hostname with pod's setHostnameAsFQDN fields

When both `setHostnameAsFQDN: true` and the subdomain field is set in the Pod spec, the kubelet writes the Pod's FQDN into the hostname for that Pod's namespace. In this case, both `hostname` and `hostname --fqdn` return the Pod's FQDN.

> Note:
> In Linux, the hostname field of the kernel (the nodename field of struct utsname) is limited to 64 characters.
>
> If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in Pending status (ContainerCreating as seen by kubectl) generating error events, such as "Failed to construct FQDN from Pod hostname and cluster domain".
>
> This means that when using this field, you must ensure the combined length of the Pod's metadata.name (or spec.hostname) and spec.subdomain fields results in an FQDN that does not exceed 64 characters.

#### Pod QoS classes

##### Quality of Service classes

 Kubernetes assigns every Pod a QoS class based on the resource requests and limits of its component Containers. QoS classes are used by Kubernetes to decide which Pods to evict from a Node experiencing [Node Pressure](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/). The possible QoS classes are `Guaranteed`, `Burstable`, and `BestEffort`. When a Node runs out of resources, Kubernetes will first evict `BestEffort` Pods running on that Node, followed by `Burstable` and finally `Guaranteed` Pods. 

1. **`Guaranteed` (æœ€é«˜ä¼˜å…ˆçº§)**
   - **æ¡ä»¶**: Pod ä¸­çš„**æ¯ä¸€ä¸ª**å®¹å™¨éƒ½å¿…é¡»åŒæ—¶è®¾ç½®äº† CPU å’Œå†…å­˜çš„ `requests` å’Œ `limits`ï¼Œå¹¶ä¸”å¯¹äºŽæ¯ä¸€ç§èµ„æºï¼Œ`requests` çš„å€¼å¿…é¡»**ä¸¥æ ¼ç­‰äºŽ** `limits` çš„å€¼ã€‚
   - **ç‰¹ç‚¹**: è¿™äº› Pod çš„èµ„æºéœ€æ±‚æ˜¯å®Œå…¨å¯é¢„æµ‹çš„ã€‚åªè¦ä¸è¶…è¿‡ `limits`ï¼Œå®ƒä»¬å°±èƒ½èŽ·å¾—æ‰€è¯·æ±‚çš„èµ„æºã€‚åœ¨èŠ‚ç‚¹èµ„æºç´§å¼ æ—¶ï¼Œè¿™ç±» Pod æœ€åŽæ‰ä¼šè¢«é©±é€ã€‚
2. **`BestEffort` (æœ€ä½Žä¼˜å…ˆçº§)**
   - **æ¡ä»¶**: Pod ä¸­çš„ä»»ä½•ä¸€ä¸ªå®¹å™¨éƒ½æ²¡æœ‰è®¾ç½®ä»»ä½• `requests` æˆ– `limits`ã€‚
   - **ç‰¹ç‚¹**: è¿™äº› Pod æ²¡æœ‰ä»»ä½•èµ„æºä¿éšœï¼Œå®ƒä»¬ä¼šä½¿ç”¨èŠ‚ç‚¹ä¸Šä¸€åˆ‡å¯ç”¨çš„ç©ºé—²èµ„æºã€‚å½“èŠ‚ç‚¹èµ„æºç´§å¼ æ—¶ï¼Œè¿™ç±» Pod æ˜¯**æœ€å…ˆè¢«é©±é€**çš„ã€‚
3. **`Burstable` (ä¸­ç­‰ä¼˜å…ˆçº§)**
   - **æ¡ä»¶**: Pod ä¸æ»¡è¶³ `Guaranteed` å’Œ `BestEffort` çš„ä»»ä½•ä¸€ä¸ªæ¡ä»¶ã€‚æ¢å¥è¯è¯´ï¼Œåªè¦ Pod ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå®¹å™¨è®¾ç½®äº† `requests`ï¼Œä½†åˆä¸å®Œå…¨æ»¡è¶³ `Guaranteed` çš„ä¸¥æ ¼è¦æ±‚ï¼Œå®ƒå°±æ˜¯ `Burstable`ã€‚
   - **ç‰¹ç‚¹**: è¿™ç±» Pod èŽ·å¾—äº†ä¸€å®šç¨‹åº¦çš„èµ„æºä¿éšœï¼ˆç”± `requests` ä¿è¯ï¼‰ï¼ŒåŒæ—¶å…è®¸åœ¨èŠ‚ç‚¹èµ„æºæœ‰å¯Œä½™æ—¶ï¼Œä½¿ç”¨è¶…è¿‡å…¶ `requests` çš„èµ„æºï¼Œæœ€å¤šä¸èƒ½è¶…è¿‡å…¶ `limits`ï¼ˆå¦‚æžœè®¾ç½®äº†çš„è¯ï¼‰ã€‚åœ¨èµ„æºç´§å¼ æ—¶ï¼Œå®ƒä»¬çš„é©±é€ä¼˜å…ˆçº§ä»‹äºŽ `Guaranteed` å’Œ `BestEffort` ä¹‹é—´ã€‚

##### Some behavior is independent of QoS class

For example:

- Any Container exceeding a resource limit will be killed and restarted by the kubelet without affecting other Containers in that Pod.
- If a Container exceeds its resource request and the node it runs on faces resource pressure, the Pod it is in becomes a candidate for [eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/). If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a replacement Pod, usually on a different node.
- The resource request of a Pod is equal to the sum of the resource requests of its component Containers, and the resource limit of a Pod is equal to the sum of the resource limits of its component Containers.
- The kube-scheduler does not consider QoS class when selecting which Pods to [preempt](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption). Preemption can occur when a cluster does not have enough resources to run all the Pods you defined.

#### Downward API

There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API.

##### Available fields

You can pass information from available Pod-level fields using `fieldRef`. [fieldRef](https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-fieldRef)

You can pass information from available Container-level fields using `resourceFieldRef`.

### Workload management

#### Deployments

##### Creating a Deployment

* Do not manage ReplicaSets owned by a Deployment.

* Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets).

  > ä¸è¦è®©ä¸åŒçš„æŽ§åˆ¶å™¨[[å®žä¾‹]ï¼ˆControllerï¼‰ä½¿ç”¨å¯ä»¥åŒ¹é…åˆ°åŒä¸€æ‰¹ Pod çš„é€‰æ‹©å™¨ï¼ˆSelectorï¼‰

* The `pod-template-hash` label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.

##### Updating a Deployment

If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match `.spec.selector` but whose template does not match `.spec.template` is scaled down.

> ReplicaSetåŽ»æŽ§åˆ¶ selectoræ²¡æœ‰å˜çš„; .spec.templateå˜åŒ–äº†

###### Label selector updates 

- Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too, otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet.

  > å¦‚æžœselctorå¢žåŠ äº†æ–°çš„label, å¯¹åº”çš„`spec.template.metadata.labels`ä¹Ÿè¦åŠ ä¸Šè¿™ä¸ªæ–°çš„label.  æ›´æ–°åŽæ—§çš„ReplicaSetså¹¶ä¸ä¼šè‡ªåŠ¨è¢«æ¸…é™¤

- Selector updates changes the existing value in a selector key -- result in the same behavior as additions.

- Selector removals removes an existing key from the Deployment selector -- do not require any changes in the Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the removed label still exists in any existing Pods and ReplicaSets.

##### Rolling Back a Deployment

when you roll back to an earlier revision, only the Deployment's Pod template part is rolled back.

> ä¸ä¼šæ”¹å˜çš„: ä½ æ‰‹åŠ¨è®¾ç½®çš„å‰¯æœ¬æ•° replicas: 5 ä¸ä¼šå›žæ»šåˆ°åˆå§‹çš„ 3ã€‚å› ä¸º replicas å­—æ®µä¸å±žäºŽ .spec.template (Pod æ¨¡æ¿) çš„ä¸€éƒ¨åˆ†ã€‚å®ƒå±žäºŽ Deployment çš„æŽ§åˆ¶å™¨ç­–ç•¥ã€‚

###### Checking Rollout History of a Deployment

check the revisions of this Deployment:

```shell
kubectl rollout history deployment/nginx-deployment
```

To see the details of each revision, run:

```shell
kubectl rollout history deployment/nginx-deployment --revision=2
```

###### Rolling Back to a Previous Revision

decided to undo the current rollout and rollback to the previous revision:

```shell
kubectl rollout undo deployment/nginx-deployment
```

 you can rollback to a specific revision by specifying it with `--to-revision`:

```shell
kubectl rollout undo deployment/nginx-deployment --to-revision=2
```

##### Scaling a Deployment

###### Proportional scaling

æ¯”ä¾‹æ‰©å®¹æ˜¯æŒ‡ï¼Œå½“ä¸€ä¸ªæ­£åœ¨è¿›è¡Œç‰ˆæœ¬æ›´æ–°çš„ Deployment éœ€è¦æ‰©å®¹æ—¶ï¼ŒKubernetes ä¸ä¼šæŠŠæ‰€æœ‰çš„æ–°å¢ž Pod éƒ½åˆ›å»ºæˆæ–°ç‰ˆæœ¬ï¼Œè€Œæ˜¯ä¼šæŒ‰ç…§å½“å‰æ–°æ—§ç‰ˆæœ¬çš„ Pod æ•°é‡æ¯”ä¾‹ï¼Œæ¥åˆ†é…è¿™äº›æ–°å¢žçš„ Podã€‚ æŒ‰æ¯”ä¾‹æ–°å¢žå®Œä»¥åŽï¼Œå†å‡çº§åˆ°æ–°çš„ç‰ˆæœ¬

- åªæœ‰åœ¨æ»šåŠ¨æ›´æ–°çš„ç‰¹å®šçª—å£æœŸå†…ï¼Œæ¯”ä¾‹æ‰©å®¹æœºåˆ¶æ‰ä¼šè¢«æ¿€æ´»å’Œä½¿ç”¨

##### Pausing and Resuming a rollout of a Deployment

å½“ä¸€ä¸ª Deployment çš„å‘å¸ƒï¼ˆrolloutï¼‰è¢«æš‚åœï¼ˆpausedï¼‰åŽï¼Œä½ å¯¹å…¶æ¨¡æ¿ï¼ˆtemplateï¼‰æ‰€åšçš„ä»»ä½•åŽç»­æ›´æ”¹ï¼Œä¾‹å¦‚ä½¿ç”¨ kubectl set image æ›´æ–°é•œåƒï¼Œéƒ½ä»…ä»…æ˜¯æ›´æ–°äº† Deployment è¿™ä¸ªå¯¹è±¡åœ¨ Kubernetes API Server ä¸­çš„å®šä¹‰ï¼ˆSpecï¼‰ã€‚ç„¶è€Œï¼ŒDeployment Controllerï¼ˆæŽ§åˆ¶å™¨ï¼‰å› ä¸ºæ”¶åˆ°äº†â€œæš‚åœâ€æŒ‡ä»¤ï¼Œæ‰€ä»¥å®ƒä¸ä¼šè§¦å‘ä»»ä½•å®žé™…çš„æ»šåŠ¨æ›´æ–°æ“ä½œã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä¸ä¼šåŽ»åˆ›å»ºæ–°çš„ ReplicaSetï¼Œä¹Ÿä¸ä¼šç”¨æ–°é•œåƒåŽ»åˆ›å»ºæ–°çš„ Podã€‚

åªæœ‰å½“ä½ æ‰§è¡Œ kubectl rollout resume å‘½ä»¤åŽï¼ŒDeployment Controller æ‰ä¼šè§£é™¤æš‚åœçŠ¶æ€ï¼Œç„¶åŽå®ƒä¼šåŽ»æ¯”è¾ƒå½“å‰çš„æ´»åŠ¨çŠ¶æ€å’Œä½ åœ¨æš‚åœæœŸé—´æ‰€åšçš„å…¨éƒ¨ä¿®æ”¹åŽçš„æœŸæœ›çŠ¶æ€ï¼ˆDesired Stateï¼‰

- è¿™ç§â€œæš‚åœ-ä¿®æ”¹-æ¢å¤â€çš„æœºåˆ¶éžå¸¸æœ‰ç”¨ï¼Œå®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªçª—å£æœŸï¼Œè®©æˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ªå‘å¸ƒå‘¨æœŸå†…å®‰å…¨åœ°åº”ç”¨å¤šä¸ªå˜æ›´ï¼Œè€Œä¸æ˜¯æ¯åšä¸€ä¸ªå°æ”¹åŠ¨å°±è§¦å‘ä¸€æ¬¡æ»šåŠ¨æ›´æ–°ã€‚

#### StatefulSets

##### Using StatefulSets

StatefulSets are valuable for applications that require one or more of the following.

- Stable, unique network identifiers.
- Stable, persistent storage.
- Ordered, graceful deployment and scaling.
- Ordered, automated rolling updates.

In the above, stable is synonymous with persistence across Pod (re)scheduling. 

##### Limitations

* Deleting and/or scaling a StatefulSet down will *not* delete the volumes associated with the StatefulSet. 

* StatefulSets currently require a [Headless Service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) to be responsible for the network identity of the Pods. You are responsible for creating this Service.

* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.

  > Kubernetes ä¼šç«‹å³å¼€å§‹æ¸…ç†å…¶æ‰€å±žçš„ Podsï¼Œä½†è¿™ä¸ªè¿‡ç¨‹ä¸ä¿è¯é¡ºåºï¼Œä¹Ÿä¸ä¿è¯ Pods èƒ½ä¼˜é›…åœ°å…³é—­

* StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.

##### Example

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  minReadySeconds: 10 # by default is 0
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: registry.k8s.io/nginx-slim:0.24
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
```

##### Pod Identity

StatefulSet Pods have a unique identity that consists of an ordinal, a stable network identity, and stable storage. The identity sticks to the Pod, regardless of which node it's (re)scheduled on.

###### Ordinal Index

For a StatefulSet with N [replicas](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#replicas), each Pod in the StatefulSet will be assigned an integer ordinal, that is unique over the Set. By default, pods will be assigned ordinals from 0 up through N-1. The StatefulSet controller will also add a pod label with this index: `apps.kubernetes.io/pod-index`.

###### Start ordinal

- `.spec.ordinals.start`: If the `.spec.ordinals.start` field is set, Pods will be assigned ordinals from `.spec.ordinals.start` up through `.spec.ordinals.start + .spec.replicas - 1`.

###### Stable Network ID

how that affects the DNS names for the StatefulSet's Pods.

| Cluster Domain | Service (ns/name) | StatefulSet (ns/name) | StatefulSet Domain              | Pod DNS                                      | Pod Hostname |
| -------------- | ----------------- | --------------------- | ------------------------------- | -------------------------------------------- | ------------ |
| cluster.local  | default/nginx     | default/web           | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |
| cluster.local  | foo/nginx         | foo/web               | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |
| kube.local     | foo/nginx         | foo/web               | nginx.foo.svc.kube.local        | web-{0..N-1}.nginx.foo.svc.kube.local        | web-{0..N-1} |

###### Stable Storage

Pod çš„è°ƒåº¦ä½ç½®ä¸ä¼šå½±å“ StatefulSet åˆ›å»ºçš„ PV æ•°é‡ã€‚åªè¦ replicas: 3 å¹¶ä¸”å®šä¹‰äº† volumeClaimTemplatesï¼Œå°±ä¸€å®šä¼šåˆ›å»º 3 ä¸ª PVCï¼Œè¿›è€Œè§¦å‘åˆ›å»º 3 ä¸ª PVï¼Œæ— è®ºè¿™äº› Pod æœ€ç»ˆåœ¨å“ªé‡Œè¿è¡Œã€‚

 Note that, the PersistentVolumes associated with the Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be done manually.

###### Pod Name Label

When the StatefulSet [controller](https://kubernetes.io/docs/concepts/architecture/controller/) creates a Pod, it adds a label, `statefulset.kubernetes.io/pod-name`, that is set to the name of the Pod. This label allows you to attach a Service to a specific Pod in the StatefulSet.

##### Deployment and Scaling Guarantees

- For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.
- When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
- Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
- Before a Pod is terminated, all of its successors must be completely shutdown.

The StatefulSet should not specify a `pod.Spec.TerminationGracePeriodSeconds` of 0. 

##### Pod Management Policies

via its `.spec.podManagementPolicy` field. 

* `OrderedReady` pod management is the default for StatefulSets.
* `Parallel` pod management tells the StatefulSet controller to launch or terminate all Pods in parallel

##### Update strategies

There are two possible values for a StatefulSet's `.spec.updateStrategy` field.

- `OnDelete`

  When a StatefulSet's `.spec.updateStrategy.type` is set to `OnDelete`, the StatefulSet controller will not automatically update the Pods in a StatefulSet. Users must manually delete Pods to cause the controller to create new Pods that reflect modifications made to a StatefulSet's `.spec.template`.

- `RollingUpdate`

  The `RollingUpdate` update strategy implements automated, rolling updates for the Pods in a StatefulSet. This is the default update strategy.

##### Rolling Updates

When a StatefulSet's `.spec.updateStrategy.type` is set to `RollingUpdate`, the StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod termination (from the largest ordinal to the smallest), updating each Pod one at a time.

###### Partitioned rolling updates

The `RollingUpdate` update strategy can be partitioned, by specifying a `.spec.updateStrategy.rollingUpdate.partition`.

`partition`ï¼ˆåˆ†åŒºï¼‰æ˜¯Kubernetes StatefulSetä¸­`RollingUpdate`ï¼ˆæ»šåŠ¨æ›´æ–°ï¼‰ç­–ç•¥çš„ä¸€ä¸ªæ ¸å¿ƒå±žæ€§ã€‚å®ƒå…è®¸ä½ å¯¹æœ‰çŠ¶æ€åº”ç”¨ï¼ˆå¦‚æ•°æ®åº“ï¼‰è¿›è¡Œ**éƒ¨åˆ†æ›´æ–°**æˆ–**é˜¶æ®µæ€§å‘å¸ƒ**ã€‚

å®ƒçš„å·¥ä½œåŽŸç†æ˜¯å……å½“ä¸€ä¸ªâ€œåˆ†ç•Œçº¿â€ï¼Œå°†æ‰€æœ‰Podå‰¯æœ¬æ ¹æ®å…¶åºå·ï¼ˆordinalï¼Œå³`pod-0`, `pod-1`...ï¼‰åˆ†ä¸ºä¸¤ä¸ªé›†åˆï¼š

1. **â€œæ›´æ–°åŒºâ€ (Update Zone):**
   - **è§„åˆ™ï¼š** åºå· **å¤§äºŽæˆ–ç­‰äºŽ ** `partition` å€¼çš„Podã€‚
   - **è¡Œä¸ºï¼š** å½“ä½ æ›´æ–°StatefulSetçš„Podæ¨¡æ¿ï¼ˆ`.spec.template`ï¼Œä¾‹å¦‚æ›´æ¢é•œåƒï¼‰æ—¶ï¼Œè¿™ä¸ªåŒºåŸŸçš„Pod**ä¼šè¢«**è‡ªåŠ¨æ»šåŠ¨æ›´æ–°åˆ°æ–°ç‰ˆæœ¬ã€‚
2. **â€œé”å®šåŒºâ€ (Locked Zone):**
   - **è§„åˆ™ï¼š** åºå· **å°äºŽ** `partition` å€¼çš„Podã€‚
   - **è¡Œä¸ºï¼š** è¿™ä¸ªåŒºåŸŸçš„Pod**ä¸ä¼š**è¢«æ›´æ–°ï¼Œå®ƒä»¬ä¼šè¢«â€œé”å®šâ€åœ¨å½“å‰ï¼ˆæ—§çš„ï¼‰ç‰ˆæœ¬ã€‚
   - **å…³é”®ä¿æŠ¤æœºåˆ¶ï¼š** å³ä½¿ä½ æ‰‹åŠ¨åˆ é™¤ä¸€ä¸ªâ€œé”å®šåŒºâ€çš„Podï¼ŒKubernetesä¹Ÿä¼š**ä½¿ç”¨æ—§ç‰ˆæœ¬çš„æ¨¡æ¿**æ¥é‡å»ºå®ƒï¼Œä»¥é˜²æ­¢æ„å¤–å‡çº§ã€‚

##### Revision history

Control retained revisions with `.spec.revisionHistoryLimit`:

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: webapp
spec:
  revisionHistoryLimit: 5  # Keep last 5 revisions
```

You can revert to a previous configuration using:

```shell
# View revision history
kubectl rollout history statefulset/webapp

# Rollback to a specific revision
kubectl rollout undo statefulset/webapp --to-revision=3
```

To view associated ControllerRevisions:

```shell
# List all revisions for the StatefulSet
kubectl get controllerrevisions -l app.kubernetes.io/name=webapp

# View detailed configuration of a specific revision
kubectl get controllerrevision/webapp-3 -o yaml
```

##### PersistentVolumeClaim retention

The optional `.spec.persistentVolumeClaimRetentionPolicy` field controls if and how PVCs are deleted during the lifecycle of a StatefulSet.

è¿™ä¸ªç­–ç•¥ä¸‹æœ‰ä¸¤ä¸ªå­ç­–ç•¥ï¼Œå®ƒä»¬æŽ§åˆ¶ç€ä¸åŒåœºæ™¯ä¸‹çš„ PVC è¡Œä¸ºï¼š

1. **`whenDeleted`**
   - **è§¦å‘æ—¶æœºï¼š** å½“æ•´ä¸ª StatefulSet èµ„æº**è¢«åˆ é™¤**æ—¶ï¼ˆä¾‹å¦‚ï¼Œä½ æ‰§è¡Œäº† `kubectl delete statefulset my-app`ï¼‰ã€‚
   - **æŽ§åˆ¶å¯¹è±¡ï¼š** *æ‰€æœ‰*ç”±è¿™ä¸ª StatefulSet åˆ›å»ºçš„ PVCã€‚
2. **`whenScaled`**
   - **è§¦å‘æ—¶æœºï¼š** å½“ StatefulSet çš„å‰¯æœ¬æ•°ï¼ˆ`replicas`ï¼‰**è¢«è°ƒå°**æ—¶ï¼ˆå³â€œç¼©å®¹â€ï¼Œä¾‹å¦‚ä»Ž 5 ä¸ªå‰¯æœ¬ç¼©å‡åˆ° 3 ä¸ªå‰¯æœ¬ï¼‰ã€‚
   - **æŽ§åˆ¶å¯¹è±¡ï¼š** *ä»…ä»…*é‚£äº›å› ç¼©å®¹è€Œè¢«åˆ é™¤çš„ Pod æ‰€å¯¹åº”çš„ PVCï¼ˆåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œå°±æ˜¯ `my-app-4` å’Œ `my-app-3` å¯¹åº”çš„ PVCï¼‰ã€‚

 ç­–ç•¥çš„ä¸¤ä¸ªé€‰é¡¹ å¯¹äºŽä¸Šè¿°çš„æ¯ä¸€ç§åœºæ™¯ï¼Œä½ éƒ½æœ‰ä¸¤ç§è¡Œä¸ºé€‰é¡¹ï¼š

1. **`Retain` (é»˜è®¤å€¼)**
   - **å«ä¹‰ï¼š** ä¿ç•™ã€‚
   - **è¡Œä¸ºï¼š** è¿™å°±æ˜¯ Kubernetes çš„ç»å…¸è¡Œä¸ºã€‚å³ä½¿ Pod è¢«åˆ é™¤ï¼ŒPVC ä¹Ÿä¼šè¢«ä¿ç•™åœ¨é›†ç¾¤ä¸­ã€‚
   - **é€‚ç”¨åœºæ™¯ï¼š** ç”Ÿäº§çŽ¯å¢ƒçš„æ•°æ®åº“ã€å…³é”®æ•°æ®å­˜å‚¨ã€‚**æ•°æ®çš„å®‰å…¨æ€§æ˜¯ç¬¬ä¸€ä½çš„ã€‚**
2. **`Delete`**
   - **å«ä¹‰ï¼š** åˆ é™¤ã€‚
   - **è¡Œä¸ºï¼š** å½“å…³è”çš„ Pod è¢«ç»ˆæ­¢*ä¹‹åŽ*ï¼ŒKubernetes ä¼šè‡ªåŠ¨åˆ é™¤è¯¥ PVCã€‚
   - **é€‚ç”¨åœºæ™¯ï¼š**
     - å¼€å‘/æµ‹è¯•çŽ¯å¢ƒï¼šå¿«é€Ÿæ¸…ç†èµ„æºï¼Œé¿å…åžƒåœ¾å †ç§¯ã€‚
     - æ•°æ®å¯å†ç”Ÿåº”ç”¨ï¼šä¾‹å¦‚ä¸€ä¸ªåˆ†å¸ƒå¼ç¼“å­˜é›†ç¾¤ï¼Œç¼“å­˜æ•°æ®ä¸¢å¤±åŽå¯ä»¥é‡æ–°ç”Ÿæˆã€‚
     - ä¸´æ—¶æ•°æ®å¤„ç†ï¼šPod åªæ˜¯ç”¨ PVC åšä¸´æ—¶è½åœ°ï¼ŒPod æ²¡äº†æ•°æ®ä¹Ÿå°±ä¸éœ€è¦äº†ã€‚

#### DaemonSet

##### Writing a DaemonSet Spec

###### Create a DaemonSet

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v5.0.1
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      # it may be desirable to set a high priority class to ensure that a DaemonSet Pod
      # preempts running Pods
      # priorityClassName: important
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log

```

###### Running Pods on select Nodes

if you specify a `.spec.template.spec.affinity`, then DaemonSet controller will create Pods on nodes which match that [node affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/). If you do not specify either, then the DaemonSet controller will create Pods on all nodes.

##### How Daemon Pods are scheduled

A DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod. The DaemonSet controller creates a Pod for each eligible node and adds the `spec.affinity.nodeAffinity` field of the Pod to match the target host. After the Pod is created, the default scheduler typically takes over and then binds the Pod to the target host by setting the `.spec.nodeName` field. If the new Pod cannot fit on the node, the default scheduler may preempt (evict) some of the existing Pods based on the [priority](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority) of the new Pod.

> æŠ¢å è¡Œä¸ºå®Œå…¨æ˜¯åŸºäºŽä¼˜å…ˆçº§ (Priority) çš„ï¼Œè€Œä¼˜å…ˆçº§æ˜¯é€šè¿‡ PriorityClass æ¥å®šä¹‰çš„

###### Taints and tolerations

The DaemonSet controller automatically adds a set of [tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) to DaemonSet Pods:

| Toleration key                                               | Effect       | Details                                                      |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ |
| [`node.kubernetes.io/not-ready`](https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready) | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted. |
| [`node.kubernetes.io/unreachable`](https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable) | `NoExecute`  | DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted. |
| [`node.kubernetes.io/disk-pressure`](https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with disk pressure issues. |
| [`node.kubernetes.io/memory-pressure`](https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with memory pressure issues. |
| [`node.kubernetes.io/pid-pressure`](https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes with process pressure issues. |
| [`node.kubernetes.io/unschedulable`](https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable) | `NoSchedule` | DaemonSet Pods can be scheduled onto nodes that are unschedulable. |
| [`node.kubernetes.io/network-unavailable`](https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable) | `NoSchedule` | **Only added for DaemonSet Pods that request host networking**, i.e., Pods having `spec.hostNetwork: true`. Such DaemonSet Pods can be scheduled onto nodes with unavailable network. |

You can add your own tolerations to the Pods of a DaemonSet as well, by defining these in the Pod template of the DaemonSet.

> ðŸ“ Taint Effect: `NoExecute` æ ¸å¿ƒç¬”è®°
>
> `NoExecute` æ˜¯ Taintï¼ˆæ±¡ç‚¹ï¼‰ä¸‰ç§æ•ˆæžœä¸­æœ€â€œå¼ºåŠ›â€çš„ä¸€ç§ï¼Œå®ƒçš„æ ¸å¿ƒæ˜¯**é©±é€ï¼ˆEvictionï¼‰**ã€‚
>
> 1. æ ¸å¿ƒåŠŸèƒ½ï¼šé©±é€æ­£åœ¨è¿è¡Œçš„ Pod
>
> - **è§¦å‘æ¡ä»¶**ï¼šå½“ä¸€ä¸ª Node è¢«æ·»åŠ  `effect: NoExecute` çš„ Taintã€‚
> - **ç«‹å³æ‰§è¡Œ**ï¼šK8s ä¼š**ç«‹å³**æ£€æŸ¥è¯¥ Node ä¸Šæ‰€æœ‰æ­£åœ¨è¿è¡Œçš„ Podã€‚
> - **é©±é€å¯¹è±¡**ï¼šæ‰€æœ‰**æ²¡æœ‰**åŒ¹é…æ­¤ Taint çš„ `toleration`ï¼ˆå®¹å¿ï¼‰çš„ Podï¼Œéƒ½ä¼š**ç«‹åˆ»**è¢«å¯åŠ¨é©±é€æµç¨‹ã€‚
> - **æ–° Pod è°ƒåº¦**ï¼š`NoExecute` æ•ˆæžœä¹ŸåŒ…å«äº† `NoSchedule` çš„åŠŸèƒ½ï¼Œå³æ–°çš„ Pod ä¹Ÿæ— æ³•è°ƒåº¦åˆ°è¿™ä¸ª Node ä¸Šï¼ˆé™¤éžå®ƒä»¬æœ‰åŒ¹é…çš„ Tolerationï¼‰ã€‚
>
> 2. é©±é€è¿‡ç¨‹ï¼šä¼˜é›…ç»ˆæ­¢ (Graceful)
>
> é©±é€å¹¶éžçž¬æ—¶çš„å¼ºåˆ¶æ€æ­»ï¼ˆ`kill -9`ï¼‰ï¼š
>
> - Pod çŠ¶æ€å˜ä¸º `Terminating`ã€‚
> - Kubelet å¼€å§‹æ‰§è¡Œ Pod çš„**ç»ˆæ­¢å®½é™æœŸ**ï¼ˆ`terminationGracePeriodSeconds`ï¼Œé»˜è®¤ 30sï¼‰ã€‚
> - Pod è¿›ç¨‹æ”¶åˆ° `SIGTERM` ä¿¡å·ï¼Œæœ‰æœºä¼šâ€œä¼˜é›…åœ°â€å…³é—­ã€‚
> - å®½é™æœŸç»“æŸåŽï¼Œå¦‚æžœ Pod ä»æœªé€€å‡ºï¼Œæ‰ä¼šè¢« `SIGKILL` å¼ºåˆ¶ç»ˆæ­¢ã€‚
> - ä¸Žæ­¤åŒæ—¶ï¼ŒDeployment ç­‰æŽ§åˆ¶å™¨ä¼šåœ¨å…¶ä»–å¯ç”¨èŠ‚ç‚¹ä¸Šåˆ›å»ºæ–°çš„æ›¿ä»£ Podã€‚
>
> 3. å…³é”®é…ç½®ï¼š`tolerationSeconds`
>
> `tolerationSeconds` æ˜¯ `NoExecute` å®¹å¿ä¸­ä¸€ä¸ª**æžå…¶é‡è¦**çš„å¯é€‰é…ç½®ã€‚
>
> - **ç›®çš„**ï¼šå…è®¸ Pod "ä¸´æ—¶å®¹å¿"ä¸€ä¸ª `NoExecute` Taint ä¸€æ®µæ—¶é—´ï¼Œè€Œä¸æ˜¯ç«‹å³è¢«é©±é€ã€‚
> - **å·¥ä½œæ–¹å¼**ï¼š
>   - Pod å¿…é¡»**æœ‰**åŒ¹é…çš„ `toleration` æ‰èƒ½ä½¿ç”¨æ­¤é…ç½®ã€‚
>   - å½“ Node å‡ºçŽ° `NoExecute` Taint æ—¶ï¼ˆä¾‹å¦‚ `node.kubernetes.io/unreachable`ï¼‰ï¼Œè®¡æ—¶å¼€å§‹ã€‚
>   - Pod ä¼šç»§ç»­è¿è¡Œ `tolerationSeconds` æ‰€æŒ‡å®šçš„ç§’æ•°ã€‚
>   - **Taint æ¶ˆå¤±**ï¼šå¦‚æžœ Taint åœ¨å€’è®¡æ—¶ç»“æŸå‰è¢«ç§»é™¤ï¼ˆå¦‚ Node æ¢å¤ï¼‰ï¼ŒPod ä¼šç»§ç»­æ­£å¸¸è¿è¡Œï¼Œä¸ä¼šè¢«é©±é€ã€‚
>   - **Taint æŒç»­**ï¼šå¦‚æžœå€’è®¡æ—¶ç»“æŸï¼ŒTaint ä»ç„¶å­˜åœ¨ï¼ŒPod å°†è¢«å¯åŠ¨é©±é€æµç¨‹ã€‚
> - **å…¸åž‹ç”¨ä¾‹**ï¼šå¸¸ç”¨äºŽ `StatefulSet`ï¼Œé˜²æ­¢å› çŸ­æš‚çš„ç½‘ç»œåˆ†åŒºï¼ˆNode æš‚æ—¶ `NotReady`ï¼‰å¯¼è‡´ Pod è¢«ç«‹å³é©±é€ï¼Œä»Žè€Œé¿å…æœ‰çŠ¶æ€åº”ç”¨çš„æ•°æ®å’ŒæœåŠ¡ä¸­æ–­ã€‚

##### Updating a DaemonSet

If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete Pods from newly not-matching nodes.

You can delete a DaemonSet. If you specify `--cascade=orphan` with `kubectl`, then the Pods will be left on the nodes. If you subsequently create a new DaemonSet with the same selector, the new DaemonSet adopts the existing Pods. 

#### Jobs

##### Running an example Job

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  ttlSecondsAfterFinished: 60  # ä»»åŠ¡å®ŒæˆåŽ 60 ç§’è‡ªåŠ¨åˆ é™¤ Job å’Œ Pod
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4

```

Check on the status of the Job with `kubectl`:

```bash
kubectl describe job pi
```

##### Writing a Job spec 

###### Job Labels

Job labels will have `batch.kubernetes.io/` prefix for `job-name` and `controller-uid`.

###### Pod Template

Only a [`RestartPolicy`](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Never` or `OnFailure` is allowed.

###### Pod selector

 é»˜è®¤æƒ…å†µä¸‹ä¸åº”æŒ‡å®š (.spec.selector)

> å¦‚æžœç”¨æˆ·å†³å®šè¦†ç›–é»˜è®¤é€»è¾‘å¹¶è‡ªå®šä¹‰ Pod é€‰æ‹©å™¨ï¼Œå¿…é¡»éžå¸¸è°¨æ…Žã€‚ä»¥ä¸‹é£Žé™©ï¼š
>
> â€¢ **æ ‡ç­¾å†²çª**ï¼šå¦‚æžœä½ æŒ‡å®šçš„æ ‡ç­¾é€‰æ‹©å™¨ä¸æ˜¯å”¯ä¸€çš„ï¼Œä¸”åŒ¹é…åˆ°äº†å…¶ä»– Job çš„ Podï¼Œå¯èƒ½ä¼šå¯¼è‡´éžé¢„æœŸè¡Œä¸ºã€‚
>
> â€¢ **é”™è¯¯çš„ Pod ç®¡ç†**ï¼š
>
>   â—¦ å±žäºŽå…¶ä»– Job çš„ Pod å¯èƒ½ä¼šè¢«è¯¯åˆ ã€‚
>
>   â—¦ è¯¥ Job å¯èƒ½ä¼šå°†å…¶ä»–ä¸ç›¸å…³çš„ Pod è®¡å…¥è‡ªå·±çš„å®Œæˆè®¡æ•°ã€‚
>
>   â—¦ ä¸€ä¸ªæˆ–å¤šä¸ª Job å¯èƒ½ä¼šå› æ­¤æ‹’ç»åˆ›å»º Pod æˆ–æ— æ³•è¿è¡Œè‡³å®Œæˆã€‚
>
> â€¢ **çº§è”å½±å“**ï¼šéžå”¯ä¸€çš„é€‰æ‹©å™¨è¿˜å¯èƒ½å¯¼è‡´å…¶ä»–æŽ§åˆ¶å™¨ï¼ˆå¦‚ ReplicationControllerï¼‰åŠå…¶ Pod è¡¨çŽ°å‡ºä¸å¯é¢„æµ‹çš„è¡Œä¸ºã€‚
>
> â€¢ **ç¼ºä¹æ‹¦æˆª**ï¼šKubernetes **ä¸ä¼šé˜»æ­¢**ç”¨æˆ·åœ¨æŒ‡å®š `.spec.selector` æ—¶çŠ¯é”™

###### Parallel execution for Jobs

1. **éžå¹¶è¡Œ Job (Non-parallel Job)**

è¿™æ˜¯æœ€ç®€å•ã€æœ€å¸¸è§çš„ Job ç±»åž‹ï¼Œé€šå¸¸ç”¨äºŽæ‰§è¡Œä¸€æ¬¡æ€§çš„è¿ç»´ä»»åŠ¡ã€‚

- **æ ¸å¿ƒç‰¹å¾**ï¼š
  - é€šå¸¸åªå¯åŠ¨ **ä¸€ä¸ª** Podã€‚
  - åªè¦è¿™ä¸ª Pod **æˆåŠŸç»ˆæ­¢**ï¼ˆExit Code 0ï¼‰ï¼Œæ•´ä¸ª Job å°±è§†ä¸ºå®Œæˆã€‚
- **é…ç½®æ–¹å¼**ï¼š
  - `.spec.completions`ï¼š**ä¸è®¾ç½®**ï¼ˆé»˜è®¤ä¸º 1ï¼‰ã€‚
  - `.spec.parallelism`ï¼š**ä¸è®¾ç½®**ï¼ˆé»˜è®¤ä¸º 1ï¼‰ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼š
  - æ•°æ®åº“è¿ç§»è„šæœ¬ (Database Migration)ã€‚
  - ä¸€æ¬¡æ€§çš„å¤‡ä»½ä»»åŠ¡ã€‚

2. **å›ºå®šå®Œæˆè®¡æ•° (Fixed Completion Count)**

å½“ä½ æœ‰ä¸€å †ä»»åŠ¡éœ€è¦å¤„ç†ï¼Œå¹¶ä¸”ä½ æ˜Žç¡®çŸ¥é“ä»»åŠ¡çš„æ€»é‡æ—¶ä½¿ç”¨æ­¤æ¨¡å¼ã€‚

- **æ ¸å¿ƒç‰¹å¾**ï¼š
  - éœ€è¦è®¾ç½®ä¸€ä¸ªéžé›¶çš„æ­£æ•´æ•°ä½œä¸ºç›®æ ‡ã€‚
  - Job Controller ä¼šä¸æ–­åˆ›å»º Podï¼Œç›´åˆ°ç´¯è®¡æœ‰æŒ‡å®šæ•°é‡ï¼ˆ`.spec.completions`ï¼‰çš„ Pod æˆåŠŸé€€å‡ºã€‚
- **é…ç½®æ–¹å¼**ï¼š
  - `.spec.completions`ï¼šè®¾ç½®ä¸º **N** (éžé›¶æ­£å€¼)ã€‚
  -  `parallelism` :  è®¾ç½®æ¯æ¬¡å…è®¸   N ä¸ª Pod å¹¶è¡Œè·‘
- **å®Œæˆæ¨¡å¼ (Completion Mode)** - *K8s v1.21+*ï¼š
  - **NonIndexed (é»˜è®¤)**ï¼š
    - Pod çš„å®Œæˆæ˜¯åŒè´¨çš„ï¼ˆHomogenousï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒPod A å®Œæˆå’Œ Pod B å®Œæˆæ²¡æœ‰åŒºåˆ«ï¼Œåªæ˜¯è®¡æ•°å™¨ +1ã€‚
  - **Indexed (ç´¢å¼•æ¨¡å¼)**ï¼š
    - **æ ¸å¿ƒæ¦‚å¿µ**ï¼šæ¯ä¸ª Pod ä¼šèŽ·å¾—ä¸€ä¸ªä»Ž `0` åˆ° `N-1` çš„å”¯ä¸€ç´¢å¼•ã€‚
    - **èŽ·å–ç´¢å¼•æ–¹å¼**ï¼šç¨‹åºå¯ä»¥é€šè¿‡ Pod çš„ Annotationã€Labelã€Hostname æˆ– çŽ¯å¢ƒå˜é‡ (`JOB_COMPLETION_INDEX`) èŽ·å–å½“å‰å¤„ç†çš„æ˜¯ç¬¬å‡ ä¸ªä»»åŠ¡ã€‚
    - **å®Œæˆæ¡ä»¶**ï¼šæ¯ä¸ªç´¢å¼•ï¼ˆ0, 1, ... N-1ï¼‰éƒ½å¿…é¡»æœ‰ä¸€ä¸ªå¯¹åº”çš„ Pod æˆåŠŸå®Œæˆã€‚
    - **é€‚ç”¨åœºæ™¯**ï¼šé™æ€åˆ†ç‰‡å¤„ç†ã€‚ä¾‹å¦‚ï¼Œæœ‰ 10 ä¸ªå¤§æ–‡ä»¶éœ€è¦è½¬ç ï¼Œä½ å¯ä»¥å¯åŠ¨ 10 ä¸ª Job Podï¼ŒPod-0 å¤„ç† `file-0.mp4`ï¼ŒPod-1 å¤„ç† `file-1.mp4`ã€‚

3. å·¥ä½œé˜Ÿåˆ— (Work Queue)

è¿™ç§æ¨¡å¼é€šå¸¸ç”¨äºŽå¹¶è¡Œå¤„ç†ï¼Œä½†ä»»åŠ¡æ€»æ•°ä¸å›ºå®šï¼Œæˆ–è€…ç”±å·¥ä½œé˜Ÿåˆ—ï¼ˆå¦‚ RabbitMQ, Redisï¼‰æ¥å†³å®šä½•æ—¶ç»“æŸã€‚

- **æ ¸å¿ƒç‰¹å¾**ï¼š
  - Pod å¿…é¡»èƒ½å¤Ÿé€šè¿‡å¤–éƒ¨æœåŠ¡ï¼ˆé˜Ÿåˆ—ï¼‰æˆ–è€…ç›¸äº’åè°ƒæ¥åˆ¤æ–­æ˜¯å¦è¿˜æœ‰å·¥ä½œè¦åšã€‚
  - **å…³é”®ç»ˆæ­¢é€»è¾‘**ï¼šä¸€æ—¦ **ä»»æ„ä¸€ä¸ª Pod æˆåŠŸç»ˆæ­¢**ï¼ŒJob Controller å°±è®¤ä¸ºæ•´ä¸ªä»»åŠ¡é˜Ÿåˆ—å·²ç»ç©ºäº†ï¼ˆå·¥ä½œå®Œæˆï¼‰ã€‚
  - æ­¤æ—¶ï¼ŒJob ä¼šç«‹å³åœæ­¢åˆ›å»ºæ–° Podï¼Œå¹¶å¼€å§‹ç»ˆæ­¢å…¶ä»–è¿˜åœ¨è¿è¡Œçš„ Podã€‚
- **é…ç½®æ–¹å¼**ï¼š
  - `.spec.completions`ï¼š**ä¸è®¾ç½®**ã€‚
  - `.spec.parallelism`ï¼šè®¾ç½®ä¸ºå¤§äºŽ 1 çš„æ•´æ•°ï¼ˆå¯ç”¨å¹¶è¡Œï¼‰ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼š
  - å¤šä¸ª Worker æ¶ˆè´¹è€…ä»Ž RabbitMQ ä¸­æŠ¢ä»»åŠ¡ï¼Œå½“é˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼ŒWorker æ­£å¸¸é€€å‡ºã€‚

###### Controlling parallelism

The requested parallelism (`.spec.parallelism`) can be set to any non-negative value. If it is unspecified, it defaults to 1. If it is specified as 0, then the Job is effectively paused until it is increased.

##### Pod failure policy

åœ¨ Kubernetes æ‰¹å¤„ç†ä»»åŠ¡ï¼ˆJobsï¼‰çš„æ¡†æž¶ä¸‹ï¼Œ**æ•…éšœå¤„ç†ä¸Žç»ˆæ­¢ç­–ç•¥**æ˜¯ç¡®ä¿ä»»åŠ¡å¯é æ€§ã€èµ„æºåˆ©ç”¨çŽ‡ä»¥åŠå¼‚å¸¸æƒ…å†µè‡ªæ„ˆçš„æ ¸å¿ƒæœºåˆ¶ã€‚æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œè¿™äº›ç­–ç•¥å¯ä»¥ä»Žå¤±è´¥é‡è¯•ã€ç²¾ç»†åŒ–ç­–ç•¥æŽ§åˆ¶ã€æ—¶é—´é™åˆ¶ä»¥åŠå®ŒæˆåŽçš„è‡ªåŠ¨æ¸…ç†å››ä¸ªç»´åº¦è¿›è¡Œæ·±å…¥æŽ¢è®¨ã€‚

1. åŸºç¡€æ•…éšœå¤„ç†ï¼šé‡è¯•ä¸Žé€€é¿æœºåˆ¶

Job çš„åŸºæœ¬èŒèƒ½æ˜¯ç¡®ä¿æŒ‡å®šæ•°é‡çš„ Pod æˆåŠŸç»ˆæ­¢ã€‚å½“æ•…éšœå‘ç”Ÿæ—¶ï¼Œç³»ç»Ÿé‡‡ç”¨ä»¥ä¸‹æœºåˆ¶ï¼š

â€¢ **é‡å¯ç­–ç•¥ï¼ˆRestart Policyï¼‰ï¼š** Job ä»…æ”¯æŒ `Never` æˆ– `OnFailure`ã€‚è‹¥è®¾ä¸º `OnFailure`ï¼Œå®¹å™¨å¤±è´¥æ—¶ä¼šåœ¨åŽŸ Pod å†…é‡å¯ï¼›è‹¥ä¸º `Never`ï¼ŒPod å¤±è´¥æ—¶ Job æŽ§åˆ¶å™¨ä¼šåˆ›å»ºæ–° Podã€‚

â€¢ **å›žé€€é™åˆ¶ï¼ˆbackoffLimitï¼‰ï¼š** å­—æ®µ `.spec.backoffLimit` å®šä¹‰äº†åœ¨å°† Job æ ‡è®°ä¸ºå¤±è´¥å‰çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤å€¼ä¸º 6ã€‚

â€¢ **æŒ‡æ•°é€€é¿å»¶è¿Ÿï¼š** å¤±è´¥çš„ Pod ä¼šä»¥æŒ‡æ•°çº§çš„å»¶è¿Ÿï¼ˆ10s, 20s, 40s...ï¼‰é‡æ–°åˆ›å»ºï¼Œå»¶è¿Ÿä¸Šé™ä¸º 6 åˆ†é’Ÿã€‚

â€¢ **æŒ‰ç´¢å¼•é€€é¿ï¼ˆbackoffLimitPerIndexï¼‰ï¼š** åœ¨ç´¢å¼• Job ä¸­ï¼Œå¯ä»¥ä¸ºæ¯ä¸ªç´¢å¼•ç‹¬ç«‹è®¾ç½®é‡è¯•ä¸Šé™ï¼ŒæŸä¸ªç´¢å¼•çš„å¤±è´¥ä¸ä¼šä¸­æ–­å…¶ä»–ç´¢å¼•çš„æ‰§è¡Œã€‚

2. ç²¾ç»†åŒ–å¤±è´¥ç­–ç•¥ï¼ˆPod Failure Policyï¼‰

ä¸ºäº†æ›´çµæ´»åœ°å¤„ç†ä¸åŒç±»åž‹çš„å¤±è´¥ï¼ŒKubernetes æä¾›äº† `.spec.podFailurePolicy`ï¼š

â€¢ **åŸºäºŽè§„åˆ™çš„åŠ¨ä½œï¼š** å¯ä»¥æ ¹æ®å®¹å™¨çš„**é€€å‡ºç **æˆ– **Pod æ¡ä»¶**ï¼ˆå¦‚ `DisruptionTarget` èŠ‚ç‚¹å¹²æ‰°ï¼‰é‡‡å–ä¸åŒè¡ŒåŠ¨ã€‚

â€¢ **å¯é€‰åŠ¨ä½œï¼š**

  â—¦ **FailJobï¼š** ä¸€æ—¦åŒ¹é…ï¼ˆå¦‚å‘çŽ°è½¯ä»¶é€»è¾‘æ¼æ´žçš„ç‰¹å®šé€€å‡ºç ï¼‰ï¼Œç«‹å³åœæ­¢æ•´ä¸ª Job å¹¶ç»ˆæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„ Podã€‚

  â—¦ **Ignoreï¼š** å¿½ç•¥æ­¤ç±»å¤±è´¥ï¼ˆå¦‚å› æŠ¢å å¯¼è‡´çš„ Pod ç»ˆæ­¢ï¼‰ï¼Œä¸è®¡å…¥ `backoffLimit` è®¡æ•°ã€‚

  â—¦ **Count/FailIndexï¼š** æŒ‰ç…§é»˜è®¤æ–¹å¼è®¡æ•°æˆ–ä»…æ ‡è®°å½“å‰ç´¢å¼•å¤±è´¥ã€‚

3. ä¸»åŠ¨ç»ˆæ­¢ä¸ŽæˆåŠŸåˆ¤å®šç­–ç•¥

é™¤äº†è¢«åŠ¨ç­‰å¾… Pod è¿è¡Œï¼ŒJob è¿˜å¯ä»¥ä¸»åŠ¨æŽ§åˆ¶ä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸï¼š

â€¢ **ä¸»åŠ¨æˆªæ­¢æ—¶é—´ï¼ˆactiveDeadlineSecondsï¼‰ï¼š** è¯¥å­—æ®µè®¾ç½® Job çš„è¿è¡Œæ—¶é—´ä¸Šé™ã€‚å®ƒå…·æœ‰**æœ€é«˜ä¼˜å…ˆçº§**ï¼Œä¸€æ—¦è¶…æ—¶ï¼Œæ— è®º `backoffLimit` æ˜¯å¦è¾¾åˆ°ï¼Œç³»ç»Ÿéƒ½ä¼šç»ˆæ­¢æ‰€æœ‰ Pod å¹¶æ ‡è®° Job ä¸ºå¤±è´¥ã€‚

â€¢ **æˆåŠŸç­–ç•¥ï¼ˆSuccess Policyï¼‰ï¼š** å…è®¸ç”¨æˆ·å®šä¹‰ Job ä½•æ—¶è¢«è§†ä¸ºæˆåŠŸã€‚ä¾‹å¦‚åœ¨æ¨¡æ‹Ÿè®¡ç®—ä¸­ï¼Œåªè¦ç‰¹å®šæ¯”ä¾‹çš„ç´¢å¼•æˆåŠŸï¼Œæˆ–è€…æŒ‡å®šçš„â€œé¢†å¯¼è€…â€ç´¢å¼•æˆåŠŸï¼Œå³å¯å®£å¸ƒ Job æˆåŠŸå¹¶ç»ˆæ­¢å‰©ä½™ Podã€‚

4. ç»ˆæ­¢åŽçš„çŠ¶æ€ç®¡ç†ä¸Žæ¸…ç†

Job è¾¾åˆ°ç»ˆæ€ï¼ˆ`Complete` æˆ– `Failed`ï¼‰åŽçš„å¤„ç†åŒæ ·é‡è¦ï¼š

â€¢ **å»¶è¿Ÿç»ˆç«¯çŠ¶æ€ç¡®è®¤ï¼š** è‡ª v1.31 èµ·ï¼ŒJob æŽ§åˆ¶å™¨ä¼šç­‰å¾…æ‰€æœ‰ Pod å½»åº•ç»ˆæ­¢åŽï¼Œæ‰æ·»åŠ æœ€ç»ˆçš„ `Complete` æˆ– `Failed` çŠ¶æ€æ ‡ç­¾ã€‚

â€¢ **è‡ªåŠ¨æ¸…ç†ï¼ˆTTL-after-finishedï¼‰ï¼š** é€šè¿‡ `.spec.ttlSecondsAfterFinished` å­—æ®µï¼ŒæŽ§åˆ¶å™¨ä¼šåœ¨ Job å®ŒæˆåŽçš„æŒ‡å®šç§’æ•°å†…æ‰§è¡Œçº§è”åˆ é™¤ï¼Œæ¸…ç† Job å¯¹è±¡åŠå…¶å…³è”çš„ Podï¼Œä»¥å‡è½» API æœåŠ¡å™¨çš„åŽ‹åŠ›ã€‚

â€¢ **æ‰‹åŠ¨æ¸…ç†ï¼š** é»˜è®¤æƒ…å†µä¸‹ï¼Œå®Œæˆçš„ Job åŠå…¶æ—¥å¿—ä¼šä¿ç•™åœ¨ API ä¸­ä¾›è¯Šæ–­ä½¿ç”¨ï¼Œç›´åˆ°ç”¨æˆ·æ‰‹åŠ¨åˆ é™¤ã€‚

\--------------------------------------------------------------------------------

**æ¯”å–»ç†è§£ï¼š** å¯ä»¥å°† **Kubernetes Job çš„æ•…éšœå¤„ç†**æƒ³è±¡æˆä¸€åœºæœ‰ä¸¥æ ¼è§„ç« çš„**ç§‘å­¦å®žéªŒ**ã€‚`backoffLimit` æ˜¯å…è®¸å®žéªŒå¤±è´¥é‡å¯çš„æ¬¡æ•°ï¼›`podFailurePolicy` å°±åƒå®žéªŒå®¤å‡†åˆ™ï¼Œè§„å®šäº†å¦‚æžœæ˜¯å› ä¸ºâ€œä»ªå™¨åäº†ï¼ˆèŠ‚ç‚¹å¹²æ‰°ï¼‰â€å°±é‡æ–°å†åšä¸€æ¬¡ä¸”ä¸æ‰£åˆ†ï¼Œä½†å¦‚æžœæ˜¯â€œå®žéªŒé€»è¾‘é”™è¯¯ï¼ˆç‰¹å®šé€€å‡ºç ï¼‰â€å°±ç›´æŽ¥ç»ˆæ­¢æ•´ä¸ªé¡¹ç›®ã€‚è€Œ `activeDeadlineSeconds` åˆ™æ˜¯å®žéªŒå®¤çš„ä¸‹ç­é“ƒå£°ï¼Œé“ƒå£°ä¸€å“ï¼Œæ— è®ºå®žéªŒè¿›åº¦å¦‚ä½•éƒ½å¿…é¡»å¼ºè¡Œåœæ­¢ã€‚æœ€åŽï¼Œ`TTL-after-finished` å°±åƒæ˜¯è‡ªåŠ¨æ¸…æ´æœºå™¨äººï¼Œåœ¨å®žéªŒç»“æŸå¹¶ç•™å‡ºè¶³å¤Ÿæ—¶é—´è®©ä½ è®°å½•æ•°æ®åŽï¼Œè‡ªåŠ¨æŠŠå®žéªŒå®¤æ‰“æ‰«å¹²å‡€ã€‚

example: 

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: robust-job-demo
spec:
  # 1. å…¨å±€é‡è¯•é™åˆ¶ (é»˜è®¤æ˜¯6ï¼Œè¿™é‡Œè®¾ä¸º4)
  backoffLimit: 4
  
  # 2. æ•´ä¸ª Job çš„ç¡¬æ€§è¶…æ—¶æ—¶é—´ (10åˆ†é’Ÿ)
  activeDeadlineSeconds: 600
  
  # 3. æ™ºèƒ½å¤±è´¥ç­–ç•¥ (v1.26+ ç¨³å®šç‰¹æ€§)
  podFailurePolicy:
    rules:
    - action: Ignore             # å¦‚æžœæ˜¯å› ä¸º Disruption (å¦‚èŠ‚ç‚¹è¢«åˆ ) å¯¼è‡´çš„å¤±è´¥ï¼Œä¸è®¡å…¥é‡è¯•æ¬¡æ•°
      onPodConditions:
      - type: DisruptionTarget
    - action: FailJob            # å¦‚æžœå®¹å™¨è¿”å›ž 42 å·é”™è¯¯ç ï¼Œç›´æŽ¥è®© Job å¤±è´¥ï¼Œåˆ«é‡è¯•äº†
      onExitCodes:
        containerName: main      # æŒ‡å®šå®¹å™¨å
        operator: In
        values: [42]

  template:
    spec:
      restartPolicy: Never       # é…åˆ podFailurePolicy æŽ¨èä½¿ç”¨ Never
      containers:
      - name: main
        image: busybox
        # æ¨¡æ‹Ÿï¼šéšæœºå¤±è´¥ï¼Œæˆ–è€…ä¼‘çœ 
        command: ["sh", "-c", "echo 'Processing...'; sleep 5; exit 1"]
```

##### Success policy

åœ¨ Kubernetes æ‰¹å¤„ç†ä»»åŠ¡ï¼ˆJobsï¼‰çš„èƒŒæ™¯ä¸‹ï¼Œ**æˆåŠŸç­–ç•¥ï¼ˆSuccess Policyï¼‰**å®šä¹‰äº† Job ä½•æ—¶å¯ä»¥è¢«å®£å‘Šä¸ºâ€œæ‰§è¡ŒæˆåŠŸâ€ã€‚è¿™ä¸€æœºåˆ¶åœ¨ v1.31 åŠæ›´é«˜ç‰ˆæœ¬ä¸­æä¾›äº†æ¯”é»˜è®¤è®¡æ•°æ›´ç²¾ç»†çš„æŽ§åˆ¶æ‰‹æ®µã€‚

ä»¥ä¸‹æ˜¯æ ¹æ®æ¥æºå¯¹ Job æˆåŠŸç­–ç•¥åŠå…¶æ ¸å¿ƒæœºåˆ¶çš„è¯¦ç»†è®¨è®ºï¼š

1. é»˜è®¤çš„æˆåŠŸåˆ¤å®šæ ‡å‡†

åœ¨æ²¡æœ‰é¢å¤–é…ç½®çš„æƒ…å†µä¸‹ï¼ŒJob çš„æˆåŠŸåˆ¤å®šéµå¾ªç®€å•çš„è®¡æ•°é€»è¾‘ï¼š

â€¢ **éžå¹¶è¡Œä»»åŠ¡**ï¼šä¸€æ—¦å”¯ä¸€çš„ Pod æˆåŠŸç»ˆæ­¢ï¼Œä»»åŠ¡å³å®£å‘ŠæˆåŠŸã€‚

â€¢ **å›ºå®šå®Œæˆè®¡æ•°å¹¶è¡Œä»»åŠ¡**ï¼šå½“æˆåŠŸç»ˆæ­¢çš„ Pod æ•°é‡è¾¾åˆ° **.spec.completions** æŒ‡å®šçš„æ•°å€¼æ—¶ï¼ŒJob æ‰è¢«è§†ä¸ºå®Œæˆã€‚

â€¢ **å·¥ä½œé˜Ÿåˆ—æ¨¡å¼**ï¼šåªè¦æœ‰**ä»»ä½•ä¸€ä¸ª** Pod æˆåŠŸç»ˆæ­¢ï¼Œä¸”æ‰€æœ‰å·²å¯åŠ¨çš„ Pod éƒ½å·²åœæ­¢ï¼ŒJob å°±ç®—æˆåŠŸã€‚

â€¢ **ç´¢å¼•æ¨¡å¼ï¼ˆIndexed Jobï¼‰**ï¼šé»˜è®¤è¦æ±‚ä»Ž 0 åˆ° `completions-1` çš„**æ¯ä¸€ä¸ªç´¢å¼•**éƒ½è‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸçš„ Podã€‚

2. ç²¾ç»†åŒ–æˆåŠŸç­–ç•¥ (`.spec.successPolicy`)

ä¸ºäº†åº”å¯¹æ›´å¤æ‚çš„ä¸šåŠ¡éœ€æ±‚ï¼ŒKubernetes å¼•å…¥äº† `.spec.successPolicy`ï¼ˆä¸»è¦é’ˆå¯¹**ç´¢å¼•ä»»åŠ¡**ï¼‰ï¼Œå…è®¸ç”¨æˆ·åœ¨ä¸ç­‰å¾…æ‰€æœ‰ç´¢å¼•æˆåŠŸçš„æƒ…å†µä¸‹å®£å‘Šä»»åŠ¡æˆåŠŸã€‚

**æ ¸å¿ƒåº”ç”¨åœºæ™¯ï¼š**

â€¢ **æ¨¡æ‹Ÿå®žéªŒ**ï¼šåœ¨è¿è¡Œå¸¦æœ‰ä¸åŒå‚æ•°çš„æ¨¡æ‹Ÿä»»åŠ¡æ—¶ï¼Œå¯èƒ½å¹¶ä¸éœ€è¦æ‰€æœ‰å‚æ•°çš„è®¡ç®—éƒ½æˆåŠŸï¼Œåªè¦å¾—åˆ°éƒ¨åˆ†ç»“æžœå³å¯è§†ä¸ºæ•´ä½“ä½œä¸šæˆåŠŸã€‚

â€¢ **é¢†å¯¼è€…-å·¥ä½œè€…æ¨¡å¼**ï¼šä¾‹å¦‚ MPI æˆ– PyTorch æ¡†æž¶ï¼Œé€šå¸¸åªæœ‰â€œé¢†å¯¼è€…ï¼ˆLeaderï¼‰â€èŠ‚ç‚¹çš„æˆåŠŸæ‰çœŸæ­£å†³å®šäº†æ•´ä¸ª Job çš„æˆè´¥ã€‚

3. æˆåŠŸç­–ç•¥çš„è§„åˆ™é…ç½®

æˆåŠŸç­–ç•¥é€šè¿‡ä¸€ç»„è§„åˆ™ï¼ˆRulesï¼‰å®šä¹‰ï¼Œè¿™äº›è§„åˆ™æŒ‰**é¡ºåºè¯„ä¼°**ï¼Œä¸€æ—¦æ»¡è¶³å…¶ä¸­ä¸€æ¡ï¼ŒåŽç»­è§„åˆ™å°†è¢«å¿½ç•¥ã€‚è§„åˆ™ä¸»è¦åŒ…å«ä¸¤ä¸ªç»´åº¦ï¼š

â€¢ **succeededIndexes****ï¼ˆæˆåŠŸç´¢å¼•é›†ï¼‰**ï¼šæŒ‡å®šå¿…é¡»æˆåŠŸçš„ç‰¹å®šç´¢å¼•èŒƒå›´ï¼ˆå¦‚ `0, 2-3`ï¼‰ã€‚

â€¢ **succeededCount****ï¼ˆæˆåŠŸè®¡æ•°ï¼‰**ï¼šæŒ‡å®šéœ€è¦æˆåŠŸçš„æœ€å°ç´¢å¼•æ•°é‡ã€‚

**é…ç½®ç»„åˆæ–¹å¼ï¼š**

1. **ä»…æŒ‡å®šç´¢å¼•é›†**ï¼šå½“è¯¥é›†åˆä¸­çš„**æ‰€æœ‰**ç´¢å¼•éƒ½æˆåŠŸæ—¶ï¼ŒJob æˆåŠŸã€‚
2. **ä»…æŒ‡å®šè®¡æ•°**ï¼šå½“æˆåŠŸçš„ç´¢å¼•**æ€»æ•°**è¾¾åˆ°è¯¥å€¼æ—¶ï¼ŒJob æˆåŠŸã€‚
3. **ä¸¤è€…ç»“åˆ**ï¼šå½“æŒ‡å®šçš„ç´¢å¼•å­é›†ä¸­æˆåŠŸçš„æ•°é‡è¾¾åˆ° `succeededCount` æ—¶ï¼ŒJob å³åˆ»å®£å‘ŠæˆåŠŸã€‚
4. ä¼˜å…ˆçº§ä¸Žç»ˆæ­¢æµç¨‹

â€¢ **ç­–ç•¥ä¼˜å…ˆçº§**ï¼šæ¥æºç‰¹åˆ«æŒ‡å‡ºï¼Œå¦‚æžœ Job åŒæ—¶å®šä¹‰äº†æˆåŠŸç­–ç•¥å’Œç»ˆæ­¢ç­–ç•¥ï¼ˆå¦‚ `.spec.backoffLimit` æˆ– `.spec.podFailurePolicy`ï¼‰ï¼Œä¸€æ—¦è§¦å‘äº†ç»ˆæ­¢ç­–ç•¥ï¼ˆåˆ¤å®šä¸ºå¤±è´¥ï¼‰ï¼Œç³»ç»Ÿå°†**ä¼˜å…ˆéµå®ˆç»ˆæ­¢ç­–ç•¥**å¹¶å¿½ç•¥æˆåŠŸç­–ç•¥ã€‚

â€¢ **æ¸…ç†å­˜ä½™ Pod**ï¼šä¸€æ—¦ Job æ»¡è¶³äº†æˆåŠŸç­–ç•¥ï¼ŒæŽ§åˆ¶å™¨ä¼šç«‹å³æ ‡è®° Job æ»¡è¶³æˆåŠŸå‡†åˆ™ï¼ˆæ·»åŠ  `SuccessCriteriaMet` æ¡ä»¶ï¼‰ï¼Œå¹¶ä¸»åŠ¨**ç»ˆæ­¢æ‰€æœ‰ä»åœ¨è¿è¡Œçš„æ®‹ä½™ Pod**ã€‚

â€¢ **çŠ¶æ€è½¬å˜**ï¼šåœ¨æ‰€æœ‰ Pod å½»åº•ç»ˆæ­¢åŽï¼ŒJob çš„æœ€ç»ˆçŠ¶æ€æ‰ä¼šè½¬å˜ä¸º `Complete`ã€‚

5. çŠ¶æ€è·Ÿè¸ªæœºåˆ¶

åœ¨ Job æ»¡è¶³æˆåŠŸæ¡ä»¶åŽï¼Œå…¶çŠ¶æ€ä¼šå‘ç”Ÿç»†å¾®å˜åŒ–ï¼š

â€¢ **SuccessCriteriaMet**ï¼šè¿™æ˜¯è§¦å‘ç»ˆæ­¢æµç¨‹çš„ä¿¡å·ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è¯¥æ¡ä»¶æå‰åˆ¤æ–­ä»»åŠ¡æ˜¯å¦å·²ç»è¾¾æˆç›®æ ‡ï¼Œè€Œæ— éœ€ç­‰å¾…æ‰€æœ‰ Pod å½»åº•å…³é—­ã€‚

â€¢ **CompletedIndexes**ï¼šæ— è®ºæ˜¯å¦è®¾ç½®äº†æˆåŠŸç­–ç•¥ï¼Œç³»ç»Ÿéƒ½ä¼šåœ¨çŠ¶æ€ä¸­è®°å½•ä¸‹æ‰€æœ‰å·²æˆåŠŸçš„ç´¢å¼•ã€‚

\--------------------------------------------------------------------------------

**æ¯”å–»ç†è§£ï¼š** å¯ä»¥å°† **Job æˆåŠŸç­–ç•¥**æƒ³è±¡æˆä¸€åœºâ€œ**é€‰æ‹”èµ›**â€ã€‚

â€¢ **é»˜è®¤æ¨¡å¼**ï¼šç›¸å½“äºŽâ€œå…¨å‘˜è¾¾æ ‡â€ï¼Œå¿…é¡»æ‰€æœ‰çš„è¿åŠ¨å‘˜ï¼ˆç´¢å¼•ï¼‰éƒ½é€šè¿‡è€ƒæ ¸ï¼Œæ•´ä¸ªä»£è¡¨é˜Ÿæ‰ç®—åˆæ ¼ã€‚

â€¢ **æˆåŠŸç­–ç•¥æ¨¡å¼**ï¼šåˆ™æä¾›äº†æ›´çµæ´»çš„è§„åˆ™ã€‚æ¯”å¦‚â€œæ ¸å¿ƒæˆå‘˜è¾¾æ ‡åˆ¶â€ï¼Œåªè¦ 1 å·ç§å­é€‰æ‰‹ï¼ˆLeader ç´¢å¼•ï¼‰èµ¢äº†ï¼Œæˆ–è€…é¢„é€‰åå•é‡Œæœ‰ä»»æ„ 3 ä¸ªäººï¼ˆ`succeededCount`ï¼‰å‡ºçº¿ï¼Œæ•´ä¸ªé˜Ÿä¼å°±å¯ä»¥æå‰å®£å‘Šèƒœåˆ©å¹¶æ”¶å·¥å›žå®¶ï¼Œå‰©ä¸‹çš„é€‰æ‹”æµç¨‹ï¼ˆä»åœ¨è¿è¡Œçš„ Podï¼‰ä¼šè¢«ç›´æŽ¥å–æ¶ˆã€‚

exampleï¼š

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: success-policy-demo
spec:
  completions: 5
  parallelism: 5
  completionMode: Indexed  # å¿…é¡»å¼€å¯ Indexed æ¨¡å¼
  
  # æ ¸å¿ƒé…ç½®ï¼šæˆåŠŸç­–ç•¥
  successPolicy:
    rules:
      - succeededIndexes: "0"  # è§„åˆ™1: ç´¢å¼• 0 å¿…é¡»æˆåŠŸ
        succeededCount: 1      # è¿™é‡Œçš„ Count æ˜¯æŒ‡å‘½ä¸­çš„ç´¢å¼•æ•°é‡
      - succeededIndexes: "1-4" # è§„åˆ™2: åœ¨ç´¢å¼• 1åˆ°4 ä¸­
        succeededCount: 2       # åªè¦æœ‰ä»»æ„ 2 ä¸ªæˆåŠŸå³å¯

  template:
    spec:
      restartPolicy: Never
      containers:
      - name: worker
        image: busybox
        # æ¨¡æ‹Ÿé€»è¾‘ï¼šæ‰“å°è‡ªå·±çš„ Indexã€‚
        # $JOB_COMPLETION_INDEX æ˜¯ Indexed Job æ³¨å…¥çš„çŽ¯å¢ƒå˜é‡
        command:
        - sh
        - -c
        - |
          echo "æˆ‘æ˜¯ Worker $JOB_COMPLETION_INDEX"
          sleep 10
          exit 0
```

##### Job termination and cleanup

åœ¨ Kubernetes æ‰¹å¤„ç†ä»»åŠ¡ï¼ˆJobsï¼‰çš„æ¡†æž¶ä¸‹ï¼Œ**ç”Ÿå‘½å‘¨æœŸç®¡ç†ä¸Žæ¸…ç†**æ˜¯ä¸€ä¸ªä»Žä»»åŠ¡å¯åŠ¨ã€çŠ¶æ€ç›‘æŽ§åˆ°æœ€ç»ˆèµ„æºå›žæ”¶çš„å®Œæ•´é—­çŽ¯ã€‚æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œè¿™ä¸€è¿‡ç¨‹ä¸ä»…æ¶‰åŠä»»åŠ¡çš„æˆè´¥åˆ¤å®šï¼Œè¿˜åŒ…æ‹¬ä¸ºäº†å‡è½» API æœåŠ¡å™¨åŽ‹åŠ›è€Œè®¾è®¡çš„å¤šç§è‡ªåŠ¨å’Œæ‰‹åŠ¨æ¸…ç†æœºåˆ¶ã€‚

ä»¥ä¸‹æ˜¯æ ¹æ®æ¥æºå¯¹ Job ç”Ÿå‘½å‘¨æœŸç®¡ç†ä¸Žæ¸…ç†çš„è¯¦ç»†è®¨è®ºï¼š

1. Job çš„ç”Ÿå‘½å‘¨æœŸé˜¶æ®µä¸Žç»ˆç«¯çŠ¶æ€

Job æ—¨åœ¨è¿è¡Œä¸€æ¬¡æ€§ä»»åŠ¡ç›´è‡³æˆåŠŸå®Œæˆã€‚å…¶ç”Ÿå‘½å‘¨æœŸåŒ…å«ä»¥ä¸‹å…³é”®èŠ‚ç‚¹ï¼š

â€¢ **é‡è¯•ä¸Žè¿½è¸ª**ï¼šJob ä¼šæŒç»­é‡è¯•æ‰§è¡Œ Podï¼Œç›´åˆ°è¾¾åˆ°æŒ‡å®šçš„æˆåŠŸå®Œæˆæ¬¡æ•°ã€‚åœ¨æ­¤æœŸé—´ï¼ŒæŽ§åˆ¶å¹³é¢ä½¿ç”¨ `batch.kubernetes.io/job-tracking` **ç»ˆæ­¢å™¨ï¼ˆFinalizerï¼‰**æ¥è¿½è¸ªæ‰€å±ž Podï¼Œç¡®ä¿ Pod åœ¨è¢«è®¡å…¥çŠ¶æ€ä¹‹å‰ä¸ä¼šè¢«å½»åº•ç§»é™¤ã€‚

â€¢ **ç»ˆç«¯çŠ¶æ€**ï¼šJob æœ€ç»ˆä¼šè¿›å…¥ä¸¤ä¸ªç»ˆç«¯çŠ¶æ€ä¹‹ä¸€ï¼š**Succeededï¼ˆæˆåŠŸï¼‰**æˆ– **Failedï¼ˆå¤±è´¥ï¼‰**ã€‚

â€¢ **çŠ¶æ€è½¬å˜çš„å»¶è¿Ÿå¤„ç†**ï¼šåœ¨ Kubernetes v1.31 åŠæ›´é«˜ç‰ˆæœ¬ä¸­ï¼ŒæŽ§åˆ¶å™¨ä¼š**å»¶è¿Ÿæ·»åŠ **ç»ˆç«¯æ¡ä»¶ï¼ˆ`Complete` æˆ– `Failed`ï¼‰ï¼Œç›´åˆ°è¯¥ Job çš„æ‰€æœ‰ Pod éƒ½å·²å½»åº•ç»ˆæ­¢ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œç³»ç»Ÿä¼šå…ˆé€šè¿‡ `SuccessCriteriaMet` æˆ– `FailureTarget` æ¡ä»¶æ¥è§¦å‘ Pod çš„ç»ˆæ­¢æµç¨‹ã€‚

2. æ‰‹åŠ¨æ¸…ç†ä¸Žè¯Šæ–­ä¿ç•™

é»˜è®¤æƒ…å†µä¸‹ï¼ŒJob å®ŒæˆåŽå…¶å¯¹è±¡å’Œ Pod **ä¸ä¼šè¢«è‡ªåŠ¨åˆ é™¤**ã€‚

â€¢ **ä¿ç•™ç›®çš„**ï¼šä¿ç•™å·²å®Œæˆçš„ Pod å…è®¸ç”¨æˆ·æŸ¥çœ‹æ ‡å‡†è¾“å‡ºæ—¥å¿—ã€è­¦å‘Šæˆ–å…¶ä»–è¯Šæ–­ä¿¡æ¯ã€‚

â€¢ **æ‰‹åŠ¨æ“ä½œ**ï¼šç”¨æˆ·éœ€è¦é€šè¿‡ `kubectl delete` æ‰‹åŠ¨åˆ é™¤æ—§ Jobã€‚åˆ é™¤ Job å¯¹è±¡æ—¶ï¼Œå®ƒæ‰€åˆ›å»ºçš„æ‰€æœ‰ Pod ä¹Ÿä¼šè¢«ä¸€å¹¶æ¸…ç†ã€‚

3. æ ¸å¿ƒè‡ªåŠ¨åŒ–æ¸…ç†æœºåˆ¶ï¼šTTL-after-finished

ä¸ºäº†é¿å…å·²å®Œæˆä»»åŠ¡åœ¨ API æœåŠ¡å™¨ä¸­æ— é™å †ç§¯ï¼ŒKubernetes æä¾›äº† **TTLï¼ˆç”Ÿå­˜æ—¶é—´ï¼‰æœºåˆ¶**ã€‚

â€¢ **å·¥ä½œåŽŸç†**ï¼šé€šè¿‡åœ¨ Job ä¸­æŒ‡å®š **.spec.ttlSecondsAfterFinished** å­—æ®µï¼Œè®¾ç½®ä»»åŠ¡å®ŒæˆåŽçš„ä¿ç•™ç§’æ•°ã€‚

â€¢ **çº§è”åˆ é™¤**ï¼šå½“ TTL è¿‡æœŸæ—¶ï¼ŒTTL æŽ§åˆ¶å™¨ä¼šæ‰§è¡Œ**çº§è”åˆ é™¤**ï¼Œå³åŒæ—¶åˆ é™¤ Job åŠå…¶æ‰€æœ‰ä¾èµ–å¯¹è±¡ï¼ˆå¦‚ Podsï¼‰ã€‚

â€¢ **çµæ´»æ€§**ï¼šæ­¤å­—æ®µå¯ä»¥éšæ—¶è®¾ç½®ï¼šæ—¢å¯ä»¥åœ¨åˆ›å»ºæ—¶æŒ‡å®šï¼Œä¹Ÿå¯ä»¥åœ¨ä»»åŠ¡å®ŒæˆåŽæ‰‹åŠ¨æ›´æ–°ï¼Œç”šè‡³å¯ä»¥é€šè¿‡ **Mutating Admission Webhook** åŠ¨æ€æ³¨å…¥ï¼ˆä¾‹å¦‚æ ¹æ®æˆåŠŸæˆ–å¤±è´¥çŠ¶æ€è®¾ç½®ä¸åŒçš„ä¿ç•™æ—¶é—´ï¼‰ã€‚

â€¢ **æ³¨æ„äº‹é¡¹**ï¼šè¯¥æœºåˆ¶å¯¹é›†ç¾¤å†…çš„**æ—¶é—´åå·®ï¼ˆTime Skewï¼‰**éžå¸¸æ•æ„Ÿï¼Œå¯èƒ½å¯¼è‡´åœ¨é”™è¯¯çš„æ—¶é—´è§¦å‘æ¸…ç†ã€‚

4. å…¶ä»–ç”Ÿå‘½å‘¨æœŸæŽ§åˆ¶æ‰‹æ®µ

â€¢ **ä¸»åŠ¨æˆªæ­¢æ—¶é—´ ( activeDeadlineSeconds **)ï¼šè¿™æ˜¯ä¸€ç§åŸºäºŽæ—¶é—´çš„ç»ˆæ­¢ç­–ç•¥ã€‚ä¸€æ—¦è¾¾åˆ°è®¾å®šçš„æ—¶é—´ä¸Šé™ï¼ŒJob ä¼šç»ˆæ­¢æ‰€æœ‰è¿è¡Œä¸­çš„ Pod å¹¶è½¬ä¸ºå¤±è´¥çŠ¶æ€ã€‚

â€¢ **CronJob ç®¡ç†**ï¼šå¦‚æžœ Job ç”± CronJob ç®¡ç†ï¼Œç³»ç»Ÿä¼šæ ¹æ® CronJob å®šä¹‰çš„**åŸºäºŽå®¹é‡çš„æ¸…ç†ç­–ç•¥**ï¼ˆCapacity-based cleanup policyï¼‰è‡ªåŠ¨æ¸…ç†åŽ†å²ä»»åŠ¡ã€‚

â€¢ **æŒ‚èµ·ä¸Žæ¢å¤**ï¼šé€šè¿‡è®¾ç½® `.spec.suspend: true`ï¼Œå¯ä»¥ä¸´æ—¶åœæ­¢ Job çš„æ‰§è¡Œå¹¶ç»ˆæ­¢å…¶æ´»è·ƒ Podï¼Œç›´åˆ°é‡æ–°æ¢å¤ï¼ˆæ­¤æ—¶ `activeDeadlineSeconds` è®¡æ—¶å™¨ä¼šé‡ç½®ï¼‰ã€‚

5. ç”Ÿå‘½å‘¨æœŸç®¡ç†çš„æœ€ä½³å®žè·µ

èµ„æ–™å»ºè®®ï¼Œå¯¹äºŽç›´æŽ¥åˆ›å»ºçš„éžæ‰˜ç®¡ Jobï¼ˆUnmanaged Jobsï¼‰ï¼Œ**å¼ºçƒˆæŽ¨èè®¾ç½® TTL å­—æ®µ**ã€‚å› ä¸ºè¿™äº› Job é»˜è®¤çš„åˆ é™¤ç­–ç•¥å¯èƒ½ä¼šå¯¼è‡´ Pod åœ¨ Job åˆ é™¤åŽå˜æˆâ€œå­¤å„¿ï¼ˆOrphanï¼‰â€ï¼Œè™½ç„¶ç³»ç»Ÿæœ€ç»ˆä¼šè¿›è¡Œåžƒåœ¾å›žæ”¶ï¼Œä½†åœ¨æ­¤ä¹‹å‰å¤§é‡å †ç§¯çš„ Pod å¯èƒ½ä¼šå¯¼è‡´é›†ç¾¤æ€§èƒ½ä¸‹é™ç”šè‡³ä¸‹çº¿ã€‚

--------------------------------------------------------------------------------

**æ¯”å–»ç†è§£**ï¼š å¯ä»¥å°† **Job çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†**æƒ³è±¡æˆä¸€ä¸ªâ€œè‡ªåŠ¨åŒ–å®žéªŒå®¤â€ã€‚

â€¢ **æ‰‹åŠ¨æ¸…ç†**å°±åƒæ˜¯å®žéªŒç»“æŸåŽï¼Œå®žéªŒå‘˜ï¼ˆç”¨æˆ·ï¼‰å¿…é¡»äº²è‡ªè¿›å…¥å®žéªŒå®¤æ‰“æ‰«ï¼ˆ`kubectl delete`ï¼‰ï¼Œå¦åˆ™å®žéªŒå™¨æï¼ˆPodï¼‰å’Œè®°å½•å•ï¼ˆJob å¯¹è±¡ï¼‰ä¼šä¸€ç›´å ç”¨ç©ºé—´ã€‚

â€¢ **TTL æœºåˆ¶**åˆ™æ˜¯ä¸€ä¸ªâ€œè‡ªåŠ¨é”€æ¯å®šæ—¶å™¨â€ã€‚å®žéªŒä¸€ç»“æŸï¼ˆå®Œæˆæˆ–å¤±è´¥ï¼‰ï¼Œå®šæ—¶å™¨å¼€å§‹å€’è®¡æ—¶ï¼›ä¸€æ—¦æ—¶é—´åˆ°ï¼Œå®žéªŒå®¤ä¼šè‡ªåŠ¨è¿›è¡Œå¤§æ‰«é™¤ï¼ŒæŠŠè®°å½•å•å’Œå™¨æå…¨éƒ¨æ¸…ç©ºã€‚

â€¢ **Finalizersï¼ˆç»ˆæ­¢å™¨ï¼‰**å°±åƒæ˜¯åœ¨æ¯ä¸ªå™¨æä¸Šè´´çš„â€œå®¡è®¡æ ‡ç­¾â€ï¼Œç¡®ä¿åœ¨å®žéªŒå®¤ç³»ç»Ÿç¡®è®¤å®žéªŒç»“æžœä¹‹å‰ï¼Œæ²¡æœ‰ä»»ä½•å™¨æä¼šè¢«å·å·æ‰”æŽ‰ã€‚

##### Advanced usage

åœ¨ Kubernetes Job çš„æž¶æž„ä¸­ï¼Œâ€œ**é«˜çº§ç”¨æ³•**â€æ¶µç›–äº†ä»Žç²¾å‡†çš„è°ƒåº¦æŽ§åˆ¶åˆ°ç”Ÿå‘½å‘¨æœŸæŽ¥ç®¡çš„ä¸€ç³»åˆ—å¤æ‚æœºåˆ¶ï¼Œæ—¨åœ¨æ»¡è¶³å¤§è§„æ¨¡ã€é«˜æ€§èƒ½æˆ–è‡ªå®šä¹‰åŒ–çš„æ‰¹å¤„ç†éœ€æ±‚ã€‚

ä»¥ä¸‹æ˜¯æ ¹æ®æ¥æºå¯¹ Job é«˜çº§ç”¨æ³•çš„è¯¦ç»†è®¨è®ºï¼š

1. ä»»åŠ¡æŒ‚èµ·ä¸Žæ¢å¤ (Suspending a Job)

ç”¨æˆ·å¯ä»¥é€šè¿‡æ›´æ–° **.spec.suspend** å­—æ®µæ¥æŽ§åˆ¶ Job çš„æ‰§è¡ŒçŠ¶æ€ã€‚

â€¢ **çµæ´»æŽ§åˆ¶**ï¼šJob å¯ä»¥åˆ›å»ºæ—¶å³å¤„äºŽæŒ‚èµ·çŠ¶æ€ï¼ˆ`true`ï¼‰ï¼Œç”±è‡ªå®šä¹‰æŽ§åˆ¶å™¨å†³å®šä½•æ—¶å¯åŠ¨ï¼›ä¹Ÿå¯ä»¥åœ¨è¿è¡Œä¸­æŒ‚èµ·ã€‚

â€¢ **èµ„æºæ¸…ç†ä¸Žé‡ç½®**ï¼šæŒ‚èµ· Job ä¼šå¯¼è‡´æ‰€æœ‰æœªå®Œæˆçš„ Pod è¢«ç»ˆæ­¢ï¼ˆå‘é€ SIGTERMï¼‰ã€‚å½“ Job æ¢å¤ï¼ˆè®¾ä¸º `false`ï¼‰æ—¶ï¼Œå…¶ `.status.startTime` ä¼šé‡ç½®ï¼Œè¿™æ„å‘³ç€ **activeDeadlineSeconds** **è®¡æ—¶å™¨ä¹Ÿä¼šéšä¹‹åœæ­¢å¹¶é‡ç½®**ã€‚

2. å¯å˜è°ƒåº¦æŒ‡ä»¤ (Mutable Scheduling Directives)

è¿™æ˜¯ä¸€é¡¹é’ˆå¯¹æŒ‚èµ· Job çš„é«˜çº§ç‰¹æ€§ï¼Œå…è®¸åœ¨ä»»åŠ¡å¯åŠ¨å‰è°ƒæ•´å…¶è°ƒåº¦çº¦æŸã€‚

â€¢ **é€‚ç”¨æ¡ä»¶**ï¼šä»…é€‚ç”¨äºŽå¤„äºŽæŒ‚èµ·çŠ¶æ€ä¸”**ä»Žæœªè¢«æ¢å¤è¿è¡Œè¿‡**çš„ Jobã€‚

â€¢ **è°ƒåº¦ä¼˜åŒ–**ï¼šè‡ªå®šä¹‰é˜Ÿåˆ—æŽ§åˆ¶å™¨å¯ä»¥åœ¨ Job å®žé™…å¯åŠ¨å‰ï¼Œæ›´æ–° Pod æ¨¡æ¿ä¸­çš„**èŠ‚ç‚¹äº²å’Œæ€§ (Node Affinity)ã€èŠ‚ç‚¹é€‰æ‹©å™¨ (Node Selector)ã€å®¹å¿åº¦ (Tolerations)** ç­‰å­—æ®µï¼Œä»Žè€Œå¼•å¯¼ Pod ç²¾ç¡®è½åœ°åˆ°ç›®æ ‡èŠ‚ç‚¹ã€‚

3. è‡ªå®šä¹‰ Pod é€‰æ‹©å™¨ (Specifying your own Pod selector)

è™½ç„¶ç³»ç»Ÿé»˜è®¤ä¼šè‡ªåŠ¨ç”Ÿæˆå”¯ä¸€çš„é€‰æ‹©å™¨ï¼Œä½†åœ¨ç‰¹å®šè¿ç»´åœºæ™¯ä¸‹å¯ä»¥æ‰‹åŠ¨å¹²é¢„ã€‚

â€¢ **æŽ¥ç®¡çŽ°æœ‰ Pod**ï¼šä¾‹å¦‚ï¼Œå½“éœ€è¦æ›´æ–° Job çš„ Pod æ¨¡æ¿æˆ–åç§°ï¼Œä½†åˆæƒ³ä¿ç•™æ­£åœ¨è¿è¡Œçš„ Pod æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ `kubectl delete --cascade=orphan` åˆ é™¤æ—§ Jobï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç›¸åŒé€‰æ‹©å™¨çš„æ–° Jobã€‚

â€¢ **é…ç½®è¦æ±‚**ï¼šåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¿…é¡»æ˜¾å¼è®¾ç½® **manualSelector: true**ï¼Œä»¥å‘ŠçŸ¥ç³»ç»Ÿè¯¥é€‰æ‹©å™¨ä¸åŒ¹é…ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆçš„ UID æ˜¯é¢„æœŸè¡Œä¸ºã€‚

4. å¼¹æ€§ç´¢å¼• Job (Elastic Indexed Jobs)

å¯¹äºŽç´¢å¼•æ¨¡å¼çš„ Jobï¼ŒKubernetes æ”¯æŒåœ¨çº¿åŠ¨æ€ç¼©æ”¾ã€‚

â€¢ **è”åŠ¨ç¼©æ”¾**ï¼šç”¨æˆ·å¯ä»¥åŒæ—¶ä¿®æ”¹ `.spec.parallelism` å’Œ `.spec.completions`ï¼Œåªè¦ä¿æŒä¸¤è€…ç›¸ç­‰ï¼ˆ**.spec.parallelism == .spec.completions**ï¼‰ï¼Œå³å¯å®žçŽ°æ‰©ç¼©å®¹ã€‚

â€¢ **åœºæ™¯åº”ç”¨**ï¼šè¿™åœ¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¦‚ MPIã€PyTorchã€Rayï¼‰ç­‰éœ€è¦æ ¹æ®èµ„æºæƒ…å†µåŠ¨æ€è°ƒæ•´å·¥ä½œè§„æ¨¡çš„åœºæ™¯ä¸­éžå¸¸æœ‰ç”¨ã€‚

5. Pod æ›¿æ¢ç­–ç•¥ (Pod Replacement Policy)

ç”¨æˆ·å¯ä»¥æŽ§åˆ¶ç³»ç»Ÿä½•æ—¶åˆ›å»ºæ›¿ä»£ Podï¼Œä»¥é¿å…èµ„æºå†—ä½™ã€‚

â€¢ **æ›¿æ¢æ—¶æœº**ï¼šé€šè¿‡è®¾ç½® **.spec.podReplacementPolicy: Failed**ï¼ŒJob æŽ§åˆ¶å™¨å°†ç­‰å¾…æ—§ Pod å®Œå…¨è¾¾åˆ°â€œå¤±è´¥â€é˜¶æ®µåŽå†åˆ›å»ºæ–° Podï¼Œè€Œä¸æ˜¯åœ¨ Pod ä¸€è¿›å…¥â€œç»ˆæ­¢ä¸­â€çŠ¶æ€å°±ç«‹å³æ›¿æ¢ã€‚

â€¢ **é˜²æ­¢è¶…é‡**ï¼šè¿™ç¡®ä¿äº†åœ¨ä»»ä½•æ—¶åˆ»ï¼Œè¿è¡Œä¸­çš„ Pod æ•°é‡éƒ½ä¸ä¼šè¶…è¿‡å¹¶è¡Œåº¦é™åˆ¶æˆ–æ¯ä¸ªç´¢å¼•ä¸€ä¸ª Pod çš„é™åˆ¶ã€‚

6. å§”æ´¾ç®¡ç† (Delegation to External Controller)

é€šè¿‡ **spec.managedBy** å­—æ®µï¼Œç”¨æˆ·å¯ä»¥ç¦ç”¨å†…ç½®çš„ Job æŽ§åˆ¶å™¨ï¼Œå¹¶å°†è¯¥ Job çš„è°ƒå’Œé€»è¾‘å®Œå…¨å§”æ´¾ç»™å¤–éƒ¨æŽ§åˆ¶å™¨ï¼ˆä¾‹å¦‚ç¬¬ä¸‰æ–¹æ‰¹å¤„ç†è°ƒåº¦å™¨ï¼‰ã€‚

â€¢ **æ ‡è¯†ç¬¦**ï¼šåªè¦è¯¥å­—æ®µçš„å€¼ä¸æ˜¯ `kubernetes.io/job-controller`ï¼Œå†…ç½®æŽ§åˆ¶å™¨å°±ä¼šå¿½ç•¥è¯¥å¯¹è±¡ã€‚

â€¢ **å¼€å‘è€…çº¦æŸ**ï¼šå¤–éƒ¨æŽ§åˆ¶å™¨å¿…é¡»éµå¾ª Job API è§„èŒƒï¼Œä¸”**ä¸å¾—ä½¿ç”¨**å†…ç½®æŽ§åˆ¶å™¨é¢„ç•™çš„è¿½è¸ªç»ˆæ­¢å™¨ï¼ˆ`batch.kubernetes.io/job-tracking`ï¼‰ã€‚

--------------------------------------------------------------------------------

**æ¯”å–»ç†è§£**ï¼š å¦‚æžœæŠŠæ™®é€š Job æ¯”ä½œä¸€å°â€œå…¨è‡ªåŠ¨æ´—è¡£æœºâ€ï¼ˆæŒ‰ä¸€ä¸‹å°±å¼€å§‹ï¼Œæ´—å®Œå°±åœï¼‰ï¼Œé‚£ä¹ˆè¿™äº›**é«˜çº§ç”¨æ³•**å°±åƒæ˜¯ç»™æ´—è¡£æœºå¢žåŠ äº†â€œä¸­é€”æš‚åœï¼ˆæŒ‚èµ·ï¼‰â€ã€â€œæ´—å‰è‡ªåŠ¨è¯†åˆ«è¡£ç‰©æè´¨å¹¶è°ƒæ•´è½¬é€Ÿï¼ˆå¯å˜è°ƒåº¦ï¼‰â€ä»¥åŠâ€œå…è®¸å¤–æŽ¥ä¸“ä¸šæ´—æ¶¤æ¨¡å—ï¼ˆå§”æ´¾ç®¡ç†ï¼‰â€çš„åŠŸèƒ½ã€‚è¿™äº›åŠŸèƒ½è®©åŽŸæœ¬ç®€å•çš„æ‰¹å¤„ç†ä»»åŠ¡èƒ½å¤Ÿé€‚åº”æ›´å¤æ‚çš„å·¥ä¸šçº§ä½œä¸šçŽ¯å¢ƒã€‚

#### Kubernetes Job è‡ªåŠ¨æ¸…ç†æœºåˆ¶

åœ¨ Kubernetes çš„æ‰¹å¤„ç†ä»»åŠ¡ï¼ˆJobsï¼‰ç®¡ç†ä¸­ï¼Œ**å·²å®Œæˆä»»åŠ¡çš„è‡ªåŠ¨æ¸…ç†**æ˜¯ä¸€ä¸ªè‡³å…³é‡è¦çš„ç»´æŠ¤æœºåˆ¶ã€‚å®ƒä¸ä»…èƒ½ä¿æŒé›†ç¾¤çš„æ•´æ´ï¼Œè¿˜èƒ½æœ‰æ•ˆå‡è½» API æœåŠ¡å™¨çš„åŽ‹åŠ›ã€‚

æ ¹æ®æä¾›çš„æ¥æºï¼Œä»¥ä¸‹æ˜¯å¯¹è¯¥æœºåˆ¶çš„è¯¦ç»†è®¨è®ºï¼š

1. æ ¸å¿ƒæœºåˆ¶ï¼šTTL-after-finished æŽ§åˆ¶å™¨

Kubernetes æä¾›äº†ä¸€ä¸ª **TTL-after-finished æŽ§åˆ¶å™¨**ï¼Œä¸“é—¨ç”¨äºŽé™åˆ¶å·²å®Œæˆ Job å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸã€‚

â€¢ **è§¦å‘æ¡ä»¶**ï¼šå½“ Job çš„çŠ¶æ€æ¡ä»¶å˜ä¸º **`Complete`ï¼ˆæˆåŠŸï¼‰**æˆ– **`Failed`ï¼ˆå¤±è´¥ï¼‰**æ—¶ï¼Œè®¡æ—¶å™¨å¼€å§‹å¯åŠ¨ã€‚

â€¢ **æ¸…ç†è¡Œä¸º**ï¼šä¸€æ—¦è®¾å®šçš„ TTLï¼ˆç”Ÿå­˜æ—¶é—´ï¼‰è¿‡æœŸï¼Œè¯¥ Job å°±ä¼šè¿›å…¥å¯è¢«æ¸…ç†çš„çŠ¶æ€ã€‚æŽ§åˆ¶å™¨ä¼šæ‰§è¡Œ**çº§è”åˆ é™¤ï¼ˆCascading Removalï¼‰**ï¼Œè¿™æ„å‘³ç€ Job å¯¹è±¡åŠå…¶å…³è”çš„æ‰€æœ‰ä¾èµ–å¯¹è±¡ï¼ˆå¦‚ç”Ÿæˆçš„ Podsï¼‰å°†ä¸€åŒè¢«åˆ é™¤ã€‚

â€¢ **ç”Ÿå‘½å‘¨æœŸä¿è¯**ï¼šå³ä½¿è§¦å‘äº†æ¸…ç†ï¼ŒKubernetes ä»ä¼šå°Šé‡å¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸä¿è¯ï¼Œä¾‹å¦‚ç­‰å¾…ç»ˆç»“å™¨ï¼ˆFinalizersï¼‰å¤„ç†å®Œæˆã€‚

2. å…³é”®é…ç½®å­—æ®µï¼š`.spec.ttlSecondsAfterFinished`

è¯¥æœºåˆ¶ä¸»è¦é€šè¿‡ Job çš„ **.spec.ttlSecondsAfterFinished** å­—æ®µè¿›è¡Œé…ç½®ï¼š

â€¢ **ç«‹å³åˆ é™¤**ï¼šå¦‚æžœå°†è¯¥å­—æ®µè®¾ç½®ä¸º `0`ï¼ŒJob åœ¨å®ŒæˆåŽä¼šç«‹å³ç¬¦åˆè¢«è‡ªåŠ¨åˆ é™¤çš„æ¡ä»¶ã€‚

â€¢ **æ— é™ä¿ç•™**ï¼šå¦‚æžœè¯¥å­—æ®µæœªè®¾ç½®ï¼ŒTTL æŽ§åˆ¶å™¨å°†ä¸ä¼šæ¸…ç†è¯¥ Jobã€‚

â€¢ **çµæ´»è®¾ç½®æ–¹å¼**ï¼š

  â—¦ åœ¨åˆ›å»º Job çš„ **Manifestï¼ˆæ¸…å•ï¼‰**ä¸­é™æ€æŒ‡å®šã€‚

  â—¦ å¯¹å·²ç»è¿è¡Œå®Œæˆçš„ Job è¿›è¡Œ**æ‰‹åŠ¨æ›´æ–°**ä»¥å¯åŠ¨æ¸…ç†ã€‚

  â—¦ ä½¿ç”¨**å‡†å…¥æ’ä»¶ï¼ˆMutating Admission Webhookï¼‰**åœ¨ Job åˆ›å»ºæ—¶åŠ¨æ€è®¾ç½®ï¼Œæˆ–è€…åœ¨ Job å®ŒæˆåŽæ ¹æ®å…¶çŠ¶æ€ï¼ˆæˆåŠŸæˆ–å¤±è´¥ï¼‰è®¾ç½®ä¸åŒçš„ TTL å€¼ã€‚

  â—¦ é€šè¿‡ç¼–å†™**è‡ªå®šä¹‰æŽ§åˆ¶å™¨**ï¼Œä¸ºåŒ¹é…ç‰¹å®šé€‰æ‹©å™¨ï¼ˆSelectorï¼‰çš„ä¸€ç±» Job ç®¡ç†æ¸…ç†ç­–ç•¥ã€‚

3. ä¸ºä»€ä¹ˆè‡ªåŠ¨æ¸…ç†è‡³å…³é‡è¦ï¼Ÿ

â€¢ **å‡è½» API æœåŠ¡å™¨åŽ‹åŠ›**ï¼šä¿ç•™å¤§é‡çš„å·²å®Œæˆä»»åŠ¡ä¼šå ç”¨ API æœåŠ¡å™¨çš„å­˜å‚¨èµ„æºå¹¶å½±å“æ€§èƒ½ã€‚

â€¢ **é˜²æ­¢ Pod å­¤å„¿åŒ–ï¼ˆOrphan Podsï¼‰**ï¼šå¯¹äºŽéž CronJob ç®¡ç†çš„â€œéžæ‰˜ç®¡ Jobï¼ˆUnmanaged Jobsï¼‰â€ï¼Œå…¶é»˜è®¤åˆ é™¤ç­–ç•¥å¯èƒ½ä¼šå¯¼è‡´ Pod åœ¨ Job åˆ é™¤åŽæ®‹ç•™ã€‚æ¥æºå¼ºçƒˆå»ºè®®ä¸ºè¿™ç±» Job è®¾ç½® TTL å­—æ®µï¼Œå› ä¸ºå¤§é‡ç§¯åŽ‹çš„æ®‹ç•™ Pod å¯èƒ½å¯¼è‡´é›†ç¾¤æ€§èƒ½ä¸‹é™ç”šè‡³ä¸‹çº¿ã€‚

â€¢ **CronJob çš„æ›¿ä»£æ–¹æ¡ˆ**ï¼šå¦‚æžœ Job æ˜¯ç”± CronJob ç®¡ç†çš„ï¼Œåˆ™é€šå¸¸éµå¾ª CronJob å®šä¹‰çš„**åŸºäºŽå®¹é‡çš„æ¸…ç†ç­–ç•¥**ï¼ˆCapacity-based cleanup policyï¼‰ã€‚

4. ä½¿ç”¨é™åˆ¶ä¸Žæ³¨æ„äº‹é¡¹

åœ¨å®žæ–½è‡ªåŠ¨æ¸…ç†æ—¶ï¼Œéœ€è¦è­¦æƒ•ä»¥ä¸‹é£Žé™©ï¼š

â€¢ **æ—¶é—´åå·®ï¼ˆTime Skewï¼‰é£Žé™©**ï¼šTTL æŽ§åˆ¶å™¨ä¾èµ–å­˜å‚¨åœ¨ Job å¯¹è±¡ä¸­çš„æ—¶é—´æˆ³ã€‚å¦‚æžœé›†ç¾¤å†…å„èŠ‚ç‚¹çš„æ—¶é’Ÿä¸ä¸€è‡´ï¼Œå¯èƒ½å¯¼è‡´æŽ§åˆ¶å™¨åœ¨**é”™è¯¯çš„æ—¶é—´**ï¼ˆè¿‡æ—©æˆ–è¿‡æ™šï¼‰æ¸…ç† Jobã€‚

â€¢ **ä¿®æ”¹è¿‡æœŸ TTL æ— æ•ˆ**ï¼šå¦‚æžœåœ¨çŽ°æœ‰çš„ TTL å·²è¿‡æœŸåŽæ‰å°è¯•æ›´æ–°å¹¶å»¶é•¿è¯¥å­—æ®µçš„å€¼ï¼ŒKubernetes **æ— æ³•ä¿è¯**ä¼šä¿ç•™è¯¥ Jobï¼Œå³ä½¿ API è¯·æ±‚è¿”å›žæˆåŠŸã€‚

â€¢ **è¯Šæ–­æ•°æ®ä¸¢å¤±**ï¼šä¸€æ—¦ Job è¢«æ¸…ç†ï¼Œå…¶å…³è”çš„ **Pod åŠå…¶æ—¥å¿—**ä¹Ÿå°†ä¸€å¹¶æ¶ˆå¤±ã€‚å› æ­¤ï¼Œé€šå¸¸éœ€è¦ä¿ç•™ä¸€æ®µæ—¶é—´ä»¥ä¾¿ç”¨æˆ·æ£€æŸ¥é”™è¯¯ã€è­¦å‘Šæˆ–è¯Šæ–­è¾“å‡ºã€‚

-----

**æ¯”å–»ç†è§£ï¼š** ä½ å¯ä»¥å°†å·²å®Œæˆçš„ Job æƒ³è±¡æˆä¸€å¼ â€œ**é¤åŽ…ç»“è´¦å•**â€ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè´¦å•ä¼šç•™åœ¨æ¡Œä¸Šï¼Œç›´åˆ°æœåŠ¡å‘˜ï¼ˆç”¨æˆ·ï¼‰æ‰‹åŠ¨æ”¶èµ°ã€‚è®¾ç½® `.spec.ttlSecondsAfterFinished` å°±åƒæ˜¯ç»™è´¦å•è£…äº†ä¸€ä¸ªâ€œ**è‡ªåŠ¨ç¢Žçº¸æœº**â€ï¼šå®ƒå…è®¸ä½ åœ¨å®¢äººç¦»å¼€ï¼ˆä»»åŠ¡å®Œæˆï¼‰åŽçš„å‡ åˆ†é’Ÿå†…æŸ¥çœ‹æ¶ˆè´¹æ˜Žç»†ï¼ˆæ—¥å¿—ï¼‰ï¼›ä¸€æ—¦å€’è®¡æ—¶ç»“æŸï¼Œç¢Žçº¸æœºå°±ä¼šè‡ªåŠ¨æŠŠè´¦å•å’Œæ¡Œä¸Šçš„é¤å…·ï¼ˆPodï¼‰å…¨éƒ¨æ¸…ç†å¹²å‡€ï¼Œè…¾å‡ºä½ç½®ç»™ä¸‹ä¸€ä½å®¢äººã€‚

#### CronJob

æ ¹æ®æä¾›çš„æ¥æºï¼Œ**CronJob** æ˜¯ Kubernetes ä¸­ç”¨äºŽç®¡ç†å‘¨æœŸæ€§ã€é‡å¤æ€§ä»»åŠ¡çš„æ ¸å¿ƒæŽ§åˆ¶å™¨ã€‚å®ƒé€šè¿‡ Cron æ ¼å¼çš„æ—¶é—´è¡¨æ¥è‡ªåŠ¨åˆ›å»º Job å¯¹è±¡ï¼Œç±»ä¼¼äºŽ Unix ç³»ç»Ÿä¸­çš„ **crontab**ã€‚

ä»¥ä¸‹æ˜¯æ¥æºä¸­å…³äºŽ CronJob çš„è¯¦ç»†è®¨è®ºï¼š

1. æ ¸å¿ƒå®šä¹‰ä¸ŽåŠŸèƒ½

- **é‡å¤æ€§è°ƒåº¦**ï¼šCronJob ä¸»è¦ç”¨äºŽæ‰§è¡Œå®šæœŸæ“ä½œï¼Œå¦‚**å¤‡ä»½ã€æŠ¥å‘Šç”Ÿæˆ**ç­‰ã€‚
- **èŒè´£åˆ†å·¥**ï¼šCronJob ä»…è´Ÿè´£æ ¹æ®é¢„è®¾çš„æ—¶é—´è¡¨åˆ›å»ºå¯¹åº”çš„ Job å¯¹è±¡ï¼Œè€Œå…·ä½“çš„ Pod ç®¡ç†å’Œä»»åŠ¡æ‰§è¡Œåˆ™ç”±äº§ç”Ÿçš„ Job è´Ÿè´£ã€‚
- **åç§°é™åˆ¶**ï¼šCronJob çš„åç§°å¿…é¡»æ˜¯æœ‰æ•ˆçš„ DNS å­åŸŸåï¼Œä¸”é•¿åº¦**ä¸å¾—è¶…è¿‡ 52 ä¸ªå­—ç¬¦**ã€‚è¿™æ˜¯å› ä¸ºæŽ§åˆ¶å™¨ä¼šè‡ªåŠ¨åœ¨åç§°åŽé™„åŠ  11 ä¸ªå­—ç¬¦ï¼Œè€Œ Job çš„æ€»é•¿åº¦é™åˆ¶ä¸º 63 ä¸ªå­—ç¬¦ã€‚

2. æ—¶é—´è¡¨ä¸Žæ—¶åŒºè®¾ç½®

- **è¯­æ³•è§„èŒƒ**ï¼š`.spec.schedule` å­—æ®µæ˜¯å¿…å¡«é¡¹ï¼Œéµå¾ªæ ‡å‡†çš„ Cron è¯­æ³•ï¼ˆåˆ†é’Ÿã€å°æ—¶ã€æ—¥ã€æœˆã€æ˜ŸæœŸï¼‰ã€‚å®ƒæ”¯æŒ **â€œ/â€ æ­¥é•¿å€¼**ï¼ˆå¦‚ `*/2` è¡¨ç¤ºæ¯ä¸¤å°æ—¶ä¸€æ¬¡ï¼‰ä»¥åŠ **â€œ?â€**ï¼ˆä¸Ž â€œ*â€ å«ä¹‰ç›¸åŒï¼‰ã€‚
- **å®å‘½ä»¤**ï¼šæ”¯æŒä½¿ç”¨ä¸€äº›é¢„è®¾å®ï¼Œå¦‚ `@monthly`ï¼ˆæ¯æœˆä¸€æ¬¡ï¼‰ã€`@weekly`ï¼ˆæ¯å‘¨ä¸€æ¬¡ï¼‰ã€`@daily`ï¼ˆæ¯å¤©ä¸€æ¬¡ï¼‰ç­‰ã€‚
- **æ—¶åŒºç®¡ç†**ï¼š
  - é»˜è®¤æƒ…å†µä¸‹ï¼Œè°ƒåº¦åŸºäºŽæŽ§åˆ¶å¹³é¢çš„**æœ¬åœ°æ—¶åŒº**è§£é‡Šã€‚
  - ç”¨æˆ·å¯ä»¥é€šè¿‡ **`.spec.timeZone`** æ˜¾å¼æŒ‡å®šæ—¶åŒºï¼ˆå¦‚ `"Etc/UTC"`ï¼‰ã€‚
  - éœ€è¦æ³¨æ„ï¼Œ**ä¸æ”¯æŒ**åœ¨ `schedule` å­—æ®µä¸­ä½¿ç”¨ `TZ` æˆ– `CRON_TZ` å˜é‡ï¼Œè¿™ä¼šå¯¼è‡´éªŒè¯é”™è¯¯ã€‚

3. å¹¶å‘ç­–ç•¥ä¸Žä»»åŠ¡æŒ‚èµ·

- **å¹¶å‘ç­–ç•¥ (`.spec.concurrencyPolicy`)**ï¼šå®šä¹‰äº†å½“æ–°ä»»åŠ¡åˆ°è¾¾è€Œæ—§ä»»åŠ¡å°šæœªå®Œæˆæ—¶çš„å¤„ç†æ–¹å¼ï¼š
  - **Allowï¼ˆé»˜è®¤ï¼‰**ï¼šå…è®¸å¹¶å‘è¿è¡Œå¤šä¸ª Jobã€‚
  - **Forbid**ï¼šç¦æ­¢å¹¶å‘ï¼›å¦‚æžœå‰ä¸€ä¸ªæœªå®Œï¼Œåˆ™**è·³è¿‡**å½“å‰è°ƒåº¦ã€‚
  - **Replace**ï¼šç”¨æ–°åˆ›å»ºçš„ Job **æ›¿æ¢**å½“å‰æ­£åœ¨è¿è¡Œçš„ Jobã€‚
- **æŒ‚èµ· (`.spec.suspend`)**ï¼šè®¾ç½®ä¸º `true` æ—¶ï¼Œæ‰€æœ‰åŽç»­è°ƒåº¦éƒ½ä¼šè¢«æš‚åœï¼Œç›´åˆ°æ¢å¤ã€‚è¿™ä¸ä¼šå½±å“å·²ç»å¼€å§‹è¿è¡Œçš„ Jobã€‚æŒ‚èµ·æœŸé—´é”™è¿‡çš„è°ƒåº¦ä¼šè¢«è®¡ä¸ºâ€œé”™è¿‡â€çš„ä»»åŠ¡ã€‚

4. å»¶è¿Ÿå¯åŠ¨ä¸Žæ•…éšœå¤„ç†

- **å¯åŠ¨æˆªæ­¢æ—¶é—´ (`.spec.startingDeadlineSeconds`)**ï¼šè¯¥å­—æ®µå®šä¹‰äº† Job é”™è¿‡é¢„å®šæ—¶é—´åŽ**ä»è¢«å…è®¸å¯åŠ¨çš„çª—å£æœŸ**ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰ã€‚
  - å¦‚æžœé”™è¿‡çš„æ—¶é•¿è¶…è¿‡æ­¤é™å€¼ï¼Œè¯¥æ¬¡æ‰§è¡Œä¼šè¢«è·³è¿‡å¹¶è§†ä¸ºå¤±è´¥ã€‚
  - å¦‚æžœè¯¥å€¼è®¾ä¸ºå°äºŽ 10 ç§’ï¼Œå¯èƒ½å¯¼è‡´ä»»åŠ¡æ— æ³•è¢«è°ƒåº¦ï¼Œå› ä¸ºæŽ§åˆ¶å™¨æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡ã€‚
- **100 æ¬¡é”™è¿‡é™åˆ¶**ï¼šå¦‚æžœæŽ§åˆ¶å™¨æ£€æµ‹åˆ°è‡ªä¸Šæ¬¡è°ƒåº¦ä»¥æ¥é”™è¿‡çš„æ¬¡æ•°**è¶…è¿‡ 100 æ¬¡**ï¼Œå®ƒå°†åœæ­¢åˆ›å»º Job å¹¶è®°å½•é”™è¯¯ã€‚
  - è®¾ç½® `startingDeadlineSeconds` ä¼šæ”¹å˜è¿™ä¸€è¡Œä¸ºï¼šæŽ§åˆ¶å™¨å°†åªè®¡ç®—**è¯¥é™å€¼æ—¶é—´èŒƒå›´å†…**é”™è¿‡çš„æ¬¡æ•°ï¼ˆä¾‹å¦‚ï¼Œè¿‡åŽ» 200 ç§’å†…é”™è¿‡äº†å‡ æ¬¡ï¼‰ï¼Œä»Žè€Œé¿å…å› é•¿æ—¶é—´åœæœºå¯¼è‡´çš„è°ƒåº¦åœæ­¢ã€‚

5. ç”Ÿå‘½å‘¨æœŸä¸Žæ¨¡æ¿ç®¡ç†

- **Job æ¨¡æ¿**ï¼š`.spec.jobTemplate` å®šä¹‰äº†åˆ›å»º Job çš„è§„æ ¼ã€‚å¯¹ CronJob çš„ä¿®æ”¹**ä»…é€‚ç”¨äºŽåŽç»­æ–°åˆ›å»ºçš„ Job**ï¼Œæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ä¸ä¼šå—åˆ°å½±å“ã€‚
- **åŽ†å²è®°å½•ä¿ç•™**ï¼šé€šè¿‡ `.spec.successfulJobsHistoryLimit`ï¼ˆé»˜è®¤ 3ï¼‰å’Œ `.spec.failedJobsHistoryLimit`ï¼ˆé»˜è®¤ 1ï¼‰æ¥æŽ§åˆ¶ä¿ç•™å¤šå°‘ä¸ªå·²å®Œæˆçš„ä»»åŠ¡è®°å½•ã€‚
- **å¹‚ç­‰æ€§è¦æ±‚**ï¼šç”±äºŽè°ƒåº¦æ˜¯è¿‘ä¼¼çš„ï¼ˆå¯èƒ½å¶å°”åˆ›å»ºä¸¤ä¸ª Job æˆ–ä¸åˆ›å»ºï¼‰ï¼Œå®šä¹‰çš„ä»»åŠ¡é€»è¾‘åº”å½“æ˜¯**å¹‚ç­‰ï¼ˆIdempotentï¼‰**çš„ã€‚
- **æ—¶é—´æˆ³æ³¨è§£**ï¼šä»Ž v1.32 å¼€å§‹ï¼ŒCronJob ä¼šä¸ºåˆ›å»ºçš„ Job æ·»åŠ  `batch.kubernetes.io/cronjob-scheduled-timestamp` æ³¨è§£ï¼Œè®°å½•åŽŸå§‹é¢„å®šæ—¶é—´ã€‚

------

**æ¯”å–»ç†è§£ï¼š** å¯ä»¥å°† **CronJob** æƒ³è±¡æˆä¸€ä¸ªå·¥åŽ‚çš„â€œ**è‡ªåŠ¨æŽ’ç­è¡¨**â€ã€‚

- **Schedule** æ˜¯æŽ’ç­çš„æ—¶é—´è¡¨ï¼ˆå¦‚æ¯å¤© 8 ç‚¹ï¼‰ï¼›

- **JobTemplate** æ˜¯ç»™å‘˜å·¥çš„â€œå·¥ä½œä»»åŠ¡ä¹¦â€ï¼›

- **Concurrency Policy** å†³å®šäº†å¦‚æžœ 8 ç‚¹è¯¥æŽ¥ç­æ—¶ï¼Œä¸Šä¸€ç­çš„äººè¿˜æ²¡å¹²å®Œï¼Œæ˜¯å¤§å®¶æŒ¤åœ¨ä¸€èµ·å¹²ï¼ˆAllowï¼‰ã€è®©æ–°æ¥çš„äººå›žå®¶ä¼‘æ¯ï¼ˆForbidï¼‰ã€è¿˜æ˜¯ç›´æŽ¥æŠŠæ—§å‘˜å·¥èµ¶èµ°æ¢æ–°äººï¼ˆReplaceï¼‰ã€‚

- **StartingDeadlineSeconds** å°±åƒæ˜¯â€œè¿Ÿåˆ°å®¹å¿æœŸâ€ï¼Œå¦‚æžœå·¥åŽ‚åœç”µäº† 10 åˆ†é’Ÿï¼Œåªè¦æ²¡è¶…è¿‡å®¹å¿æœŸï¼Œæ¥ç”µåŽå‘˜å·¥è¿˜æ˜¯å¯ä»¥è¡¥ä¸Šç­ï¼›ä½†å¦‚æžœåœç”µå¤ªä¹…è¶…è¿‡äº†å®¹å¿æœŸï¼Œé‚£è¿™ä¸€ç­å°±å¹²è„†ä¸ä¸Šäº†ã€‚

  Deployment VS CronJob 

- | é•¿æœŸæœåŠ¡ (Service/Web) | ä¸€æ¬¡æ€§/å®šæ—¶ä»»åŠ¡ (Batch) | å…³ç³»è¯´æ˜Ž                               |
  | ---------------------- | ----------------------- | -------------------------------------- |
  | Deployment             | CronJob                 | æœ€ä¸Šå±‚æŽ§åˆ¶å™¨ (å®šä¹‰æœŸæœ›çŠ¶æ€/æ—¶é—´è¡¨)     |
  | ReplicaSet             | Job                     | ä¸­é—´å±‚æŽ§åˆ¶å™¨ (ç¡®ä¿å‰¯æœ¬æ•°/ç¡®ä¿ä»»åŠ¡æˆåŠŸ) |
  | Pod                    | Pod                     | æœ€å°æ‰§è¡Œå•å…ƒ (å®¹å™¨å°è£…)                |

### Managing Workloads

åœ¨ Kubernetes ä¸­ï¼Œå·¥ä½œè´Ÿè½½ç®¡ç†ï¼ˆWorkload Managementï¼‰æ¶µç›–äº†ä»Žèµ„æºç»„ç»‡ã€æ‰¹é‡æ“ä½œåˆ°åº”ç”¨ç¨‹åºçš„å¹³æ»‘æ›´æ–°ä¸Žè‡ªåŠ¨æ‰©ç¼©å®¹çš„å®Œæ•´ä½“ç³»ã€‚

ä»¥ä¸‹æ˜¯æ ¹æ®æ¥æºå¯¹ Kubernetes å·¥ä½œè´Ÿè½½ç®¡ç†æ ¸å¿ƒå†…å®¹çš„è®¨è®ºï¼š

1. èµ„æºé…ç½®çš„ç»„ç»‡ä¸Žç®¡ç†

é«˜æ•ˆçš„ç®¡ç†å§‹äºŽè‰¯å¥½çš„èµ„æºç»„ç»‡ï¼š

â€¢ **èµ„æºåˆ†ç»„**ï¼šå»ºè®®å°†å±žäºŽåŒä¸€å¾®æœåŠ¡æˆ–åº”ç”¨ç¨‹åºå±‚çº§çš„å¤šä¸ªèµ„æºï¼ˆå¦‚ Deployment å’Œ Serviceï¼‰ç»„ç»‡åœ¨åŒä¸€ä¸ª YAML æ–‡ä»¶ä¸­ï¼Œå¹¶ç”¨ `---` åˆ†éš”ã€‚

â€¢ **åˆ›å»ºé¡ºåº**ï¼šåœ¨æ¸…å•æ–‡ä»¶ä¸­ï¼Œ**å»ºè®®ä¼˜å…ˆå®šä¹‰ Service**ã€‚è¿™èƒ½ç¡®ä¿è°ƒåº¦ç¨‹åºåœ¨æŽ§åˆ¶å™¨ï¼ˆå¦‚ Deploymentï¼‰åˆ›å»º Pod æ—¶ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°å°†å®ƒä»¬åˆ†æ•£éƒ¨ç½²ã€‚

â€¢ **å¤–éƒ¨ç®¡ç†å·¥å…·**ï¼šé™¤äº†åŽŸç”Ÿæ¸…å•ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ **Helm**ï¼ˆé€šè¿‡ Chart ç®¡ç†é¢„é…ç½®çš„èµ„æºåŒ…ï¼‰æˆ– **Kustomize**ï¼ˆç”¨äºŽæ·»åŠ ã€åˆ é™¤æˆ–æ›´æ–°é…ç½®é€‰é¡¹ï¼‰æ¥ç®€åŒ–å·¥ä½œè´Ÿè½½ç®¡ç†ã€‚

2. æ‰¹é‡ä¸Žé€’å½’æ“ä½œ

`kubectl` æä¾›äº†å¼ºå¤§çš„æ‰¹é‡å¤„ç†èƒ½åŠ›ï¼š

â€¢ **æ‰¹é‡æ‰§è¡Œ**ï¼šå¯ä»¥ä½¿ç”¨ `kubectl apply -f <ç›®å½•æˆ– URL>` åŒæ—¶åˆ›å»ºå¤šä¸ªèµ„æºã€‚

â€¢ **é€’å½’å¤„ç†**ï¼šå¦‚æžœèµ„æºåˆ†å¸ƒåœ¨å¤šä¸ªå­ç›®å½•ä¸­ï¼Œé€šè¿‡æŒ‡å®š **--recursive** **æˆ–** **-R** å‚æ•°ï¼Œå¯ä»¥é€’å½’åœ°å¯¹æ‰€æœ‰å­ç›®å½•æ‰§è¡Œåˆ›å»ºã€èŽ·å–ã€åˆ é™¤æˆ–æ»šåŠ¨æ›´æ–°ç­‰æ“ä½œã€‚

â€¢ **ç­›é€‰ä¸Žé“¾å¼æ“ä½œ**ï¼šåˆ©ç”¨æ ‡ç­¾é€‰æ‹©å™¨ï¼ˆ`-l` æˆ– `--selector`ï¼‰å¯ä»¥ç²¾å‡†è¿‡æ»¤å¹¶æ‰¹é‡åˆ é™¤èµ„æºã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥å°† `kubectl` çš„è¾“å‡ºï¼ˆresource/name æ ¼å¼ï¼‰é€šè¿‡ç®¡é“ä¼ é€’ç»™å…¶ä»–å‘½ä»¤è¿›è¡Œé“¾å¼å¤„ç†ã€‚

3. åº”ç”¨ç¨‹åºçš„æ— æŸæ›´æ–°

   > Sometimes it's necessary to make narrow, non-disruptive updates to resources you've created.

æ›´æ–°æ˜¯å·¥ä½œè´Ÿè½½ç®¡ç†çš„æ—¥å¸¸æ ¸å¿ƒï¼š

â€¢ **æ»šåŠ¨æ›´æ–°ï¼ˆRolloutï¼‰**ï¼šæ”¯æŒåœ¨ä¸ä¸­æ–­æœåŠ¡çš„æƒ…å†µä¸‹æ›´æ–°åº”ç”¨ã€‚ç³»ç»Ÿä¼šé€æ¸å°†æµé‡ä»Žæ—§ Pod è½¬ç§»åˆ°å¥åº·çš„æ–° Podã€‚ç”¨æˆ·å¯ä»¥ä½¿ç”¨ `kubectl rollout` æ¥ç®¡ç†æ›´æ–°è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æŸ¥çœ‹çŠ¶æ€ã€æš‚åœã€æ¢å¤æˆ–å–æ¶ˆæ›´æ–°ã€‚

â€¢ **é‡‘ä¸é›€å‘å¸ƒï¼ˆCanary Deploymentsï¼‰**ï¼šé€šè¿‡ä½¿ç”¨ä¸åŒçš„æ ‡ç­¾ï¼ˆå¦‚ `track: stable` å’Œ `track: canary`ï¼‰æ ‡è®°ä¸åŒç‰ˆæœ¬çš„å‰¯æœ¬ï¼Œå¯ä»¥å®žçŽ°æ–°æ—§ç‰ˆæœ¬å¹¶å­˜ã€‚é€šè¿‡è°ƒæ•´å„ç‰ˆæœ¬å‰¯æœ¬çš„æ•°é‡æ¯”ä¾‹ï¼Œå¯ä»¥æŽ§åˆ¶åˆ†æµåˆ°æ–°ç‰ˆæœ¬çš„ç”Ÿäº§æµé‡æ¯”ä¾‹ã€‚

4. æ‰©ç¼©å®¹ç­–ç•¥ (Scaling)

ä¸ºäº†åº”å¯¹è´Ÿè½½å˜åŒ–ï¼ŒKubernetes æä¾›äº†æ‰‹åŠ¨å’Œè‡ªåŠ¨ä¸¤ç§æ–¹å¼ï¼š

â€¢ **æ‰‹åŠ¨æ‰©ç¼©å®¹**ï¼šä½¿ç”¨ `kubectl scale` ç›´æŽ¥å¢žåŠ æˆ–å‡å°‘å‰¯æœ¬æ•°é‡ã€‚

â€¢ **è‡ªåŠ¨æ‰©ç¼©å®¹**ï¼šä½¿ç”¨ `kubectl autoscale` é…ç½® **HorizontalPodAutoscaler (HPA)**ã€‚åœ¨å­˜åœ¨å®¹å™¨å’Œ Pod æŒ‡æ ‡æºçš„æƒ…å†µä¸‹ï¼Œç³»ç»Ÿä¼šæ ¹æ®å®žé™…è´Ÿè½½åœ¨è®¾å®šçš„èŒƒå›´å†…ï¼ˆå¦‚ 1 åˆ° 3 ä¸ªå‰¯æœ¬ï¼‰è‡ªåŠ¨è°ƒæ•´æ•°é‡ã€‚

5. èµ„æºæ›´æ–°çš„æœºåˆ¶

æ›´æ–°èµ„æºçš„æ–¹å¼å–å†³äºŽå˜æ›´çš„æ€§è´¨ï¼š

â€¢ **å°±åœ°æ›´æ–°**ï¼š

  â—¦ **kubectl apply**ï¼šå°†å½“å‰é…ç½®ä¸Žä¹‹å‰ç‰ˆæœ¬è¿›è¡Œæ¯”è¾ƒï¼Œä»…åº”ç”¨æ›´æ”¹éƒ¨åˆ†ï¼Œä¸ä¼šè¦†ç›–æœªæŒ‡å®šçš„è‡ªåŠ¨åŒ–å±žæ€§ã€‚

  â—¦ **kubectl edit**ï¼šåœ¨ç¼–è¾‘å™¨ä¸­ç›´æŽ¥äº¤äº’å¼ä¿®æ”¹èµ„æºã€‚

  â—¦ **kubectl patch**ï¼šç”¨äºŽå¯¹ API å¯¹è±¡è¿›è¡Œç‹­çª„ã€éžç ´åæ€§çš„å°±åœ°æ›´æ–°ã€‚

â€¢ **ç ´åæ€§æ›´æ–°**ï¼šå¯¹äºŽæŸäº›ä¸€æ—¦åˆå§‹åŒ–å°±æ— æ³•æ›´æ”¹çš„å­—æ®µï¼Œå¿…é¡»ä½¿ç”¨ **replace --force**ã€‚è¯¥æ“ä½œä¼šåˆ é™¤çŽ°æœ‰èµ„æºå¹¶é‡æ–°åˆ›å»ºå®ƒã€‚

--------------------------------------------------------------------------------

**æ¯”å–»ç†è§£ï¼š** ç®¡ç† Kubernetes å·¥ä½œè´Ÿè½½å°±åƒ**ç»è¥ä¸€å®¶è¿žé”é¤åŽ…**ã€‚**èµ„æºç»„ç»‡**å°±åƒæ˜¯ç¼–å†™æ ‡å‡†åŒ–çš„èœè°±ï¼ˆYAMLï¼‰å¹¶å°†ç›¸å…³çš„é£Ÿææ‰“åŒ…ï¼›**æ»šåŠ¨æ›´æ–°**å°±åƒæ˜¯åœ¨è¥ä¸šæ—¶é€ä¸ªæ›´æ¢æ—§æ¡Œæ¤…ï¼Œç¡®ä¿å®¢äººåœ¨ä»»ä½•æ—¶å€™éƒ½æœ‰ä½å­åï¼›è€Œ**è‡ªåŠ¨æ‰©ç¼©å®¹**åˆ™åƒæ˜¯ä¸€ä¸ªæ™ºèƒ½ç»ç†ï¼Œæ ¹æ®é—¨å£æŽ’é˜Ÿçš„å®¢æµé‡ï¼ˆè´Ÿè½½æŒ‡æ ‡ï¼‰ï¼Œè‡ªåŠ¨å†³å®šæ˜¯å¤šå¼€å‡ ä¸ªæœåŠ¡çª—å£è¿˜æ˜¯è®©éƒ¨åˆ†å‘˜å·¥ä¸‹ç­ä¼‘æ¯ã€‚

### Autoscaling Workloads

åœ¨ Kubernetes ä¸­ï¼Œ**å·¥ä½œè´Ÿè½½è‡ªåŠ¨æ‰©ç¼©ï¼ˆAutoscalingï¼‰** æ˜¯æŒ‡ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®å½“å‰çš„èµ„æºéœ€æ±‚ï¼Œè‡ªåŠ¨æ›´æ–°ç®¡ç† Pod çš„å¯¹è±¡ï¼ˆå¦‚ Deploymentï¼‰ï¼Œä»Žè€Œä½¿é›†ç¾¤èƒ½å¤Ÿæ›´å…·å¼¹æ€§å’Œæ•ˆçŽ‡åœ°å“åº”è´Ÿè½½å˜åŒ–ã€‚

æ ¹æ®æ¥æºï¼Œè‡ªåŠ¨æ‰©ç¼©ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªç»´åº¦å’Œå·¥å…·ï¼š

1. æ°´å¹³æ‰©ç¼©ä¸Žåž‚ç›´æ‰©ç¼©

è¿™æ˜¯è‡ªåŠ¨æ‰©ç¼©çš„ä¸¤ç§åŸºæœ¬æ–¹æ³•ï¼š

â€¢ **æ°´å¹³è‡ªåŠ¨æ‰©ç¼© (Horizontal Scaling)**ï¼šé€šè¿‡ **HorizontalPodAutoscaler (HPA)** å®žçŽ°ï¼Œå…¶æ ¸å¿ƒæ˜¯è°ƒæ•´å·¥ä½œè´Ÿè½½çš„**å‰¯æœ¬æ•°é‡ï¼ˆReplicasï¼‰**ã€‚HPA ä¼šå®šæœŸæ ¹æ®è§‚å¯Ÿåˆ°çš„èµ„æºåˆ©ç”¨çŽ‡ï¼ˆå¦‚ CPU æˆ–å†…å­˜ä½¿ç”¨æƒ…å†µï¼‰æ¥å¢žåŠ æˆ–å‡å°‘å®žä¾‹æ•°é‡ã€‚

â€¢ **åž‚ç›´è‡ªåŠ¨æ‰©ç¼© (Vertical Scaling)**ï¼šé€šè¿‡ **VerticalPodAutoscaler (VPA)** å®žçŽ°ï¼Œå…¶æ ¸å¿ƒæ˜¯è°ƒæ•´åˆ†é…ç»™å®¹å™¨çš„ **CPU å’Œå†…å­˜èµ„æºå¤§å°**ã€‚

  â—¦ **æ³¨æ„**ï¼šVPA ä¸æ˜¯ Kubernetes çš„å†…ç½®æ ¸å¿ƒç»„ä»¶ï¼Œè€Œæ˜¯éœ€è¦é¢å¤–éƒ¨ç½²çš„**æ’ä»¶**ï¼Œä¸”è¿è¡Œå®ƒå¿…é¡»å®‰è£… **Metrics Server**ã€‚

  â—¦ **çŽ°çŠ¶**ï¼šæˆªè‡³ Kubernetes v1.35ï¼ŒVPA å°šä¸æ”¯æŒ Pod èµ„æºçš„â€œå°±åœ°ï¼ˆIn-placeï¼‰â€è°ƒæ•´ï¼Œç›¸å…³é›†æˆå·¥ä½œä»åœ¨è¿›è¡Œä¸­ã€‚

2. åŸºäºŽé›†ç¾¤è§„æ¨¡çš„è‡ªåŠ¨æ‰©ç¼©

å¯¹äºŽæŸäº›éœ€è¦éšé›†ç¾¤å¤§å°å˜åŠ¨çš„ç³»ç»Ÿç»„ä»¶ï¼ˆå¦‚ `cluster-dns`ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ç‰¹å®šå·¥å…·ï¼š

â€¢ **Cluster Proportional Autoscaler**ï¼šç›‘æŽ§é›†ç¾¤ä¸­å¯è°ƒåº¦çš„**èŠ‚ç‚¹æ•°å’Œæ ¸å¿ƒæ•°**ï¼Œå¹¶ç›¸åº”åœ°ç¼©æ”¾ç›®æ ‡å·¥ä½œè´Ÿè½½çš„å‰¯æœ¬æ•°ã€‚

â€¢ **Cluster Proportional Vertical Autoscaler**ï¼šç›®å‰å¤„äºŽ Beta é˜¶æ®µï¼Œå®ƒä¸æ”¹å˜å‰¯æœ¬æ•°ï¼Œè€Œæ˜¯æ ¹æ®é›†ç¾¤èŠ‚ç‚¹æˆ–æ ¸å¿ƒçš„æ•°é‡æ¥è°ƒæ•´å·¥ä½œè´Ÿè½½çš„èµ„æºè¯·æ±‚ï¼ˆResource Requestsï¼‰ã€‚

3. äº‹ä»¶é©±åŠ¨ä¸Žå®šæ—¶æ‰©ç¼©

é™¤äº†åŸºäºŽèµ„æºæŒ‡æ ‡ï¼ˆCPU/å†…å­˜ï¼‰ï¼Œå·¥ä½œè´Ÿè½½è¿˜å¯ä»¥åŸºäºŽå¤–éƒ¨äº‹ä»¶æˆ–æ—¶é—´è¡¨è¿›è¡Œæ‰©ç¼©ï¼š

â€¢ **äº‹ä»¶é©±åŠ¨æ‰©ç¼©**ï¼šåˆ©ç”¨ **KEDA (Kubernetes Event Driven Autoscaler)**ï¼Œå¯ä»¥æ ¹æ®å¾…å¤„ç†çš„**äº‹ä»¶æ•°é‡**ï¼ˆä¾‹å¦‚é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯æ•°ï¼‰è¿›è¡Œæ‰©ç¼©ã€‚KEDA æä¾›äº†ä¸°å¯Œçš„é€‚é…å™¨æ¥è¿žæŽ¥ä¸åŒçš„äº‹ä»¶æºã€‚

â€¢ **å®šæ—¶æ‰©ç¼©**ï¼šé€šè¿‡ KEDA çš„ **Cron æ‰©ç¼©å™¨**ï¼Œç”¨æˆ·å¯ä»¥å®šä¹‰ç‰¹å®šçš„**æ—¶é—´è¡¨å’Œæ—¶åŒº**æ¥æ‰§è¡Œæ‰©ç¼©æ“ä½œã€‚è¿™åœ¨ä¸šåŠ¡é«˜å³°æœŸå‰æå‰æ‰©å®¹ï¼Œæˆ–åœ¨éžé«˜å³°æœŸï¼ˆOff-peak hoursï¼‰ç¼©å®¹ä»¥èŠ‚çœæˆæœ¬æ—¶éžå¸¸æœ‰ç”¨ã€‚

4. åŸºç¡€è®¾æ–½å±‚é¢çš„æ‰©ç¼©

å½“å·¥ä½œè´Ÿè½½å±‚çš„è‡ªåŠ¨æ‰©ç¼©ä»æ— æ³•æ»¡è¶³éœ€æ±‚æ—¶ï¼Œå¯èƒ½éœ€è¦å¯¹**é›†ç¾¤åŸºç¡€è®¾æ–½**æœ¬èº«è¿›è¡Œæ‰©ç¼©ã€‚è¿™é€šå¸¸æ„å‘³ç€è‡ªåŠ¨æ·»åŠ æˆ–ç§»é™¤é›†ç¾¤ä¸­çš„**èŠ‚ç‚¹ï¼ˆNodesï¼‰**ã€‚

--------------------------------------------------------------------------------

**æ¯”å–»ç†è§£ï¼š** å¯ä»¥å°† **Kubernetes å·¥ä½œè´Ÿè½½è‡ªåŠ¨æ‰©ç¼©** æƒ³è±¡æˆä¸€å®¶**è‡ªåŠ¨åŒ–çš„åˆ¶è¡£åŽ‚**ï¼š

â€¢ **æ°´å¹³æ‰©ç¼© (HPA)** å°±åƒæ˜¯å‘çŽ°è®¢å•å˜å¤šæ—¶ï¼Œè‡ªåŠ¨å¢žå¼€å‡ æ¡åŒæ ·çš„ç”Ÿäº§çº¿ï¼ˆå‰¯æœ¬ï¼‰ï¼›

â€¢ **åž‚ç›´æ‰©ç¼© (VPA)** åˆ™æ˜¯å‘çŽ°è¡£æœå¤ªåŽšï¼Œè‡ªåŠ¨ç»™çŽ°æœ‰çš„ç¼çº«æœºå‡çº§æ›´å¼ºåŠ›çš„é©¬è¾¾ï¼ˆèµ„æºï¼‰ï¼›

â€¢ **åŸºäºŽé›†ç¾¤è§„æ¨¡çš„æ‰©ç¼©** å°±åƒæ˜¯åŽ‚æˆ¿æ¯æ‰©å»ºä¸€ä¸ªè½¦é—´ï¼Œå°±è‡ªåŠ¨å¤šé…å‡ ä¸ªä¿æ´å‘˜ï¼›

â€¢ **äº‹ä»¶é©±åŠ¨ (KEDA)** åˆ™åƒæ˜¯ä»“åº“é—¨å£çš„æ„Ÿåº”å™¨ï¼Œçœ‹åˆ°è´§è½¦ï¼ˆæ¶ˆæ¯é˜Ÿåˆ—ï¼‰æŽ’é˜Ÿäº†ï¼Œæ‰å‘½ä»¤å·¥åŽ‚ç«‹åˆ»å¼€å·¥ã€‚
