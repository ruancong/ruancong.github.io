<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.26" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const useChoice = localStorage.getItem('vuepress-color-scheme')
      const systemStatus =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (useChoice === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (useChoice === 'dark' || systemStatus) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <link rel="icon" href="/images/logo.svg"><title>Notes</title><meta name="description" content="Daily Learning Notes">
    <link rel="preload" href="/assets/style-CMLn62r-.css" as="style"><link rel="stylesheet" href="/assets/style-CMLn62r-.css">
    <link rel="modulepreload" href="/assets/app-5bSHB77E.js"><link rel="modulepreload" href="/assets/k8s.html-ByzAVqCA.js">
    <link rel="prefetch" href="/assets/index.html-EIsbJnGC.js" as="script"><link rel="prefetch" href="/assets/commands.html-CWDxsP_D.js" as="script"><link rel="prefetch" href="/assets/linux.html-Hh983vSa.js" as="script"><link rel="prefetch" href="/assets/MinIO.html-DsPNN8_U.js" as="script"><link rel="prefetch" href="/assets/PostgreSQL.html-Cui4y5Ah.js" as="script"><link rel="prefetch" href="/assets/docker.html-DNFNdpVI.js" as="script"><link rel="prefetch" href="/assets/forget-lock-psw.html-DBZeO6zn.js" as="script"><link rel="prefetch" href="/assets/raft.html-Dg7iV-E6.js" as="script"><link rel="prefetch" href="/assets/split-brain.html-BQCdre1U.js" as="script"><link rel="prefetch" href="/assets/gemini_manual.html-iremtdM-.js" as="script"><link rel="prefetch" href="/assets/network-basis.html-BgNpkERV.js" as="script"><link rel="prefetch" href="/assets/问题解决.html-BAMc_jzj.js" as="script"><link rel="prefetch" href="/assets/English-Grammar.html-DK1tJDkU.js" as="script"><link rel="prefetch" href="/assets/英语句子解析.html-Tkl4MvX-.js" as="script"><link rel="prefetch" href="/assets/语法学习.html-Xfi9-y47.js" as="script"><link rel="prefetch" href="/assets/template.html-DZY2X9am.js" as="script"><link rel="prefetch" href="/assets/react-router.html-BGDUfxfg.js" as="script"><link rel="prefetch" href="/assets/react.html-7AT65Uo7.js" as="script"><link rel="prefetch" href="/assets/nextjs.html-DWmTNmzI.js" as="script"><link rel="prefetch" href="/assets/spingboot-3.x.html-BSQKzoB1.js" as="script"><link rel="prefetch" href="/assets/05-20.html-CyDVyjG8.js" as="script"><link rel="prefetch" href="/assets/05-21.html-BXZD2C0J.js" as="script"><link rel="prefetch" href="/assets/05-22.html-Bigh-iIG.js" as="script"><link rel="prefetch" href="/assets/05-23.html-2Ll2Kbgk.js" as="script"><link rel="prefetch" href="/assets/02-26.html-DaxfOTFv.js" as="script"><link rel="prefetch" href="/assets/01-03.html-BiqHbmtY.js" as="script"><link rel="prefetch" href="/assets/01-08.html-BMGyebDQ.js" as="script"><link rel="prefetch" href="/assets/01-10.html-4ub-YINf.js" as="script"><link rel="prefetch" href="/assets/01-15.html-C7LTsHei.js" as="script"><link rel="prefetch" href="/assets/07-09.html-D5CIHEkK.js" as="script"><link rel="prefetch" href="/assets/07-10.html-B_WjzG8S.js" as="script"><link rel="prefetch" href="/assets/07-11.html-Iiy2Ypqc.js" as="script"><link rel="prefetch" href="/assets/07-12.html-D4u7SZVf.js" as="script"><link rel="prefetch" href="/assets/07-13.html-CxW0Squ3.js" as="script"><link rel="prefetch" href="/assets/07-14.html-CM9dPTCf.js" as="script"><link rel="prefetch" href="/assets/07-15.html-noJK1RFF.js" as="script"><link rel="prefetch" href="/assets/07-16.html-CYetZYEI.js" as="script"><link rel="prefetch" href="/assets/09-03.html-WkZ5qhGg.js" as="script"><link rel="prefetch" href="/assets/404.html-BT93DM84.js" as="script"><link rel="prefetch" href="/assets/VocabularyAudio-Ctd_Kc6Z.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/"><img class="vp-site-logo" src="/images/logo.svg" alt="Notes"><span class="vp-site-name vp-hide-mobile" aria-hidden="true">Notes</span></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="auto-link external-link" href="https://github.com/ruancong/ruancong.github.io" aria-label="GitHub" rel="noopener noreferrer" target="_blank"><!--[--><!--[--><!--]--><!--]-->GitHub<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="route-link auto-link" href="/" aria-label="首页"><!--[--><!--[--><!--]--><!--]-->首页<!--[--><!--[--><!--]--><!--]--></a></div><div class="vp-navbar-item"><a class="auto-link external-link" href="https://github.com/ruancong/ruancong.github.io" aria-label="GitHub" rel="noopener noreferrer" target="_blank"><!--[--><!--[--><!--]--><!--]-->GitHub<!--[--><!--[--><!--]--><!--]--></a></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><p tabindex="0" class="vp-sidebar-item vp-sidebar-heading"> <!----></p><!----></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div vp-content><!--[--><!--]--><div id="content"><h2 id="测试工具安装" tabindex="-1"><a class="header-anchor" href="#测试工具安装"><span>测试工具安装</span></a></h2><ol><li><p>安装kubectl</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 第一步</span></span>
<span class="line"><span class="token function">curl</span> <span class="token parameter variable">-LO</span> <span class="token string">&quot;https://dl.k8s.io/release/<span class="token variable"><span class="token variable">$(</span><span class="token function">curl</span> <span class="token parameter variable">-L</span> <span class="token parameter variable">-s</span> https://dl.k8s.io/release/stable.txt<span class="token variable">)</span></span>/bin/linux/amd64/kubectl&quot;</span></span>
<span class="line"><span class="token comment"># 第二步</span></span>
<span class="line"><span class="token function">sudo</span> <span class="token function">install</span> <span class="token parameter variable">-o</span> root <span class="token parameter variable">-g</span> root <span class="token parameter variable">-m</span> 0755 kubectl /usr/local/bin/kubectl</span>
<span class="line"><span class="token comment"># 验证版本号</span></span>
<span class="line">kubectl version <span class="token parameter variable">--client</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>安装k3d</p><p>参数 <code>https://k3d.io/stable/#releases</code></p><p>安装的k3d默认创建的集群的对应的版本一般不是最新的对应的k8s API的版本。可以用配置文件来指定最新的k3s镜像，来对应k8s版本。</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token comment"># k3d 启动的配置文件, 定义使用最新的k3s版本[k8s版本]</span></span>
<span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> k3d.io/v1alpha5 <span class="token comment"># 使用最新的API版本以获得所有功能</span></span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Simple</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>cluster <span class="token comment"># 可以定义集群名称，也可以不指定，在k3d create cluster时指定</span></span>
<span class="line"><span class="token key atrule">image</span><span class="token punctuation">:</span> rancher/k3s<span class="token punctuation">:</span>latest <span class="token comment"># 在这里指定你想要的“默认”K3s镜像版本</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment">## 以下输入是静态的，是跟着k3d的版本走的</span></span>
<span class="line">leite@leite-company ~<span class="token operator">&gt;</span> k3d version</span>
<span class="line">k3d version v5.8.3</span>
<span class="line">k3s version v1.31.5-k3s1 <span class="token punctuation">(</span>default<span class="token punctuation">)</span></span>
<span class="line"><span class="token comment">## 输出以下说明已正确应用了最新的k3s版本</span></span>
<span class="line">leite@leite-company ~ <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">&gt;</span> kubectl version</span>
<span class="line">Client Version: v1.34.1</span>
<span class="line">Kustomize Version: v5.7.1</span>
<span class="line">Server Version: v1.34.1+k3s1</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol><h2 id="kubernetes-components" tabindex="-1"><a class="header-anchor" href="#kubernetes-components"><span>Kubernetes Components</span></a></h2><p>The components of a Kubernetes cluster:</p><p><img src="/assets/image-20250815091851689-BttBua7w.png" alt="image-20250815091851689"></p><h2 id="pod" tabindex="-1"><a class="header-anchor" href="#pod"><span>Pod</span></a></h2><ul><li><p>Pods are the smallest deployable units of computing that you can create and manage in Kubernetes</p></li><li><p>You don&#39;t need to run multiple containers to provide replication (for resilience or capacity)</p></li><li><p>Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for running container(s). A Pod persists until it is deleted.</p></li><li><p>Modifying the pod template or switching to a new pod template has no direct effect on the Pods that already exist.</p><blockquote><p>只会根据新的更新的模板重新创建一个pod</p></blockquote></li><li><p>The name of a Pod must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostname. For best compatibility, the name should follow the more restrictive rules for a DNS label</p></li><li><p><code>kubectl describe pod</code> 命令。它会告诉你 Pod 启动过程中发生的详细事件记录。</p></li><li><p>怎么知道我的pod有没有启动成功？</p><blockquote><ol><li>宏观检查：<code>kubectl get pods</code> 还可以增加<code>--watch</code> 还实时观察</li><li>详细诊断：<code>kubectl describe pod [pod-name]</code></li><li>深入应用内部：<code>kubectl logs [pod-name]</code> 【可以用--previous选择来查看上一次的日志，还可以用-f】</li></ol></blockquote></li><li><p>确认 Pod 内的应用是否真的正常工作</p><blockquote><p>运行以下命令，它会在你的本地 <code>8084</code> 端口和 Deployment 中的一个 Pod 的 <code>8084</code> 端口之间建立一个临时的、直接的通道：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl port-forward deployment/springboot3-deployment <span class="token number">8084</span>:8084</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><blockquote><p>在你的<strong>本地计算机</strong>和集群内的 <strong>Pod</strong> 之间，通过 Kubernetes API Server 建立了一条<strong>临时的、加密的、点对点的通信隧道</strong>。</p><p><strong>没有负载均衡</strong>：<code>port-forward</code> 不会在多个副本（replicas）之间轮询或分发流量。所有请求都会被送到同一个 Pod 实例上。</p><p><strong>会话是“粘性”的</strong>：在你按下 <code>Ctrl+C</code> 结束 <code>port-forward</code> 命令之前，这个隧道会一直连接到最初选定的那个 Pod。</p><p><strong>没有自动故障转移</strong>：如果在 <code>port-forward</code> 运行期间，它连接的那个 Pod 恰好“坏了”并被 Kubernetes 重启，你的 <code>port-forward</code> 连接会<strong>中断</strong>，命令会报错并退出。</p></blockquote><p>还可以在在集群内部测试 ：</p><p><strong>启动一个临时的测试 Pod</strong>：我们可以运行一个包含 <code>curl</code> 等网络工具的临时 Pod。</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 运行一个临时的 busybox Pod，并在结束后自动删除</span></span>
<span class="line">kubectl run my-test-pod <span class="token parameter variable">--image</span><span class="token operator">=</span>busybox <span class="token parameter variable">-it</span> <span class="token parameter variable">--rm</span> -- <span class="token function">sh</span></span>
<span class="line"><span class="token comment">## kubectl run my-debug-pod --image=curlimages/curl -i --tty --rm -- /bin/sh 这个也可以</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>在临时 Pod 内通过 Service 名称访问</strong>：Kubernetes 自带了 DNS 服务，你可以直接通过 Service 的名称来访问它。</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 假设你已经在 my-test-pod 的 shell 中</span></span>
<span class="line"><span class="token comment"># 语法: wget -qO- http://[service-name]:[service-port]</span></span>
<span class="line"><span class="token function">wget</span> -qO- http://springboot3-service:80</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果返回了应用的正确响应，说明 Service 的服务发现和端口转发都是正常的。</p></blockquote></li><li><p>什么时候会单独定义和使用 Pod</p><blockquote><p><strong>场景示例</strong>：你想测试一下集群内部的网络是否通畅，或者想看某个 <code>Service</code> 是否能被访问到。</p><p><strong>操作</strong>：你可以快速创建一个包含网络工具（如 <code>curl</code>, <code>ping</code>, <code>dig</code>）的 Pod，然后通过 <code>kubectl exec</code> 进入这个 Pod 进行调试。调试结束后，直接删除这个 Pod 即可，不留任何痕迹。</p><p><strong>示例 YAML (</strong><code>debug-pod.yaml</code><strong>)：</strong></p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line"><span class="token key atrule">name</span><span class="token punctuation">:</span> curl<span class="token punctuation">-</span>pod</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line"> <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>curl <span class="token comment"># 我们用一个包含 curl 的镜像，并让它一直运行，以便我们能 exec 进去</span></span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> curlimages/curl<span class="token punctuation">:</span>latest</span>
<span class="line">    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sleep&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;3600&quot;</span><span class="token punctuation">]</span> <span class="token comment"># 让容器保持运行，否则它会立即退出</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>使用命令</strong>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 创建 Pod</span></span>
<span class="line">kubectl apply <span class="token parameter variable">-f</span> debug-pod.yaml</span>
<span class="line"></span>
<span class="line"><span class="token comment"># 进入 Pod 内部执行命令</span></span>
<span class="line">kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> curl-pod -- <span class="token function">sh</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># (在 Pod 内部)</span></span>
<span class="line"><span class="token comment"># curl [your-service-name].[namespace].svc.cluster.local</span></span>
<span class="line"><span class="token comment"># exit</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 调试完毕后删除 Pod</span></span>
<span class="line">kubectl delete pod curl-pod </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>Pods that are part of a DaemonSet tolerate being run on an unschedulable Node. DaemonSets typically provide node-local services that should run on the Node even if it is being drained of workload applications.</p><blockquote><p>在 Kubernetes 中，这些“必须安装在每个节点上”的后台服务，就是通过 DaemonSet 来部署的。常见的例子有：日志收集器，节点监控器，网络插件，存储插件</p></blockquote></li></ul><h2 id="deployment" tabindex="-1"><a class="header-anchor" href="#deployment"><span>Deployment</span></a></h2><ul><li><p>Deployment：负责管理和维护你的应用实例（Pod）。它会确保指定数量的 Nginx Pod 正在运行。如果某个 Pod 挂掉了，Deployment 会自动创建一个新的来替代它</p></li><li><p>在 Deployment（以及 ReplicaSet, StatefulSet, Job, CronJob 等这类控制器）的 Pod 模板（spec.template）中，metadata.name 这个字段是不能设置的。如果你尝试设置它，Kubernetes API Server 会拒绝你的请求。</p></li><li><p>Deployment 要能够正常工作（特别是运行多个副本、进行滚动更新和自我修复），其底层的 Pod 必须通过类似 <code>generateName</code> 的机制来创建，以保证每个 Pod 名称的唯一性</p></li><li><p>The server may generate a name when generateName is provided instead of name in a resource create request. When generateName is used, the provided value is used as a name prefix, which server appends a generated suffix to.</p><blockquote><p>Kubernetes v1.31以后会重试8次以使生成唯一的名字</p></blockquote></li><li><p>在 Deployment（以及 ReplicaSet, StatefulSet, Job, CronJob 等这类控制器）的 Pod 模板（<code>spec.template</code>）中，<code>metadata.name</code> 这个字段是<strong>不能设置</strong>的。如果你尝试设置它，Kubernetes API Server 会拒绝你的请求。</p></li><li><p>一个 Deployment 实际上并不直接管理 Pod，它的工作流程是这样的：</p><ol><li><p><strong>Deployment</strong>: 你创建了一个 Deployment 资源，它的名称是固定的（比如 <code>nginx-deployment</code>）。这个 Deployment 负责管理“版本”。</p></li><li><p><strong>ReplicaSet</strong>: Deployment 会根据自己的 Pod 模板，创建一个 <strong>ReplicaSet</strong> 资源。这个 ReplicaSet 的名称是<strong>动态生成的</strong>，通常是 <code>[Deployment名称]-[Pod模板的哈希值]</code>，例如 <code>nginx-deployment-66b6c48dd5</code>。这个哈希值确保了每次你更新 Deployment 的 Pod 模板时（比如更换镜像版本），都会创建一个全新的、不同名称的 ReplicaSet。</p><blockquote><p>kubectl get rs</p><p>kubectl get replicateSet</p></blockquote></li><li><p><strong>Pod</strong>: ReplicaSet 的任务很简单，就是确保有指定数量的、符合其模板的 Pod 正在运行。它会根据自己的名称作为<strong>前缀</strong>，去创建 Pod。所以，最终 Pod 的名称也是<strong>动态生成的</strong>，格式通常是 <code>[ReplicaSet名称]-[随机后缀]</code>，例如 <code>nginx-deployment-66b6c48dd5-x7p9m</code>。</p></li></ol></li></ul><h2 id="servcie" tabindex="-1"><a class="header-anchor" href="#servcie"><span>Servcie</span></a></h2><ul><li><p>Service：负责为一组 Pod 提供一个稳定、统一的访问入口。因为 Pod 是“短暂”的，它们的 IP 地址会变化。Service 提供了一个固定的 IP 地址和 DNS 名称，使得其他应用或外部用户可以方便地访问到你的 Nginx 服务，而无需关心后端具体是哪个 Pod 在提供服务。</p><blockquote><p>Service 的 IP 地址 (<code>ClusterIP</code>) 和 DNS 名称的“固定”，是相对于 <strong>Service 这个 API 对象的生命周期</strong> 而言的。</p><p>简单来说，只要你不删除这个 Service 对象（<code>kubectl delete service my-service</code>），它的 <code>ClusterIP</code> 和 DNS 名称就<strong>不会改变</strong>。</p><p>FQDN (Fully Qualified Domain Name)： [service-name].[namespace-name].svc.[cluster-domain].</p><ul><li><p><code>[cluster-domain]</code> 集群域名是集群级别的配置，通常是固定的。最可靠的查找方法是进入任意一个正在运行的 Pod，查看它的 DNS 配置文件 <code>/etc/resolv.conf</code></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 1. 首先，随便找一个正在运行的 Pod</span></span>
<span class="line">kubectl get pods</span>
<span class="line"></span>
<span class="line"><span class="token comment"># 假设我们找到了一个叫 nginx-deployment-6bcfd6f857-klmno 的 Pod</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 2. 使用 kubectl exec 进入该 Pod 并查看 resolv.conf 文件</span></span>
<span class="line">kubectl <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> nginx-deployment-6bcfd6f857-klmno -- <span class="token function">cat</span> /etc/resolv.conf</span>
<span class="line"><span class="token comment">## 在上面这个命令中，cat 前面的`--` 起到分隔的作用，告诉kubectl的命令已经结束。与docker exec不同，docker exec 不需要这个                                   </span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>输出中<code>search default.svc.cluster.local svc.cluster.local cluster.local</code> 最后这个<code>cluster.local</code> 就是这个值</p></li><li><p>还可以在集群内直接查询</p><p>启动一个临时的调试 Pod</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 运行一个临时的 busybox Pod，并进入其 shell 环境</span></span>
<span class="line">kubectl run dns-test <span class="token parameter variable">-it</span> <span class="token parameter variable">--rm</span> <span class="token parameter variable">--image</span><span class="token operator">=</span>busybox:1.28 -- <span class="token function">sh</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>在 Pod 内部使用 <code>nslookup</code> 进行查询</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># (因为我们在 default 命名空间里)</span></span>
<span class="line"><span class="token function">nslookup</span> springboot3-service</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div></li></ul></blockquote></li><li><p>你可以把 app: nginx 理解为你和 Kubernetes 的一个约定：你给一组 Pod 贴上这个独特的“名牌”，然后告诉 Deployment 和 Service 按照这个“名牌”去认领和查找它们</p></li><li><p>Service <code>type</code> 是 <code>ClusterIP</code>【默认值】时ip不直接暴露到集群外部，只能被集群内的 Ingress 控制器找到。type为loadBalancer时, 端口会暴露到集群外。【在k3d中测试时，把service的type设置为loadBalance并不生效】</p><blockquote><p><code>LoadBalancer</code> 类型是 <code>NodePort</code> 的扩展。它会向底层云平台（如 AWS, GCP, Azure）请求一个外部负载均衡器，并将这个负载均衡器的 IP 地址作为 Service 的外部访问入口。</p><ul><li><strong>作用</strong>：这是将服务暴露到公网的 <strong>标准方式</strong>。云服务提供商会为你创建一个负载均衡器，并将流量导向你所有节点的 <code>NodePort</code>。</li><li><strong>使用场景</strong>：适用于生产环境，当你需要一个稳定、高可用的公网 IP 来暴露你的服务时。</li></ul></blockquote></li></ul><h2 id="config-file" tabindex="-1"><a class="header-anchor" href="#config-file"><span>Config file</span></a></h2><ul><li><p>kubectl 默认会在你的用户主目录下的 .kube 文件夹中寻找名为 config 的文件。</p><blockquote><p>在 Linux 和 macOS 上，路径通常是 ~/.kube/config。</p></blockquote></li><li><p>Most often, you provide the information to kubectl in a file known as a manifest. By convention, manifests are YAML (you could also use JSON format).</p></li><li><p>YAML 文件在两种模式下的“角色”</p><ul><li>在 kubectl create -f (命令式) 中：YAML 文件是一个一次性的模板。你命令 Kubernetes：“按照这个模板，给我创建一个对象”。创建完成后，这个 YAML 文件和集群中的那个对象之间，就没有必然的联系了。Kubernetes 不会“记住”你是用哪个文件创建的它。</li><li>在 kubectl apply -f (声明式) 中：YAML 文件是对象的**“期望状态”的声明**。你告诉 Kubernetes：“请确保集群中有一个与这个 YAML 文件描述的状态相匹配的对象”。Kubernetes 不仅会创建这个对象，还会记录下这个“期望状态”，以便于未来的比较和更新。</li></ul></li><li><p>仅仅修改并保存在本地 configs/ 目录下的 YAML 文件，并不会对集群产生任何影响。 Kubernetes 集群完全不知道你本地文件的变化。你必须通过 kubectl apply 这个动作，明确地告诉 Kubernetes：“请按照我最新的配置文件，去同步集群的状态。”</p></li><li><p>仅仅修改并保存在本地 configs/ 目录下的 YAML 文件，并不会对集群产生任何影响。 Kubernetes 集群完全不知道你本地文件的变化。你必须通过 kubectl apply 这个动作，明确地告诉 Kubernetes：“请按照我最新的配置文件，去同步集群的状态。”</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"> kubectl <span class="token function">diff</span> <span class="token parameter variable">-f</span> configs/</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Our previous example (replicas): The change from kubectl scale was NOT retained because the replicas field was &quot;owned&quot; by your YAML file. apply enforced your file&#39;s value.</p></li><li><p>The note&#39;s meaning (LoadBalancer example): Changes from other controllers (like adding a clusterIP) ARE retained, because those fields are not &quot;owned&quot; by your YAML file. The patch mechanism surgically updates only the fields you explicitly manage in your file.</p></li><li><p>Starting with Kubernetes v1.25, the API server offers server side field validation that detects unrecognized or duplicate fields in an object. It provides all the functionality of kubectl --validate on the server side.</p><blockquote><p><strong>服务端试运行 (Server-side Dry Run)</strong></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl apply <span class="token parameter variable">-f</span> <span class="token punctuation">[</span>your-manifest<span class="token punctuation">]</span>.yaml --dry-run<span class="token operator">=</span>server</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果文件有错误，它会像上面的例子一样报错。如果文件格式正确，它会返回一个成功的提示（但不会真的创建资源）。<strong>总之，--dry-run=server 是一个非常安全的验证工具。</strong> 它的设计初衷就是为了让您在真正部署到集群之前，百分之百确认您的配置清单是有效且被集群所接受的，而无需担心会意外创建或修改任何东西。</p></blockquote></li></ul><h2 id="object" tabindex="-1"><a class="header-anchor" href="#object"><span>Object</span></a></h2><p>Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.</p><ul><li><p>Keep in mind that label Key must be unique for a given object</p></li><li><p>Names of resources need to be unique within a namespace, but not across namespaces.</p></li><li><p>When you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as some basic information about the object (such as a name).</p></li><li><p>Almost every Kubernetes object includes two nested object fields that govern the object&#39;s configuration: the object spec and the object status.</p></li><li><p>The status describes the current state of the object, supplied and updated by the Kubernetes system and its components.</p><blockquote><p>status可以理解为“看起来是什么样”，而state是“实际是什么样子的”</p></blockquote></li><li><p>Each object in your cluster has a Name that is unique for that type of resource. Every Kubernetes object also has a UID that is unique across your whole cluster.</p><blockquote><p>For example, you can only have one Pod named <code>myapp-1234</code> within the same namespace, but you can have one Pod and one Deployment that are each named <code>myapp-1234</code>.</p></blockquote></li><li><p><strong>Kubernetes 的世界观是建立在它自己的 API 对象上的</strong>。它通过 Kubelet 等组件来观测外部物理世界的状态，并尽力使其与内部的声明式状态保持一致。但如果外部世界发生了它无法观测到的剧烈变化（比如一个节点被偷偷替换了），而内部的逻辑对象没有被相应更新，就会导致这种“身份混淆”和状态不一致，从而引发各种难以排查的诡异问题。</p><blockquote><p>在物理/虚拟层面销毁一个节点之前，<strong>务必先在 Kubernetes 中将其删除</strong>。</p><p>正确的操作流程应该是：</p><ol><li><p><strong>标记节点不可调度</strong>：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl cordon worker-01</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这能防止新的 Pod 被调度到该节点上。</p></li><li><p><strong>驱逐节点上的所有 Pod</strong>：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl drain worker-01 --ignore-daemonsets</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这会安全地将该节点上现有的 Pod 迁移到其他节点。<code>--ignore-daemonsets</code> 是因为 DaemonSet 管理的 Pod 不需要被驱逐。</p></li><li><p><strong>从 Kubernetes 中删除节点对象</strong>：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl delete <span class="token function">node</span> worker-01</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这一步就是“销毁学籍卡”，彻底清除它在 Kubernetes 中的所有记录。</p></li><li><p><strong>销毁物理/虚拟机</strong>： 现在，你可以安全地去你的云平台或虚拟化平台删除这台服务器了。</p></li></ol></blockquote></li><li><p>A client-provided string that refers to an object in a <a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#standard-api-terminology" target="_blank" rel="noopener noreferrer">resource</a> URL, such as <code>/api/v1/pods/some-name</code>.</p><p>Only one object of a given kind can have a given name at a time. Names must be unique across <strong>all API versions</strong> of the same resource. API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name. In other words, API version is irrelevant in this context.</p></li><li><p>In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.</p><blockquote><ol><li><p>标记节点不可调度</p><p>kubectl cordon worker-01</p></li><li><p><strong>驱逐节点上的所有 Pod</strong></p><p>kubectl drain worker-01 --ignore-daemonsets</p></li><li><p>从 Kubernetes 中删除节点对象</p><p>kubectl delete node worker-01</p></li></ol></blockquote></li></ul><h2 id="kubernetes-api" tabindex="-1"><a class="header-anchor" href="#kubernetes-api"><span>Kubernetes API</span></a></h2><ul><li><p>There are two mechanisms that Kubernetes uses to publish these API specifications</p><blockquote><ol><li>The Discovery API</li><li>The Kubernetes OpenAPI Document</li></ol></blockquote></li><li><p>首先，我们必须明白 Discovery API 的目的。无论是 kubectl、Rancher UI 还是任何其他与 Kubernetes 集群交互的客户端，它们在执行操作之前，都需要先知道：</p><ul><li>“这个集群里有哪些 API Group？”: (例如 apps, batch, networking.k8s.io 等)。</li><li>“每个 Group 下有哪些版本？” : (例如 apps group 下有v1)“</li><li>每个 Group/Version 下有哪些资源 (Resource)？” : (例如 apps/v1 下有 deployments, statefulsets, daemonsets 等)</li><li>“这些资源支持哪些操作 (Verb)？” : (例如 deployments 支持 create, get, list, delete 等)</li></ul></li><li><p>Unaggregated Discovery (非聚合发现) Unaggregated Discovery 指的是 单个 API 服务器自身 提供的、关于 它自己所能服务的 API 的发现信息。</p></li><li><p>Aggregated Discovery (聚合发现) Aggregated Discovery 正是 Kubernetes API Aggregation Layer (聚合层) 的强大之处。它提供了一个 统一的、聚合后 的视图。 当客户端（如 kubectl）查询主 kube-apiserver 的发现端点时，聚合层不仅会返回 kube-apiserver 自己的 API 信息，还会智能地将所有已注册的扩展 API 服务器（通过 APIService 对象注册）的发现信息也一并包含进来并返回。</p></li><li><p>Kubernetes offers stable support for aggregated discovery, publishing all resources supported by a cluster through two endpoints (/api and /apis).</p><blockquote><ul><li>/api: 列出核心 API Group (只有 v1)。【<strong>核心 API (Core API)</strong> 或称为<strong>历史遗留 API (Legacy API)</strong>】</li><li>/apis: 列出所有非核心的 API Group (如 apps, batch, apiextensions.k8s.io 等)。【分组 API (Grouped API)】 为什么会有两个端点： 最初的设计: 在 Kubernetes 的早期，所有的 API 资源对象（如 Pod, Service, Node, ReplicationController 等）都被放在一个没有名字的 API Group 里，这个 Group 就是我们所说的“核心组 (Core Group) ”。由于它没有名字，为了访问它，API Server 就提供了 /api/v1 这个特殊的端点。在当时，这就是 Kubernetes 的全部 API。 发现扩展性问题: 随着项目的发展，开发者们很快意识到，把所有东西都塞进一个没有分组的 API 里是无法扩展的。如果我想添加一组新的 API 用于处理“批处理任务”，或者另一组 API 用于处理“网络策略”，把它们都堆在核心组里会变得非常混乱。 “命名组”的诞生: 为了解决这个问题，Kubernetes 引入了“API Group（命名组）”的概念。这允许开发者根据功能领域将 API 资源进行逻辑分组。例如： apps 组：包含 Deployment, StatefulSet, DaemonSet 等。batch 组：包含 Job, CronJob 等。 networking.k8s.io 组：包含 Ingress, NetworkPolicy 等。 所有这些“命名组”的 API 都通过一个统一的前缀 /apis 来访问，例如 /apis/apps/v1，/apis/batch/v1。</li></ul></blockquote></li><li><p>执行这个命令，你会看到 kubectl 正在向 apiserver 发出一系列的 GET 请求来发现资源</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pods <span class="token parameter variable">--v</span><span class="token operator">=</span><span class="token number">8</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li><li><p>查询所有可用api</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl api-versions</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li><li><p>直接访问 API Server去查询有哪些有用api-versions</p><p><code>kubectl</code> 命令实际上是在后台向 Kubernetes API Server 发送 HTTP 请求。我们也可以手动模拟这个过程来探索 API。为了安全地访问 API Server，最简单的方式是使用 <code>kubectl proxy</code>。</p><p>在一个终端中运行以下命令，让这个终端保持运行。</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl proxy</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>打开另一个终端，使用 <code>curl</code> 进行查询。</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">curl</span> http://127.0.0.1:8001/api</span>
<span class="line"><span class="token comment">## /apis</span></span>
<span class="line"><span class="token function">curl</span> http://127.0.0.1:8001/apis</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>查询所有可用的 API 资源 (<code>api-resources</code>)</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl api-resources</span>
<span class="line"><span class="token comment"># --api-group=&quot;&quot; 表示查询核心组</span></span>
<span class="line">kubectl api-resources --api-group<span class="token operator">=</span><span class="token string">&quot;&quot;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>Without indicating the resource type using the Accept header, the default response for the /api and /apis endpoint is an unaggregated discovery document.</p></li><li><p>the kubectl tool fetches and caches the API specification for enabling command-line completion and other features. The two supported mechanisms are as follows:</p><ul><li>Discovery API 就像是这本书的 “目录”。</li><li>OpenAPI Document 就像是这本书 “正文内容中所有名词的详细解释和语法结构说明”</li></ul></li></ul><h2 id="k3d-测试相关" tabindex="-1"><a class="header-anchor" href="#k3d-测试相关"><span>k3d 测试相关</span></a></h2><ul><li><p>在生产环境中，通常会有多个 Master 节点（在 k3d/k3s 里被称为 Server 节点）来确保高可用性。你不会直接连接到某一个 Master 节点，因为如果那个节点宕机了，你就无法访问集群了。正确的做法是连接到一个<strong>负载均衡器 (Load Balancer)</strong>，由它来将你的请求转发给后面健康的 Master 节点。</p><p>k3d 在本地用 Docker 容器巧妙地复现了这套架构：</p><ol><li><strong><code>k3d-my-cluster-server-0</code> 容器</strong>: 这是真正的 K3s Server，它在<strong>容器内部</strong>运行着 Kubernetes API Server，监听着 <code>6443</code> 端口。这个容器没有直接暴露端口到宿主机，所以你从外部无法直接访问它。</li><li><strong><code>k3d-my-cluster-serverlb</code> 容器</strong>: 这是一个基于 NGINX 的反向代理/负载均衡器。k3d 启动它，并让它监听宿主机的一个端口，然后将流量转发给后端的 K3s Server 容器。</li></ol></li><li><p>你的 kubectl 并不是直接和 K3s Server 容器通信。它在和一个作为负载均衡器的代理容器 (k3d-my-cluster-serverlb) 通信。这个代理容器负责将你的请求安全地转发给真正的 K3s Server 容器。39753 是 k3d 为这个负载均衡器随机选择的、暴露在你宿主机上的端口。</p></li><li><p>在k3d里测试时，设置type=LoadBalancer时没有用，即使设置k3d cluster create my-cluster -p &quot;8080:80@loadbalancer&quot;，需要映射type=NodePort 的端口，如8080:30080</p></li><li><p>在用k3d做测试时，集群节点 &quot;看&quot; 不到你本地机器上的 Docker 镜像</p><blockquote><ol><li><p>使用 <code>k3d image import</code> 命令</p><p><code>k</code>3d image import springboot3:v1.0.10 -c my-cluster``</p></li><li><p><strong>修改你的 Deployment YAML 文件</strong></p><p>imagePullPolicy: IfNotPresent # &lt;-- 关键！添加这一行</p></li></ol><p>在生产环境或更复杂的开发环境中，最佳实践是搭建一个镜像仓库（Registry），比如 Harbor、Nexus，或者直接使用 Docker Hub、阿里云 ACR 等。</p></blockquote></li></ul><h2 id="label" tabindex="-1"><a class="header-anchor" href="#label"><span>Label</span></a></h2><ul><li><p>Labels are key/value pairs. Valid label keys have two segments: an optional prefix and name, separated by a slash (/).</p><blockquote><p>Valid label value:</p><ul><li>must be 63 characters or less (can be empty),</li><li>unless empty, must begin and end with an alphanumeric character (<code>[a-z0-9A-Z]</code>),</li><li>could contain dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>), and alphanumerics between.</li></ul></blockquote></li><li><p>The API currently supports two types of selectors: equality-based and set-based.</p></li><li><p>If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler, kube-controller-manager, kube-apiserver, kubectl, or other third-party automation) which add labels to end-user objects must specify a prefix.</p></li><li><p>the comma separator acts as a logical AND (&amp;&amp;) operator.</p></li><li><p>selector: { component: redis } 是旧版的、简洁的写法。</p><p>selector: { matchLabels: { component: redis } } 是新版的、更结构化、更推荐的写法。</p><p>Kubernetes API 在处理第一种写法时，会自动将其理解为第二种写法。</p></li><li><p>Newer resources, such as Job, Deployment, ReplicaSet, and DaemonSet, support set-based requirements as well.</p><blockquote><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">component</span><span class="token punctuation">:</span> redis</span>
<span class="line">  <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token punctuation">{</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> tier<span class="token punctuation">,</span> <span class="token key atrule">operator</span><span class="token punctuation">:</span> In<span class="token punctuation">,</span> <span class="token key atrule">values</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>cache<span class="token punctuation">]</span> <span class="token punctuation">}</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token punctuation">{</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> environment<span class="token punctuation">,</span> <span class="token key atrule">operator</span><span class="token punctuation">:</span> NotIn<span class="token punctuation">,</span> <span class="token key atrule">values</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>dev<span class="token punctuation">]</span> <span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>set-based requirements 应用用引号包起来</p><blockquote><p>kubectl get pods -l &#39;environment in (production),tier in (frontend)&#39;</p></blockquote></li><li><p>kubectl get pods -l environment=production,tier=frontend</p></li><li><p>kubectl get pods -Lapp -Ltier -Lrole</p><blockquote><p>‘-L’ 参数不是过滤作用，而是在最终的查询结果中以列的形式显示</p></blockquote></li><li><p>更新label</p><blockquote><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl label pods <span class="token parameter variable">-l</span> <span class="token assign-left variable">app</span><span class="token operator">=</span>nginx <span class="token assign-left variable">tier</span><span class="token operator">=</span>fe</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>This first filters all pods with the label &quot;app=nginx&quot;, and then labels them with the &quot;tier=fe&quot;。除了用-l app=nginx标签来过滤，还可以用pod的名字来过滤需要操作的pods</p><p>默认情况下，当已经存在tier标签时，不会更新成功。可以加入<code>kubectl label --overwrite pods</code>这个参数</p></blockquote></li></ul><h2 id="namespace" tabindex="-1"><a class="header-anchor" href="#namespace"><span>Namespace</span></a></h2><ul><li><p>Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.)</p></li><li><p>For a production cluster, consider not using the default namespace. Instead, make other namespaces and use those.</p></li><li><p>Kubernetes starts with four initial namespaces:</p><blockquote><ol><li>default</li><li>kube-node-lease</li><li>kube-public</li><li>kube-system</li></ol></blockquote></li><li><p>Avoid creating namespaces with the prefix kube-, since it is reserved for Kubernetes system namespaces.</p></li><li><p>kubectl get namespace</p></li><li><p>To set the namespace for a current request, use the --namespace flag.</p><blockquote><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl run nginx <span class="token parameter variable">--image</span><span class="token operator">=</span>nginx <span class="token parameter variable">--namespace</span><span class="token operator">=</span><span class="token punctuation">[</span>insert-namespace-name-here<span class="token punctuation">]</span></span>
<span class="line">kubectl get pods <span class="token parameter variable">--namespace</span><span class="token operator">=</span><span class="token punctuation">[</span>insert-namespace-name-here<span class="token punctuation">]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>You can permanently save the namespace for all subsequent kubectl commands in that context.</p><blockquote><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl config set-context <span class="token parameter variable">--current</span> <span class="token parameter variable">--namespace</span><span class="token operator">=</span><span class="token punctuation">[</span>insert-namespace-name-here<span class="token punctuation">]</span></span>
<span class="line"><span class="token comment"># Validate it</span></span>
<span class="line">kubectl config view <span class="token parameter variable">--minify</span> <span class="token operator">|</span> <span class="token function">grep</span> namespace:</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>Not all objects are in a namespace</p><blockquote><p>However namespace resources are not themselves in a namespace. And low-level resources, such as nodes and persistentVolumes, are not in any namespace.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># In a namespace</span></span>
<span class="line">kubectl api-resources <span class="token parameter variable">--namespaced</span><span class="token operator">=</span>true</span>
<span class="line"></span>
<span class="line"><span class="token comment"># Not in a namespace</span></span>
<span class="line">kubectl api-resources <span class="token parameter variable">--namespaced</span><span class="token operator">=</span>false</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>The Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on all namespaces. The value of the label is the namespace name</p><blockquote><p>kubectl describe namespaces kube-system</p></blockquote></li><li><p>The keys and the values in the map must be strings. In other words, you cannot use numeric, boolean, list or other types for either the keys or the values.</p></li></ul><h2 id="annotations" tabindex="-1"><a class="header-anchor" href="#annotations"><span>Annotations</span></a></h2><ul><li><p>Annotations are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (/).</p><blockquote><p>The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (<code>[a-z0-9A-Z]</code>) with dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>), and alphanumerics between.</p></blockquote></li><li><p>Shared labels and annotations share a common prefix: app.kubernetes.io. Labels without a prefix are private to users. The shared prefix ensures that shared labels do not interfere with custom user labels.</p><blockquote><p><strong>Shared Labels</strong> 是一套 <strong>官方推荐的、标准化的标签</strong>。它们使用 <code>app.kubernetes.io/</code> 这个统一的前缀，目的是为了让不同的工具、团队和用户能够用一种通用的方式来描述和识别在 Kubernetes 中运行的应用程序</p></blockquote></li><li><p>The metadata is organized around the concept of an application. Kubernetes is not a platform as a service (PaaS) and doesn&#39;t have or enforce a formal notion of an application. Instead, applications are informal and described with metadata. The definition of what an application contains is loose.</p></li></ul><h2 id="field-selectors" tabindex="-1"><a class="header-anchor" href="#field-selectors"><span>Field selectors</span></a></h2><ul><li><p>Field selectors are essentially resource filters. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the kubectl queries kubectl get pods and kubectl get pods --field-selector &quot;&quot; equivalent.</p></li><li><p>You can use the =, ==, and != operators with field selectors (= and == mean the same thing).</p><blockquote><p>kubectl get services --all-namespaces --field-selector metadata.namespace!=default</p></blockquote></li><li><p>As with label and other selectors, field selectors can be chained together as a comma-separated list.</p><blockquote><p>kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always</p></blockquote></li></ul><h2 id="finalizer" tabindex="-1"><a class="header-anchor" href="#finalizer"><span>Finalizer</span></a></h2><ul><li><p>Finalizer 是一个存在于资源对象 metadata 中的字符串列表。</p></li><li><p>这个 Finalizer 确保了当你删除这个 Service 时，Kubernetes 会先调用云平台的 API 去删除那个真实的、会产生费用的负载均衡器，然后再删除 Service 对象本身。如果没有这个机制，你可能会留下很多无人管理的“僵尸”云资源。</p><blockquote><p><strong>Finalizer</strong>: <code>service.kubernetes.io/load-balancer-cleanup</code> (在一些云厂商的实现中)</p></blockquote></li><li><p>为什么资源会卡在 Terminating 状态？🚨 这是你在实践中一定会遇到的经典问题。当一个资源长时间处于 Terminating 状态时，几乎 100% 是 Finalizer 导致的。</p><blockquote><p><strong>原因</strong>：负责清理并移除那个 Finalizer 的控制器<strong>无法完成它的工作</strong>。</p></blockquote></li><li><p>Finalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.</p><blockquote><p><strong>Marked for deletion (标记为删除)</strong>: 资源有了 <code>deletionTimestamp</code>，处于 <code>Terminating</code> 状态。它对外已经“死亡”（比如 Pod 不再接收流量），但它的“尸体”（在 etcd 中的记录）还在。</p><p><strong>Fully deleted (彻底删除)</strong>: 资源的记录从 etcd 中被彻底抹除，它不复存在了。</p><p><strong>Specific conditions are met (特定条件被满足)</strong>: 这个“特定条件”非常明确，<strong>指的就是</strong> <code>metadata.finalizers</code> <strong>列表变为空</strong>。</p><p>那么谁来清空这个列表呢？答案是<strong>控制器 (Controller)</strong>。</p><ul><li>每个 Finalizer 字符串都对应一个正在运行的控制器。</li><li>这个控制器一直在监控，当它发现自己负责的资源出现了 <code>deletionTimestamp</code> 时，它就知道该干活了（执行清理任务）。</li><li>清理任务完成后（比如云硬盘被删了，数据库备份好了），控制器就会发起一个 API 请求，把自己负责的那个 Finalizer 字符串从列表中<strong>移除</strong>。</li><li>当所有控制器都完成了自己的任务，<code>finalizers</code> 列表就变空了。</li></ul><p>它实际上是一个<strong>字符串</strong>。这些字符串存在于一个列表里，位置在 <code>metadata.finalizers</code></p><p>它们像带有名空间的键一样，是独一无二的标识符</p><p>简单来说：你可以把它理解为“带有唯一前缀的特殊标签”。</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">finalizers</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token punctuation">-</span> kubernetes.io/pv<span class="token punctuation">-</span>protection  <span class="token comment"># 一个遵循 &quot;namespaced key&quot; 格式的字符串</span></span>
<span class="line">  <span class="token punctuation">-</span> another.tool.com/do<span class="token punctuation">-</span>backup    <span class="token comment"># 另一个遵循同样格式的字符串</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>Custom finalizer names must be publicly qualified finalizer names, such as example.com/finalizer-name. Kubernetes enforces this format; the API server rejects writes to objects where the change does not use qualified finalizer names for any custom finalizer.</p></li><li><p>Dependent objects also have an ownerReferences.blockOwnerDeletion field that takes a boolean value and controls whether specific dependents can block garbage collection from deleting their owner object. Kubernetes automatically sets this field to true if a controller (for example, the Deployment controller) sets the value of the metadata.ownerReferences field. You can also set the value of the blockOwnerDeletion field manually to control which dependents block garbage collection.</p><blockquote><p>关系链: Deployment -&gt; ReplicaSet -&gt; Pod。</p><p>删除链: 删除 Deployment -&gt; 删除 ReplicaSet -&gt; 删除 Pod。</p><p>blockOwnerDeletion: true: 是一个 “刹车”。Dependent 对象对 Owner 说：“别删我老板，除非我先走！”</p><p>kubectl delete deployment 触发的是一个“有序解散”，而非“斩首行动【直接删除deployment】”</p></blockquote></li><li><p>In foreground deletion, it adds the foreground finalizer so that the controller must delete dependent resources that also have ownerReferences.blockOwnerDeletion=true before it deletes the owner.</p></li><li><p>kubectl delete deployment my-app --cascade=orphan</p><blockquote><p><strong>会发生什么？</strong></p><ol><li><code>Deployment</code> <strong>对象被立即删除</strong>：<code>my-app</code> 这个 <code>Deployment</code> 资源瞬间就消失了。</li><li><code>ReplicaSet</code> <strong>和</strong> <code>Pod</code> <strong>完好无损</strong>：你会惊讶地发现，<code>ReplicaSet</code> 和所有的 <code>Pod</code> 依然在运行！</li><li><code>ReplicaSet</code> <strong>成为孤儿</strong>：如果你查看那个幸存的 <code>ReplicaSet</code> 的 YAML (<code>kubectl get rs [rs-name] -o yaml</code>)，你会发现它 <code>metadata</code> 里的 <code>ownerReferences</code> 字段<strong>已经不见了</strong>。它不再属于任何人，变成了一个独立的、没人管理的 <code>ReplicaSet</code>。</li></ol></blockquote></li></ul><h2 id="k3d-运行-plg-栈-promtail-loki-grafana" tabindex="-1"><a class="header-anchor" href="#k3d-运行-plg-栈-promtail-loki-grafana"><span>K3d 运行（ <strong>PLG 栈</strong>: Promtail + Loki + Grafana）</span></a></h2><ol><li><p>准备工作: 安装helm</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token function">sudo</span> snap <span class="token function">install</span> helm <span class="token parameter variable">--classic</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div></li><li><p>第一步：理解架构</p><p>在动手之前，先看一眼我们将要搭建的架构：</p><ol><li><strong>Promtail (搬运工)</strong>：它以 <strong>DaemonSet</strong> 的形式运行，意味着你 k3d 的每一个“节点容器”里都会自动运行一个 Promtail。它负责去 <code>/var/log/pods</code>（就是你刚才进去的那个目录）抓取日志，并打上标签（Pod名、Namespace等）。</li><li><strong>Loki (仓库)</strong>：它是核心存储，负责接收 Promtail 发来的流，并进行压缩存储。</li><li><strong>Grafana (仪表盘)</strong>：可视化的 Web 界面，我们在这里查询和看图。</li></ol></li><li><p>第二步：使用 Helm 安装 Loki-Stack 为了简化流程，我们使用官方的 loki-stack Chart，它会把上面三个组件打包一起装好。</p><p><strong>1. 添加 Grafana 仓库</strong> 在你的终端执行：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">helm repo <span class="token function">add</span> grafana https://grafana.github.io/helm-charts</span>
<span class="line">helm repo update</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>2. 创建一个独立的 Namespace</strong> 把监控相关的资源隔离出来是个好习惯：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl create namespace logging</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>3. 安装 PLG 栈</strong> 执行下面的命令。 <em>注意：我们显式开启了 Grafana，因为这个 Chart 默认可能不安装它。</em></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">helm upgrade <span class="token parameter variable">--install</span> loki grafana/loki-stack <span class="token punctuation">\</span></span>
<span class="line">  <span class="token parameter variable">--namespace</span> logging <span class="token punctuation">\</span></span>
<span class="line">  <span class="token parameter variable">--set</span> <span class="token assign-left variable">grafana.enabled</span><span class="token operator">=</span>true <span class="token punctuation">\</span></span>
<span class="line">  <span class="token parameter variable">--set</span> <span class="token assign-left variable">promtail.enabled</span><span class="token operator">=</span>true <span class="token punctuation">\</span></span>
<span class="line">  <span class="token parameter variable">--set</span> <span class="token assign-left variable">loki.persistence.enabled</span><span class="token operator">=</span>false</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><em>(注：为了 k3d 实验方便，我关闭了 persistence 持久化存储。如果你重启 k3d 集群，Loki 里的旧日志会丢失，但对实验来说足够了且更轻量。)</em></p></li><li><p>第三步：验证安装</p><p>等待几分钟，查看 Pod 是否都跑起来了：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pods <span class="token parameter variable">-n</span> logging</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>你应该能看到类似这样的列表：</p><ul><li><code>loki-0</code> (或者 <code>loki-promtail-...</code>)</li><li><code>loki-grafana-...</code></li></ul><p>如果状态都是 <code>Running</code>，恭喜你，系统已经由守转攻了！</p></li><li><p>第四步：获取 Grafana 密码</p><p>Grafana 默认生成的 <code>admin</code> 密码保存在 K8s 的 Secret 里。我们需要把它解密出来。</p><p>执行这条命令（这是运维人员必备的“黑客”技能）：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 获取 secret，提取 admin-password 字段，并 base64 解码</span></span>
<span class="line">kubectl get secret <span class="token parameter variable">--namespace</span> logging loki-grafana <span class="token parameter variable">-o</span> <span class="token assign-left variable">jsonpath</span><span class="token operator">=</span><span class="token string">&quot;{.data.admin-password}&quot;</span> <span class="token operator">|</span> base64 <span class="token parameter variable">--decode</span> <span class="token punctuation">;</span> <span class="token builtin class-name">echo</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>终端会输出一串字符，<strong>复制它</strong>，这就是你的登录密码。</p></li><li><p>第五步：访问 Grafana (Port Forwarding)</p><p>因为你在 k3d 里面，外部无法直接访问 ClusterIP。我们需要用 <code>port-forward</code> 把 Grafana 的端口映射到你物理机的 <code>localhost</code></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 将 k8s 内部的 80 端口映射到你电脑的 3000 端口</span></span>
<span class="line">kubectl port-forward <span class="token parameter variable">--namespace</span> logging service/loki-grafana <span class="token number">3000</span>:80</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p><em>注意：这个命令会占用终端，不要关闭它。</em></p></li><li><p>第六步：见证奇迹的时刻</p><ol><li>打开浏览器，访问：<code>http://localhost:3000</code></li><li><strong>用户名</strong>：<code>admin</code></li><li><strong>密码</strong>：刚才复制的那串字符。</li><li>进入首页后，点击左侧菜单的 <strong>&quot;Explore&quot; (指南针图标)</strong>。</li><li>在顶部的下拉框中，确保选择了 <strong>&quot;Loki&quot;</strong> 作为数据源。</li></ol><p><strong>现在，我们来查你的 Spring Boot 日志！</strong></p></li><li><p>高级查询</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment">## ~ 为正则</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">|</span> json <span class="token operator">|</span> <span class="token assign-left variable">level</span><span class="token operator">=</span><span class="token string">&quot;ERROR&quot;</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">|</span> json <span class="token operator">|</span> message <span class="token operator">!=</span> <span class="token string">&quot;exception&quot;</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">|</span> json <span class="token operator">|</span> message <span class="token operator">!</span>~ <span class="token string">&quot;(?i).*exception.*&quot;</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">|</span> json <span class="token operator">|</span> message <span class="token operator">=</span> <span class="token string">&quot;exception&quot;</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">|</span> json <span class="token operator">|</span> message <span class="token operator">=~</span> <span class="token string">&quot;(?i).*exception.*&quot;</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">|</span><span class="token operator">=</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">|</span>~ </span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">!=</span></span>
<span class="line"><span class="token punctuation">{</span>namespace<span class="token operator">=</span><span class="token string">&quot;default&quot;</span><span class="token punctuation">}</span> <span class="token operator">!</span>~</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol><h2 id="其它" tabindex="-1"><a class="header-anchor" href="#其它"><span>其它</span></a></h2><ul><li><p><code>kind: Ingress</code> 会暴露一个 IP 地址吗？</p><blockquote><p>不会，<code>Ingress</code> 资源本身不会。一个 <code>kind: Ingress</code> 的 YAML 文件，它仅仅是<strong>一套规则</strong>的集合，就像一张写着“<code>a.com</code> 的流量请走A门，<code>b.com</code> 的流量请走B门”的说明书。这张说明书本身并没有地址，它需要被人（也就是 <code>Ingress Controller</code>）去阅读和执行。</p><p>真正暴露 IP 地址的，是 <code>Ingress Controller</code> 的 <code>Service</code>！</p><p>回顾一下 <code>Ingress Controller</code> 是如何被安装的：</p><ol><li><strong>Ingress Controller 是一个需要被</strong><code>安装</code><strong>到集群中的应用</strong>，它不是 K8s 自带的。</li><li><strong>安装的本质</strong>是应用一套包含了 <code>Deployment</code>、<code>Service</code>、<code>RBAC</code> 等资源的 YAML 文件。</li><li><strong>K3s 用户最幸福</strong>，因为 K3s 已经内置了 <strong>Traefik</strong>，无需手动安装。你只需要直接在 <code>Ingress</code> 中使用 <code>ingressClassName: &quot;traefik&quot;</code> 即可。</li><li>在<strong>标准 K8s 环境</strong>中，最常用的选择是 <strong>NGINX Ingress Controller</strong>，可以通过官方 <code>kubectl apply</code> 命令或 <code>Helm</code> Chart 来安装。</li><li><strong><code>IngressClass</code> 资源是在安装 Controller 的过程中被自动创建的</strong>。它像一个“告示牌”，告诉整个集群：“嘿，我这里有一个名为 <code>nginx</code> (或 <code>traefik</code>) 的 Controller，你们谁需要处理 Ingress 规则，就通过 <code>ingressClassName</code> 来找我！”</li></ol><p>现在，<code>Ingress</code> (规则)、<code>Ingress Controller</code> (执行者) 和 <code>IngressClass</code> (联系方式) 这三者之</p><p>它通常包含一个 <code>Deployment</code> (运行 Controller 的 Pods) 和一个 <code>Service</code> (把这些 Pods 暴露出去)。这个 <code>Service</code> 的类型通常是 <code>LoadBalancer</code> 或 <code>NodePort</code>。</p><ul><li>当 <code>Service</code> 的类型是 <code>LoadBalancer</code> 时，云服务商会为<strong>这个 Service</strong>分配一个<strong>公网 IP 地址</strong>。</li><li>当 <code>Service</code> 的类型是 <code>NodePort</code> 时，你可以通过<strong>任何一个节点的 IP</strong> + <code>NodePort</code> 端口来访问。</li></ul><p>那个宝贵的、唯一的、对外服务的公网 IP 地址，是属于 <strong>Ingress Controller 的 Service</strong> 的，而不是属于你创建的某一个 <code>Ingress</code> 规则对象的。</p><p>你可以通过以下命令查看到这个 IP 地址：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 查看 Ingress Controller 的 Service</span></span>
<span class="line"><span class="token comment"># 注意命名空间，如果你是用 helm 装的 nginx-ingress，那就在 ingress-nginx 命名空间</span></span>
<span class="line"><span class="token comment"># 如果是 k3s 自带的 traefik，那就在 kube-system</span></span>
<span class="line">kubectl get <span class="token function">service</span> <span class="token parameter variable">-n</span> ingress-nginx </span>
<span class="line"></span>
<span class="line"><span class="token comment"># 你会看到类似这样的输出</span></span>
<span class="line"><span class="token comment"># NAME                       TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                      AGE</span></span>
<span class="line"><span class="token comment"># ingress-nginx-controller   LoadBalancer   10.43.151.108   203.0.113.55     80:32168/TCP,443:30256/TCP   10m</span></span>
<span class="line"><span class="token comment">#                                                          ^^^^^^^^^^^^</span></span>
<span class="line"><span class="token comment"># 就是这个 IP！</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>故障排查三步法</p><blockquote><p>第 1 步：确认 Pods 是否健康运行</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get pods <span class="token parameter variable">-o</span> wide</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>第 2 步：确认 Service 是否正确关联了 Pods</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl describe <span class="token function">service</span> nginx-service</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>正确的状态</strong>: <code>Endpoints</code> 后面应该列出了一个或多个 IP 地址和端口，这些 IP 应该与你在上一步中看到的 Pod IP 完全一致。<code>Endpoints: 10.42.0.5:80,10.42.0.6:80</code></p><p>第 3 步：确认 k3d 节点的端口映射 (最可能的原因)</p></blockquote></li><li><p>最快、最直接的绕过网络问题的方法，我们在上次讨论中也提到了。它不依赖任何端口映射，而是直接在你的电脑和 Service 之间建立一条隧道。</p><blockquote><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 在新终端中运行</span></span>
<span class="line">kubectl port-forward service/nginx-service <span class="token number">8080</span>:80</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>MetalLB (强烈推荐) 这是在自建集群（Bare-Metal）中实现 type: LoadBalancer 的最佳实践方案。MetalLB 是一个开源项目，它能为你的集群模拟云服务商的负载均衡器功能。</p></li><li><p>使用 kubectl explain 命令：这是一个非常有用的命令，可以帮助你了解任何 Kubernetes 资源的结构和字段。例如，如果你想知道 Deployment 的 apiVersion</p><blockquote><p>如</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl explain Deployment</span>
<span class="line"><span class="token comment"># GROUP:      apps</span></span>
<span class="line"><span class="token comment"># KIND:       Deployment</span></span>
<span class="line"><span class="token comment"># VERSION:    v1</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>所以定义Deployment时为：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code class="language-text"><span class="line">apiVersion: apps/v1</span>
<span class="line">kind: Deployment</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div></blockquote></li><li><p>一个完整的应用[系统]，一般只有一个type为loadbalancer的service?</p><blockquote><ul><li><p>对于一个完整的、现代化的应用系统（特别是基于微服务架构的 Web 应用），通常最佳实践就是只使用一个 Type=LoadBalancer 的 Service。标准的应用暴露架构：“LoadBalancer + Ingress Controller”</p></li><li><p>如果你的应用系统包含一些非 HTTP/HTTPS 的服务，比如：</p><ul><li>一个需要直接暴露给外部客户端的 数据库 (如 PostgreSQL)。</li><li>一个 MQTT 消息代理服务。</li><li>一个 SFTP 文件服务。</li></ul><p>这些服务工作在 TCP/UDP 层，Ingress Controller（通常为 HTTP 设计）无法处理。在这种情况下，为这些特定的服务再额外创建一个独立的 Type=LoadBalancer Service 是完全合理的。</p></li></ul></blockquote></li><li><p>In case of a Node, it is implicitly assumed that an instance using the same name will have the same state (e.g.network settings, root disk contents) and attributes like node labels.</p><blockquote><p>这里的instance是指虚拟机或者物理机。 Kubernetes 认‘名’不认‘人’。它把节点名称当作身份证号。如果一个新人拿了旧人的身份证号来报到，系统会把他当成旧人，但这个新人的能力和背景（磁盘内容、硬件属性）是全新的。这种身份与实际能力的不匹配，正是很多诡异问题的根源。请务必确保在替换节点时，先‘注销’旧的身份信息（kubectl delete node），再让新人用自己的身份注册。</p></blockquote></li><li><p>Register the node with the given list of taints</p><blockquote><p>可以把 Taint (污点) 想象成节点（Node）上的一个“排斥标签”或者“谢绝入内”的牌子。 一旦一个节点被打上了某个 Taint，Kubernetes 的调度器（Scheduler）默认就不会把任何 Pod 调度到这个节点上。这就好像一个房间门口挂着“请勿打扰”的牌子，正常情况下，没有人会进去。</p></blockquote></li></ul><h2 id="container" tabindex="-1"><a class="header-anchor" href="#container"><span>Container</span></a></h2><h3 id="container-hooks" tabindex="-1"><a class="header-anchor" href="#container-hooks"><span>Container hooks</span></a></h3><ul><li><p>PostStart</p><p>However, if the <code>PostStart</code> hook takes too long to execute or if it hangs, it can prevent the container from transitioning to a <code>running</code> state.</p></li><li><p>PreStop</p><p><code>PreStop</code> hooks are not executed asynchronously from the signal to stop the Container; the hook must complete its execution before the TERM signal can be sent.</p></li></ul><blockquote><p><strong>Hook 失败的影响</strong>：</p><ul><li><code>postStart</code> Hook 失败：如果 <code>postStart</code> Hook 执行失败，容器将无法进入 <code>Running</code> 状态，<code>kubelet</code> 会杀死并尝试重启这个容器，导致 Pod 进入 <code>CrashLoopBackOff</code> 状态。</li><li><code>preStop</code> Hook 失败：<code>preStop</code> Hook 的失败不会阻止容器的终止。Kubernetes 在尝试执行 <code>preStop</code> Hook 后（无论成功与否），仍然会向容器的主进程发送 <code>TERM</code> 信号。</li></ul><p>其实也可以总结为这两个hook只要有一个失败，容器都会被killed</p></blockquote><h3 id="hook-handler-implementations" tabindex="-1"><a class="header-anchor" href="#hook-handler-implementations"><span>Hook handler implementations</span></a></h3><ul><li>Exec</li></ul><ol><li><strong>执行环境</strong>：<code>Exec</code> 类型的 Hook Handler <strong>完全在容器内部执行</strong>。它和你在容器启动后使用 <code>kubectl exec</code> 或 <code>docker exec</code> 进入容器执行命令的环境是一模一样的。</li><li><strong>资源归属</strong>：因此，这个脚本或命令所消耗的 <strong>所有资源（CPU、内存等）都计算在该容器的账上</strong>。它会受到为该容器配置的 <code>resources.limits</code> 和 <code>resources.requests</code> 的约束。</li></ol><ul><li>HTTP</li><li>Sleep</li></ul><blockquote><p><code>httpGet</code>, <code>tcpSocket</code> (<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.31/#lifecyclehandler-v1-core" target="_blank" rel="noopener noreferrer">deprecated</a>) and <code>sleep</code> are executed by the kubelet process (请求的接收和处理发生在容器内，因此<strong>处理该请求所消耗的资源归属于容器</strong>), and <code>exec</code> is executed in the container.</p></blockquote><p>配置preStop sample:</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx</span>
<span class="line">    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span><span class="token number">1.25</span></span>
<span class="line">    <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line">    <span class="token key atrule">lifecycle</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">preStop</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token comment">## 或者 httpGet:</span></span>
<span class="line">        <span class="token key atrule">exec</span><span class="token punctuation">:</span>   </span>
<span class="line">        <span class="token punctuation">...</span>.</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="hook-delivery-guarantees" tabindex="-1"><a class="header-anchor" href="#hook-delivery-guarantees"><span>Hook delivery guarantees</span></a></h3><p>这些hook可能会运行多次。这个概念在分布式系统中非常常见，被称为 <strong>“至少一次 (At-Least-Once)”</strong> 投递语义</p><h3 id="debugging-hook-handlers" tabindex="-1"><a class="header-anchor" href="#debugging-hook-handlers"><span>Debugging Hook handlers</span></a></h3><p>这些Hook handlers如果执行失败，可以执行像类似的语句来查看日志<code>kubectl describe pod lifecycle-demo</code></p><h2 id="workloads" tabindex="-1"><a class="header-anchor" href="#workloads"><span>Workloads</span></a></h2><p>A workload is an application running on Kubernetes. Whether your workload is a single component or several that work together, on Kubernetes you run it inside a set of <a href="https://kubernetes.io/docs/concepts/workloads/pods/" target="_blank" rel="noopener noreferrer"><em>pods</em></a>. In Kubernetes, a Pod represents a set of running <a href="https://kubernetes.io/docs/concepts/containers/" target="_blank" rel="noopener noreferrer">containers</a> on your cluster.</p><p>several <strong>built-in workload resources</strong>:</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener noreferrer">Deployment</a> and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/" target="_blank" rel="noopener noreferrer">ReplicaSet</a> (replacing the legacy resource <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-replication-controller" target="_blank" rel="noopener noreferrer">ReplicationController</a>).</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/" target="_blank" rel="noopener noreferrer">StatefulSet</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/" target="_blank" rel="noopener noreferrer">DaemonSet</a> defines Pods that provide facilities that are local to nodes.</li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" target="_blank" rel="noopener noreferrer">Job</a> and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/" target="_blank" rel="noopener noreferrer">CronJob</a> provide different ways to define tasks that run to completion and then stop. You can use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" target="_blank" rel="noopener noreferrer">Job</a> to define a task that runs to completion, just once. You can use a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/" target="_blank" rel="noopener noreferrer">CronJob</a> to run the same Job multiple times according a schedule.</li></ul><p>In the wider Kubernetes ecosystem, you can find third-party workload resources that provide additional behaviors. Using a <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/" target="_blank" rel="noopener noreferrer">custom resource definition</a>, you can add in a third-party workload resource if you want a specific behavior that&#39;s not part of Kubernetes&#39; core.</p><h3 id="pods" tabindex="-1"><a class="header-anchor" href="#pods"><span>Pods</span></a></h3><p><em>Pods</em> are the smallest deployable units of computing that you can create and manage in Kubernetes. 你可以把一个 Pod 想象成一台独立的“逻辑主机”或虚拟机。这台“主机”有自己唯一的 IP 地址。</p><p>Pods in a Kubernetes cluster are used in two main ways:</p><ul><li><p><strong>Pods that run a single container</strong>.</p></li><li><p><strong>Pods that run multiple containers that need to work together</strong>.</p><p>You should use this pattern only in specific instances in which your containers are tightly coupled. You don&#39;t need to run multiple containers to provide replication (for resilience or capacity);</p></li></ul><h4 id="using-pods" tabindex="-1"><a class="header-anchor" href="#using-pods"><span>Using Pods</span></a></h4><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx</span>
<span class="line">    <span class="token key atrule">image</span><span class="token punctuation">:</span> nginx<span class="token punctuation">:</span>1.14.2</span>
<span class="line">    <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Pods are generally not created directly and are created using workload resources. Instead, create them using workload resources such as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener noreferrer">Deployment</a> or <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" target="_blank" rel="noopener noreferrer">Job</a>. If your Pods need to track state, consider the <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/" target="_blank" rel="noopener noreferrer">StatefulSet</a> resource.</p><h4 id="working-with-pods" tabindex="-1"><a class="header-anchor" href="#working-with-pods"><span>Working with Pods</span></a></h4><p>Pods are designed as relatively ephemeral, disposable entities. The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is <em>evicted</em> for lack of resources, or the node fails.</p><blockquote><p>Note:</p><p>Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for running container(s). A Pod persists until it is deleted.</p></blockquote><p>The name of a Pod must be a valid <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names" target="_blank" rel="noopener noreferrer">DNS subdomain</a> value, but this can produce unexpected results for the Pod hostname. For best compatibility, the name should follow the more restrictive rules for a <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-label-names" target="_blank" rel="noopener noreferrer">DNS label</a>.</p><h5 id="pod-os" tabindex="-1"><a class="header-anchor" href="#pod-os"><span>Pod OS</span></a></h5><ol><li><p>对 <code>.spec.os.name</code> 字段的理解</p><p>在 Pod 的 YAML 中设置 <code>.spec.os.name</code> 字段<strong>并不会影响 <code>kube-scheduler</code> (调度器) 的实际调度决策</strong>。它的主要作用有两点：</p><ol><li><strong>声明与识别 (Declaration &amp; Identification)</strong>: 它是一个明确的元数据字段，用来<strong>声明</strong>这个 Pod 内的容器是为哪个操作系统构建的（目前是 <code>linux</code> 或 <code>windows</code>）。这使得集群中的其他组件或工具（比如监控系统、安全策略工具）能够轻松识别 Pod 的操作系统类型。</li><li><strong>策略应用 (Policy Enforcement)</strong>: 如文档中提到的，<code>Pod Security Standards</code> (Pod 安全标准) 会利用这个字段。例如，某些安全策略只适用于 Linux 环境（比如与 <code>seccomp</code> 或 <code>AppArmor</code> 相关的策略），在 Windows 节点上强制执行这些策略是没有意义的。通过读取 <code>.spec.os.name</code>, 系统可以智能地避免在不相关的操作系统上应用这些策略。</li><li><strong>面向未来 (Future-proofing)</strong>: 社区可能在未来的版本中赋予这个字段更多的功能，甚至可能直接影响调度。但就目前而言，它更多的是一个描述性、供其他组件消费的字段。</li></ol></li><li><p><code>kubernetes.io/os</code> Label 打在哪个资源上？</p><p><code>kubernetes.io/os</code> 是一个 <strong>Node Label</strong> (节点标签)。它被打在 <strong>Node (节点)</strong> 资源上。你可以通过以下命令来查看你集群中所有节点的标签:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl get nodes --show-labels</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><blockquote><p><code>kubernetes.io/os</code> 这个标签主要是<strong>由 <code>kubelet</code> 自动添加</strong>的。</p><p>kubectl describe node [your-node-name] 看到相应的System Info</p></blockquote></li><li><p>到底是什么决定了 Pod 分配到对应的操作系统？</p><p>真正决定 Pod 被调度到特定操作系统节点上的机制，是 <strong>Pod Spec (Pod 规约) 中的调度约束</strong>与 <strong>Node (节点) 上的标签</strong>之间的匹配。</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>windows<span class="token punctuation">-</span>pod</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token comment"># 步骤 1: 声明 Pod 的操作系统类型</span></span>
<span class="line">  <span class="token comment"># 这本身不影响调度，但是一个好习惯，也为了符合安全策略等。</span></span>
<span class="line">  <span class="token key atrule">os</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">name</span><span class="token punctuation">:</span> windows</span>
<span class="line"> <span class="token punctuation">...</span></span>
<span class="line">  <span class="token key atrule">nodeSelector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">kubernetes.io/os</span><span class="token punctuation">:</span> windows</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ol><p>为了方便你记忆，我们可以做一个简单的类比：</p><table><thead><tr><th>字段/机制</th><th>功能</th><th>好比是...</th></tr></thead><tbody><tr><td><strong><code>.spec.os.name</code></strong></td><td><strong>声明 (Declaration)</strong></td><td>包裹上的“内含物品”清单，写着“Windows 软件”。</td></tr><tr><td><strong>Node Label <code>kubernetes.io/os</code></strong></td><td><strong>属性 (Attribute)</strong></td><td>每个房门上的标签，写着“本户使用 Windows 系统”或“本户使用 Linux 系统”。</td></tr><tr><td><strong>Pod <code>nodeSelector</code></strong></td><td><strong>指令 (Instruction)</strong></td><td>快递单上的“投递要求”，明确指示：“必须投递到使用 Windows 系统的住户”。</td></tr></tbody></table><h5 id="pods-and-controllers" tabindex="-1"><a class="header-anchor" href="#pods-and-controllers"><span>Pods and controllers</span></a></h5><p>You can use workload resources to create and manage multiple Pods for you.</p><p>Here are some examples of workload resources that manage one or more Pods:</p><ul><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener noreferrer">Deployment</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/" target="_blank" rel="noopener noreferrer">StatefulSet</a></li><li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset" target="_blank" rel="noopener noreferrer">DaemonSet</a></li></ul><h5 id="pod-templates" tabindex="-1"><a class="header-anchor" href="#pod-templates"><span>Pod templates</span></a></h5><p>PodTemplates are specifications for creating Pods, and are included in workload resources such as <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener noreferrer">Deployments</a>, <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/" target="_blank" rel="noopener noreferrer">Jobs</a>, and <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/" target="_blank" rel="noopener noreferrer">DaemonSets</a>. The <code>PodTemplate</code> is part of the desired state of whatever workload resource you used to run your app.</p><p>the sample:</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Job</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> hello</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token comment"># This is the pod template</span></span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> hello</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox<span class="token punctuation">:</span><span class="token number">1.28</span></span>
<span class="line">        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sh&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;-c&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;echo &quot;Hello, Kubernetes!&quot; &amp;&amp; sleep 3600&#39;</span><span class="token punctuation">]</span></span>
<span class="line">      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> OnFailure</span>
<span class="line">    <span class="token comment"># The pod template ends here</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="pod-update-and-replacement" tabindex="-1"><a class="header-anchor" href="#pod-update-and-replacement"><span>Pod update and replacement</span></a></h4><p>Kubernetes doesn&#39;t prevent you from managing Pods directly. 虽然 Kubernetes <strong>允许</strong> 你直接操作 Pod，但这通常是一种<strong>反模式（anti-pattern）</strong>，主要用于调试或紧急情况。</p><p>用edit 交互式去更新：<code>kubectl edit pod [pod-name]</code> ；还可以用patch去更新 <code>kubectl patch pod my-test-pod -p &#39;{&quot;spec&quot;:{&quot;activeDeadlineSeconds&quot;:60}}&#39;</code></p><h4 id="resource-sharing-and-communication" tabindex="-1"><a class="header-anchor" href="#resource-sharing-and-communication"><span>Resource sharing and communication</span></a></h4><p>pod内的containers共用一个ip, 不同的container如果想要expose端口，只能是不同的。同一pod里面的container用localhost+端口进行通讯</p><h4 id="static-pods" tabindex="-1"><a class="header-anchor" href="#static-pods"><span>Static Pods</span></a></h4><p>我们平时用 <code>kubectl apply -f my-pod.yaml</code> 创建的 Pod，我们称之为<strong>标准 Pod</strong> 或 <strong>API Server 管理的 Pod</strong>。它们的生命周期完全由 Kubernetes 的控制平面（特别是 API Server）来管理。</p><p>而<strong>静态 Pod</strong>则完全不同。</p><ul><li><strong>定义</strong>：静态 Pod 是直接由特定节点上的 <strong>Kubelet</strong> 守护进程管理的 Pod，它不通过 API Server 进行管理。</li><li><strong>来源</strong>：Kubelet 会监视其所在节点上的一个特定目录（通常是 <code>/etc/kubernetes/manifests</code>）。任何放在这个目录下的标准 Pod 定义 YAML/JSON 文件，都会被 Kubelet 自动识别并创建为静态 Pod。</li><li><strong>生命周期</strong>： <ul><li><strong>创建</strong>：将 Pod 的 YAML 文件放入 Kubelet 的监视目录。</li><li><strong>删除</strong>：从该目录中删除 Pod 的 YAML 文件。</li><li><strong>更新</strong>：修改该目录中的 Pod YAML 文件（Kubelet 会自动停止旧的 Pod，并根据新文件启动新的 Pod）。</li></ul></li></ul><p><strong>镜像 Pod (Mirror Pod)</strong> 现在我们回到了你问题的核心。既然静态 Pod 不受 API Server 管理，那我们执行 kubectl get pods 时，能看到它们吗？如果看不到，那集群管理员就无法感知到这些关键组件的存在，这会给监控和管理带来麻烦。</p><ul><li><strong>定义</strong>：当 Kubelet 在节点上成功创建了一个静态 Pod 后，它会<strong>自动地</strong>、<strong>主动地</strong>在 API Server 上为这个静态 Pod 创建一个对应的、<strong>只读的</strong>对象。这个在 API Server 上的对象就叫做“镜像 Pod”。</li><li><strong>目的</strong>：它的唯一目的就是让这个静态 Pod 在 Kubernetes 的 API 中<strong>可见 (Visible)</strong>。</li></ul><h4 id="pods-with-multiple-containers" tabindex="-1"><a class="header-anchor" href="#pods-with-multiple-containers"><span>Pods with multiple containers</span></a></h4><p>For example, you might have a container that acts as a web server for files in a shared volume, and a separate <a href="https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" rel="noopener noreferrer">sidecar container</a> that updates those files from a remote source, as in the following diagram:</p><p><img src="/assets/image-20250919162927803-7Bs-HsX1.png" alt="image-20250919162927803"></p><h4 id="pod-lifecycle" tabindex="-1"><a class="header-anchor" href="#pod-lifecycle"><span>Pod Lifecycle</span></a></h4><p>Pods follow a defined lifecycle, starting in the <code>Pending</code> <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase" target="_blank" rel="noopener noreferrer">phase</a>, moving through <code>Running</code> if at least one of its primary containers starts OK, and then through either the <code>Succeeded</code> or <code>Failed</code> phases depending on whether any container in the Pod terminated in failure.</p><h5 id="pod-lifetime" tabindex="-1"><a class="header-anchor" href="#pod-lifetime"><span>Pod lifetime</span></a></h5><p>Pods are only <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/" target="_blank" rel="noopener noreferrer">scheduled</a> once in their lifetime; assigning a Pod to a specific node is called <em>binding</em>, and the process of selecting which node to use is called <em>scheduling</em>.</p><p>You can use <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-scheduling-readiness/" target="_blank" rel="noopener noreferrer">Pod Scheduling Readiness</a> to delay scheduling for a Pod until all its <em>scheduling gates</em> are removed. For example, you might want to define a set of Pods but only trigger scheduling once all the Pods have been created.</p><blockquote><p>可以定义不同的调度门，在真的被schedule之前需要相就的controller 被这些调度门都删除</p></blockquote><h5 id="pods-and-fault-recovery" tabindex="-1"><a class="header-anchor" href="#pods-and-fault-recovery"><span>Pods and fault recovery</span></a></h5><p>不会把那个旧的、失败的 Pod 实例（identified by a UID）拿起来，拍拍灰尘，然后放到一个新节点上让它继续运行。我们是直接放弃旧的，然后由像 Deployment 这样的控制器创建一个全新的替代品，这个替代品再由调度器找一个新家。</p><p>node故障 vs Pod 故障：</p><table><thead><tr><th>特性</th><th>场景 1: 节点宕机 (Node Failure)</th><th>场景 2: Pod 故障 (Pod Failure on Healthy Node)</th></tr></thead><tbody><tr><td><strong>主要处理者</strong></td><td><code>kube-controller-manager</code> (在控制平面)</td><td><code>kubelet</code> (在工作节点上)</td></tr><tr><td><strong>处理对象</strong></td><td>整个 <strong>Pod 对象</strong></td><td>Pod 内部的<strong>容器 (Container)</strong></td></tr><tr><td><strong>结果</strong></td><td>旧 Pod 被驱逐/删除；<strong>创建全新的 Pod</strong></td><td><strong>在同一个 Pod 内重启容器</strong></td></tr><tr><td><strong>Pod UID</strong></td><td>替代品的 UID 是<strong>新的</strong></td><td>Pod 的 UID <strong>保持不变</strong></td></tr><tr><td><strong>Pod IP 地址</strong></td><td>替代品的 IP 地址是<strong>新的</strong></td><td>Pod 的 IP 地址<strong>保持不变</strong></td></tr><tr><td><strong>所在节点</strong></td><td>替代品被调度到<strong>新的健康节点</strong>上</td><td>仍然在<strong>原来的节点</strong>上</td></tr><tr><td><strong>恢复速度</strong></td><td>较慢 (分钟级别，有 5 分钟等待期)</td><td>非常快 (秒级)</td></tr><tr><td><strong><code>kubectl</code> 表现</strong></td><td>旧 Pod 消失，新 Pod 出现</td><td>同一个 Pod 的 <code>RESTARTS</code> 计数增加</td></tr></tbody></table><h5 id="associated-lifetimes" tabindex="-1"><a class="header-anchor" href="#associated-lifetimes"><span>Associated lifetimes</span></a></h5><p>When something is said to have the same lifetime as a Pod, such as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/" target="_blank" rel="noopener noreferrer">volume</a>, that means that the thing exists as long as that specific Pod (with that exact UID) exists.</p><p><img src="/assets/image-20250919162927803-7Bs-HsX1.png" alt="image-20250919162927803"></p><p>这个multi-container Pod 一旦结束生命，那么所关联的Volume也将结束。</p><h5 id="pod-phase" tabindex="-1"><a class="header-anchor" href="#pod-phase"><span>Pod phase</span></a></h5><p>Here are the possible values for <code>phase</code>:</p><table><thead><tr><th style="text-align:left;">Value</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;"><code>Pending</code></td><td style="text-align:left;">The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.</td></tr><tr><td style="text-align:left;"><code>Running</code></td><td style="text-align:left;">The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.</td></tr><tr><td style="text-align:left;"><code>Succeeded</code></td><td style="text-align:left;">All containers in the Pod have terminated in success, and will not be restarted.</td></tr><tr><td style="text-align:left;"><code>Failed</code></td><td style="text-align:left;">All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.</td></tr><tr><td style="text-align:left;"><code>Unknown</code></td><td style="text-align:left;">For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.</td></tr></tbody></table><blockquote><p>Make sure not to confuse <em>Status</em>, a kubectl display field for user intuition, with the pod&#39;s <code>phase</code>. When a pod is failing to start repeatedly, <code>CrashLoopBackOff</code> may appear in the <code>Status</code> field of some kubectl commands. Similarly, when a pod is being deleted, <code>Terminating</code> may appear in the <code>Status</code> field of some kubectl commands.</p></blockquote><blockquote><p><strong>Kubernetes Pod 终止生命周期 (v1.27+) 核心知识点</strong></p><p><strong>1. 核心变化：</strong></p><ul><li>自 K8s v1.27 起，被删除的 Pod 不会从 <code>Terminating</code> 状态直接消失。</li><li>它会先根据容器的最终退出码，过渡到一个明确的<strong>终端阶段</strong>：<code>Succeeded</code> (所有容器退出码为0) 或 <code>Failed</code> (至少一个容器退出码非0)。</li><li><strong>目的</strong>：极大增强了 Pod 的<strong>可观测性</strong>，方便准确追踪一次性任务（如 Job）的最终成败。</li></ul><p><strong>2. “终端阶段”停留时长由谁决定？</strong></p><p>这个停留时间由两种机制控制，<strong>优先级从高到低</strong>：</p><ul><li><strong>机制一 (精确控制 - 推荐): <code>ttlSecondsAfterFinished</code></strong><ul><li><strong>配置</strong>: 在 Pod 或 Job 的 <code>spec</code> 中设置 <code>ttlSecondsAfterFinished: &lt;秒数&gt;</code>。</li><li><strong>行为</strong>: Pod 到达 <code>Succeeded</code>/<code>Failed</code> 状态后，会<strong>精确地</strong>等待指定的秒数，然后被垃圾回收机制自动删除。</li><li><strong>示例</strong>: 设置为 <code>100</code> 则保留100秒；设置为 <code>0</code> 则会立即清理。</li></ul></li><li><strong>机制二 (集群兜底 - 不精确): <code>terminated-pod-gc-threshold</code></strong><ul><li><strong>触发条件</strong>: 仅当 Pod <strong>未设置</strong> <code>ttlSecondsAfterFinished</code> 时此机制才生效。</li><li><strong>行为</strong>: 由集群控制平面 (<code>kube-controller-manager</code>) 的全局参数 <code>--terminated-pod-gc-threshold</code> 控制。只有当集群中已终止的 Pod 总数超过此阈值时，才会开始清理最旧的 Pod。</li><li><strong>结论</strong>: 停留时间<strong>不确定</strong>，可能非常久。</li></ul></li></ul><p><strong>3. 最佳实践：</strong> 为了可预测地管理 Pod 生命周期并保持集群整洁，应始终为你的一次性任务（尤其是 <code>Job</code> 资源）<strong>明确设置 <code>spec.ttlSecondsAfterFinished</code></strong>。</p></blockquote><h5 id="container-states" tabindex="-1"><a class="header-anchor" href="#container-states"><span>Container states</span></a></h5><p>There are three possible container states: <code>Waiting</code>, <code>Running</code>, and <code>Terminated</code>.</p><p>To check the state of a Pod&#39;s containers, you can use <code>kubectl describe pod &lt;name-of-pod&gt;</code>.</p><p><strong><code>Waiting</code></strong></p><p>When you use <code>kubectl</code> to query a Pod with a container that is <code>Waiting</code>, you also see a Reason field to summarize why the container is in that state.</p><p><strong><code>Running</code></strong></p><p>When you use <code>kubectl</code> to query a Pod with a container that is <code>Running</code>, you also see information about when the container entered the <code>Running</code> state.</p><p><strong><code>Terminated</code></strong></p><p>When you use <code>kubectl</code> to query a Pod with a container that is <code>Terminated</code>, you see a reason, an exit code, and the start and finish time for that container&#39;s period of execution.</p><h5 id="how-pods-handle-problems-with-containers" tabindex="-1"><a class="header-anchor" href="#how-pods-handle-problems-with-containers"><span>How Pods handle problems with containers</span></a></h5><p>Kubernetes 容器崩溃处理流程总结</p><table><thead><tr><th>序号</th><th>描述 (Description)</th><th>类型 (Type)</th><th>你能看到的 (<code>kubectl</code>)</th></tr></thead><tbody><tr><td><strong>1</strong></td><td>首次崩溃 (Initial crash)</td><td><strong>事件 (Event)</strong></td><td>几乎看不到，一闪而过</td></tr><tr><td><strong>2</strong></td><td>重复崩溃 (Repeated crashes)</td><td><strong>过程 (Process)</strong></td><td>Pod 状态不稳定, <code>RESTARTS</code> 计数增加</td></tr><tr><td><strong>3</strong></td><td><code>CrashLoopBackOff</code> 状态 (CrashLoopBackOff state)</td><td><strong>状态/原因 (State/Reason)</strong></td><td>明确看到 <code>STATUS</code> 列为 <code>CrashLoopBackOff</code></td></tr><tr><td><strong>4</strong></td><td>退避重置 (Backoff reset)</td><td><strong>动作 (Action)</strong></td><td>看不到这个动作，但能看到结果：Pod 稳定在 <code>Running</code> 状态</td></tr></tbody></table><p>To investigate the root cause of a <code>CrashLoopBackOff</code> issue, a user can:</p><ol><li><strong>Check logs</strong>: Use <code>kubectl logs &lt;name-of-pod&gt;</code> to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes.</li><li><strong>Inspect events</strong>: Use <code>kubectl describe pod &lt;name-of-pod&gt;</code> to see events for the Pod, which can provide hints about configuration or resource issues.</li><li><strong>Review configuration</strong>: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available.</li><li><strong>Check resource limits</strong>: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue.</li><li><strong>Debug application</strong>: There might exist bugs or misconfigurations in the application code. Running this container image locally or in a development environment can help diagnose application specific issues.</li></ol><h6 id="pod-level-container-restart-policy" tabindex="-1"><a class="header-anchor" href="#pod-level-container-restart-policy"><span>Pod-level container restart policy</span></a></h6><p>The <code>spec</code> of a Pod has a <code>restartPolicy</code> field with possible values Always, OnFailure, and Never. The default value is Always.</p><p>The <code>restartPolicy</code> for a Pod applies to <a href="https://kubernetes.io/docs/reference/glossary/?all=true#term-app-container" target="_blank" rel="noopener noreferrer">app containers</a> in the Pod and to regular <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" target="_blank" rel="noopener noreferrer">init containers</a>. <a href="https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" rel="noopener noreferrer">Sidecar containers</a> ignore the Pod-level <code>restartPolicy</code> field</p><p>After containers in a Pod exit, the kubelet restarts them with an exponential backoff delay (10s, 20s, 40s, …), that is capped at 300 seconds (5 minutes). Once a container has executed for 10 minutes without any problems, the kubelet resets the restart backoff timer for that container.</p><h5 id="pod-conditions" tabindex="-1"><a class="header-anchor" href="#pod-conditions"><span>Pod conditions</span></a></h5><p>A Pod has a PodStatus, which has an array of PodConditions through which the Pod has or has not passed. Kubelet manages the following PodConditions:</p><ul><li><code>PodScheduled</code>: the Pod has been scheduled to a node.</li><li><code>PodReadyToStartContainers</code>: (beta feature; enabled by <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-has-network" target="_blank" rel="noopener noreferrer">default</a>) the Pod sandbox has been successfully created and networking configured.</li><li><code>ContainersReady</code>: all containers in the Pod are ready.</li><li><code>Initialized</code>: all <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/" target="_blank" rel="noopener noreferrer">init containers</a> have completed successfully.</li><li><code>Ready</code>: the Pod is able to serve requests and should be added to the load balancing pools of all matching Services.</li><li><code>DisruptionTarget</code>: the pod is about to be terminated due to a disruption (such as preemption, eviction or garbage-collection).</li><li><code>PodResizePending</code>: a pod resize was requested but cannot be applied. See <a href="https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status" target="_blank" rel="noopener noreferrer">Pod resize status</a>.</li><li><code>PodResizeInProgress</code>: the pod is in the process of resizing. See <a href="https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#pod-resize-status" target="_blank" rel="noopener noreferrer">Pod resize status</a>.</li></ul><p>用<code>kubectl describe pod [pod-name]</code> 来查看。每个condition的都有下面的字段的值，用describe pod来查看的时候，只显示 type, status两个最重要的字段，如果要详细的看可以用<code>kubectl get pod [你的pod名称] -o yaml</code></p><table><thead><tr><th style="text-align:left;">Field name</th><th style="text-align:left;">Description</th></tr></thead><tbody><tr><td style="text-align:left;"><code>type</code></td><td style="text-align:left;">Name of this Pod condition.</td></tr><tr><td style="text-align:left;"><code>status</code></td><td style="text-align:left;">Indicates whether that condition is applicable, with possible values &quot;<code>True</code>&quot;, &quot;<code>False</code>&quot;, or &quot;<code>Unknown</code>&quot;.</td></tr><tr><td style="text-align:left;"><code>lastProbeTime</code></td><td style="text-align:left;">Timestamp of when the Pod condition was last probed.</td></tr><tr><td style="text-align:left;"><code>lastTransitionTime</code></td><td style="text-align:left;">Timestamp for when the Pod last transitioned from one status to another.</td></tr><tr><td style="text-align:left;"><code>reason</code></td><td style="text-align:left;">Machine-readable, UpperCamelCase text indicating the reason for the condition&#39;s last transition.</td></tr><tr><td style="text-align:left;"><code>message</code></td><td style="text-align:left;">Human-readable message indicating details about the last status transition.</td></tr></tbody></table><h6 id="pod-readiness" tabindex="-1"><a class="header-anchor" href="#pod-readiness"><span>Pod readiness</span></a></h6><p>Your application can inject extra feedback or signals into PodStatus: <em>Pod readiness</em>. To use this, set <code>readinessGates</code> in the Pod&#39;s <code>spec</code> to specify a list of additional conditions that the kubelet evaluates for Pod readiness.</p><p>example:</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod</span>
<span class="line"><span class="token punctuation">...</span></span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">readinessGates</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">conditionType</span><span class="token punctuation">:</span> <span class="token string">&quot;www.example.com/feature-1&quot;</span></span>
<span class="line"><span class="token key atrule">status</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">conditions</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> Ready                              <span class="token comment"># a built in PodCondition</span></span>
<span class="line">      <span class="token key atrule">status</span><span class="token punctuation">:</span> <span class="token string">&quot;False&quot;</span></span>
<span class="line">      <span class="token key atrule">lastProbeTime</span><span class="token punctuation">:</span> <span class="token null important">null</span></span>
<span class="line">      <span class="token key atrule">lastTransitionTime</span><span class="token punctuation">:</span> <span class="token datetime number">2018-01-01T00:00:00Z</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> <span class="token string">&quot;www.example.com/feature-1&quot;</span>        <span class="token comment"># an extra PodCondition</span></span>
<span class="line">      <span class="token key atrule">status</span><span class="token punctuation">:</span> <span class="token string">&quot;False&quot;</span></span>
<span class="line">      <span class="token key atrule">lastProbeTime</span><span class="token punctuation">:</span> <span class="token null important">null</span></span>
<span class="line">      <span class="token key atrule">lastTransitionTime</span><span class="token punctuation">:</span> <span class="token datetime number">2018-01-01T00:00:00Z</span></span>
<span class="line">  <span class="token key atrule">containerStatuses</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">containerID</span><span class="token punctuation">:</span> docker<span class="token punctuation">:</span>//abcd<span class="token punctuation">...</span></span>
<span class="line">      <span class="token key atrule">ready</span><span class="token punctuation">:</span> <span class="token boolean important">true</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>一个 Pod 必须同时满足所有 readinessGates 条件 并且 其自身的 readinessProbe 成功，才会被 Kubernetes 最终标记为 Ready 状态，然后 Service 才会将流量转发给它。ready好了没有，可以用这个来查询。 其中的READY 字段</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">leite@leite-company ~<span class="token operator">&gt;</span> kubectl get pods</span>
<span class="line">NAME                                      READY   STATUS    RESTARTS   AGE</span>
<span class="line">springboot3-deployment-559c8cc88b-l6sjg   <span class="token number">0</span>/1     Running   <span class="token number">0</span>          13s</span>
<span class="line">springboot3-deployment-559c8cc88b-rr22r   <span class="token number">0</span>/1     Running   <span class="token number">0</span>          13s</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="container-probes" tabindex="-1"><a class="header-anchor" href="#container-probes"><span>Container probes</span></a></h5><h6 id="check-mechanisms" tabindex="-1"><a class="header-anchor" href="#check-mechanisms"><span>Check mechanisms</span></a></h6><ol><li><code>exec</code> (执行命令)</li></ol><ul><li><p><strong>核心思想</strong>: 在容器内部执行一个你指定的命令。</p></li><li><p><strong>成功标准</strong>: 该命令执行后的退出码（Exit Code）为 <code>0</code>。任何非 <code>0</code> 的退出码都被认为是失败。</p></li><li><p><strong>适用场景</strong>:</p><ul><li>当你的应用程序没有提供 HTTP 健康检查接口时。</li><li>需要检查应用内部的特定状态，例如：检查某个重要文件是否存在 (<code>cat /tmp/healthy</code>)，或者运行一个自定义的健康检查脚本 (<code>/usr/bin/check-health.sh</code>)。</li><li>非常灵活，可以实现复杂的健康检查逻辑。</li></ul><p><em>示例</em>：</p><p>YAML</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">exec</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">command</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> cat</span>
<span class="line">    <span class="token punctuation">-</span> /tmp/healthy</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><ol start="2"><li><code>httpGet</code> (HTTP GET 请求)</li></ol><ul><li><p><strong>核心思想</strong>: 向容器的 IP 地址、指定端口和路径发送一个 HTTP GET 请求。</p></li><li><p><strong>成功标准</strong>: 收到的 HTTP 响应状态码（Status Code）大于等于 <code>200</code> 且小于 <code>400</code>（即 <code>2xx</code> 或 <code>3xx</code> 系列）。</p></li><li><p><strong>适用场景</strong>:</p><ul><li><strong>最常用</strong>的方式，几乎所有 Web 应用或提供 HTTP API 的服务都适用。</li><li>通常应用会专门提供一个用于健康检查的端点（endpoint），比如 <code>/healthz</code> 或 <code>/status</code>。</li></ul><p><em>示例</em>：</p><p>YAML</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">readinessProbe</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">httpGet</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">path</span><span class="token punctuation">:</span> /healthz</span>
<span class="line">    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">8080</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><ol start="3"><li><code>tcpSocket</code> (TCP 套接字)</li></ol><ul><li><p><strong>核心思想</strong>: 尝试与容器的指定端口建立一个 TCP 连接。</p></li><li><p><strong>成功标准</strong>: TCP “三次握手”成功，即端口是开放的。只要能成功建立连接，就被认为是健康的，即使连接立即被关闭。</p></li><li><p><strong>适用场景</strong>:</p><ul><li>适用于那些不提供 HTTP 接口，但监听特定 TCP 端口的服务。</li><li>例如：数据库（MySQL 监听 <code>3306</code>），缓存服务（Redis 监听 <code>6379</code>），或者其他任何基于 TCP 的应用。</li></ul><p><em>示例</em>：</p><p>YAML</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">tcpSocket</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">3306</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><ol start="4"><li><code>grpc</code> (gRPC 远程过程调用)</li></ol><ul><li><p><strong>核心思想</strong>: 使用 gRPC 协议执行一个远程过程调用。这是比较新且特定的一种方式。</p></li><li><p><strong>成功标准</strong>: 响应的状态是 <code>SERVING</code>。这要求你的应用必须实现 <a href="https://www.google.com/search?q=https://github.com/grpc/grpc/blob/master/doc/health-checking.md" target="_blank" rel="noopener noreferrer">gRPC Health Checking Protocol</a>。</p></li><li><p><strong>适用场景</strong>:</p><ul><li>专门用于基于 gRPC 构建的微服务。</li><li>如果你的技术栈广泛使用 gRPC，这是一种比 <code>httpGet</code> 更原生、更高效的检查方式。</li></ul><p><em>示例</em>：</p><p>YAML</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line"><span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> grpc</span>
<span class="line">  <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">9000</span></span>
<span class="line"><span class="token key atrule">livenessProbe</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">grpc</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">9000</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><h6 id="probe-outcome" tabindex="-1"><a class="header-anchor" href="#probe-outcome"><span>Probe outcome</span></a></h6><p>Each probe has one of three results:</p><ul><li><p><code>Success</code></p><p>The container passed the diagnostic.</p></li><li><p><code>Failure</code></p><p>The container failed the diagnostic.</p></li><li><p><code>Unknown</code></p><p>The diagnostic failed (no action should be taken, and the kubelet will make further checks).</p></li></ul><h6 id="types-of-probe" tabindex="-1"><a class="header-anchor" href="#types-of-probe"><span>Types of probe</span></a></h6><ul><li><p><strong>livenessProbe</strong></p></li><li><p><strong>readinessProbe</strong></p><p>readiness probe返回Failure并不会导致容器重启</p></li><li><p><strong>startupProbe</strong></p><p>Startup probes are useful for Pods that have containers that take a long time to come into service.</p><p>Indicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until it succeeds</p><p>livenessProbe (存活探针) 和 readinessProbe (就绪探针) 在 startupProbe 首次成功之前，根本不会开始执行。</p></li></ul><p><strong>为什么需要 <code>startupProbe</code>？</strong></p><p>想象一个场景：你有一个复杂的 Java 应用，它启动时需要加载大量数据、预热缓存、建立数据库连接池等，整个过程可能需要2到3分钟。</p><ul><li><strong>如果没有 <code>startupProbe</code></strong>：你可能会配置一个 <code>livenessProbe</code>，让它每10秒检查一次应用的健康状况。但由于应用启动需要180秒，这个 <code>livenessProbe</code> 在前170秒内所有的探测都会是失败的。如果你的 <code>failureThreshold</code> (失败阈值) 设置为5，那么在第50秒时 (10秒 * 5次)，kubelet 就会认为你的应用已经死了，从而杀死并重启它。这个过程会无限循环，你的应用永远也启动不起来。</li><li><strong>有了 <code>startupProbe</code></strong>：你可以专门为这个漫长的启动过程配置一个 <code>startupProbe</code>。例如，设置一个很长的探测周期和足够高的失败阈值，给它总共5分钟的时间来完成启动。 <ul><li>在这5分钟内，只有 <code>startupProbe</code> 在工作。</li><li><code>livenessProbe</code> 和 <code>readinessProbe</code> 会一直“袖手旁观”。</li><li>一旦 <code>startupProbe</code> 探测成功，它就“功成身退”，此后永远不会再执行。</li><li>紧接着，<code>livenessProbe</code> 和 <code>readinessProbe</code> 开始接管，分别负责监控容器在运行期间是否健康以及是否准备好接收流量。</li></ul></li></ul><h5 id="termination-of-pods" tabindex="-1"><a class="header-anchor" href="#termination-of-pods"><span>Termination of Pods</span></a></h5><p>If the kubelet or the container runtime&#39;s management service is restarted while waiting for processes to terminate, the cluster retries from the start including the full original grace period.</p><h6 id="pod-termination-flow" tabindex="-1"><a class="header-anchor" href="#pod-termination-flow"><span>Pod Termination Flow</span></a></h6><p>If the <code>preStop</code> hook is still running after the grace period expires, the kubelet requests a small, one-off grace period extension of 2 seconds.</p><h6 id="forced-pod-termination" tabindex="-1"><a class="header-anchor" href="#forced-pod-termination"><span>Forced Pod termination</span></a></h6><p>By default, all deletes are graceful within 30 seconds. The <code>kubectl delete</code> command supports the <code>--grace-period=&lt;seconds&gt;</code> option which allows you to override the default and specify your own value.</p><p>Using kubectl, You must specify an additional flag <code>--force</code> along with <code>--grace-period=0</code> in order to perform force deletions.</p><h6 id="pod-shutdown-and-sidecar-containers" tabindex="-1"><a class="header-anchor" href="#pod-shutdown-and-sidecar-containers"><span>Pod shutdown and sidecar containers</span></a></h6><p>If your Pod includes one or more <a href="https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" rel="noopener noreferrer">sidecar containers</a> (init containers with an Always restart policy), the kubelet will delay sending the TERM signal to these sidecar containers until the last main container has fully terminated.</p><h4 id="init-containers" tabindex="-1"><a class="header-anchor" href="#init-containers"><span>Init Containers</span></a></h4><h5 id="understanding-init-containers" tabindex="-1"><a class="header-anchor" href="#understanding-init-containers"><span>Understanding init containers</span></a></h5><p>Init containers are exactly like regular containers, except:</p><ul><li>Init containers always run to completion.</li><li>Each init container must complete successfully before the next one starts.</li></ul><h6 id="differences-from-regular-containers" tabindex="-1"><a class="header-anchor" href="#differences-from-regular-containers"><span>Differences from regular containers</span></a></h6><p>Regular init containers (in other words: excluding sidecar containers) do not support the <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or <code>startupProbe</code> fields.</p><p>sidecar containers continue running during a Pod&#39;s lifetime, and <em>do</em> support some probes.</p><h6 id="differences-from-sidecar-containers" tabindex="-1"><a class="header-anchor" href="#differences-from-sidecar-containers"><span>Differences from sidecar containers</span></a></h6><p>Unlike <a href="https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/" target="_blank" rel="noopener noreferrer">sidecar containers</a>, init containers are not continuously running alongside the main containers.</p><p>init containers do not support <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or <code>startupProbe</code> whereas sidecar containers support all these <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#types-of-probe" target="_blank" rel="noopener noreferrer">probes</a> to control their lifecycle.</p><h5 id="detailed-behavior" tabindex="-1"><a class="header-anchor" href="#detailed-behavior"><span>Detailed behavior</span></a></h5><p>However, if the Pod <code>restartPolicy</code> is set to Always, the init containers use <code>restartPolicy</code> OnFailure.</p><blockquote><p>即使pod的restartPolicy是always, 但对于init containers来说其实相当于Onfaiure.</p></blockquote><p>A Pod that is initializing is in the <code>Pending</code> state but should have a condition <code>Initialized</code> set to false.</p><p>If the Pod <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#pod-restart-reasons" target="_blank" rel="noopener noreferrer">restarts</a>, or is restarted, all init containers must execute again.</p><p>Because init containers can be restarted, retried, or re-executed, init container code should be idempotent.</p><p>However, Kubernetes prohibits <code>readinessProbe</code> from being used because init containers cannot define readiness distinct from completion.</p><p>However it is recommended to use <code>activeDeadlineSeconds</code> only if teams deploy their application as a Job, because <code>activeDeadlineSeconds</code> has an effect even after initContainer finished.</p><h6 id="resource-sharing-within-containers" tabindex="-1"><a class="header-anchor" href="#resource-sharing-within-containers"><span>Resource sharing within containers</span></a></h6><ol><li>核心概念：<code>Effective Init Request/Limit</code></li></ol><ul><li><strong>定义</strong>: 它是 Kubernetes 计算出的一个<strong>中间值</strong>，代表 <code>init</code> 阶段对<strong>单一资源</strong>（如 <code>memory</code> 或 <code>cpu</code>）的最大需求。</li><li><strong>计算方法</strong>: 取<strong>所有 <code>init</code> 容器</strong>中，对<strong>同一种资源</strong>（<code>cpu</code> 或 <code>memory</code>）设置的 <code>request</code> 或 <code>limit</code> 的<strong>最大值</strong>。 <ul><li><code>Effective Init Request</code> = <code>MAX(init_container_1_request, init_container_2_request, ...)</code></li><li><code>Effective Init Limit</code> = <code>MAX(init_container_1_limit, init_container_2_limit, ...)</code></li></ul></li></ul><ol start="2"><li><p>Pod 最终资源规格的计算规则</p><p>Pod 启动所需的资源，必须同时满足 <code>init</code> 容器（轮流执行）和 <code>main</code> 容器（同时执行）的需求。</p></li></ol><ul><li><strong>Pod 总请求 (Request)</strong> = <code>MAX</code> ( <strong>所有主容器请求之和</strong> , <strong>Effective Init Request</strong> )</li><li><strong>Pod 总限制 (Limit)</strong> = <code>MAX</code> ( <strong>所有主-容器限制之和</strong> , <strong>Effective Init Limit</strong> )</li></ul><ol start="3"><li>关键要点与边界情况</li></ol><ul><li><strong>独立计算</strong>: <code>cpu</code> 和 <code>memory</code> 两种资源的 <code>request</code> 和 <code>limit</code> 是完全分开独立计算的。</li><li><strong>主容器优先</strong>: 如果任何<strong>一个主容器</strong>没有设置 <code>limit</code>，那么整个 Pod 的 <code>limit</code> 就是<strong>无限制</strong>的。<code>init</code> 容器设置的 <code>limit</code> 无法约束主容器。</li><li><strong>影响 QoS</strong>: 未设置 <code>limit</code> 会导致 Pod 的 QoS 等级降为 <code>Burstable</code>，在节点资源紧张时有被驱逐的风险。</li><li><strong>调度依据</strong>: Pod 的总请求 (<code>Pod Total Request</code>) 是调度器 (<code>kube-scheduler</code>) 在为 Pod 选择节点时的重要依据。</li></ul><h6 id="pod-restart-reasons" tabindex="-1"><a class="header-anchor" href="#pod-restart-reasons"><span>Pod restart reasons</span></a></h6><table><thead><tr><th>场景</th><th>核心触发事件</th><th>Init 记录丢失的角色</th><th>Pod 是否重启</th><th>Init 容器是否重新运行</th></tr></thead><tbody><tr><td><strong>第一种</strong></td><td>主容器全部终止</td><td>附加条件</td><td><strong>是</strong></td><td><strong>是</strong>（在 Pod 重启流程中）</td></tr><tr><td><strong>第二种</strong></td><td>仅 Init 记录丢失</td><td>唯一事件</td><td><strong>否</strong></td><td><strong>否</strong></td></tr></tbody></table><h4 id="sidecar-containers" tabindex="-1"><a class="header-anchor" href="#sidecar-containers"><span>Sidecar Containers</span></a></h4><h5 id="sidecar-containers-in-kubernetes" tabindex="-1"><a class="header-anchor" href="#sidecar-containers-in-kubernetes"><span>Sidecar containers in Kubernetes</span></a></h5><p>These restartable <em>sidecar</em> containers are independent from other init containers and from the main application container(s) within the same pod. These can be started, stopped, or restarted without affecting the main application container and other init containers.</p><p>Example:</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Deployment</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp</span>
<span class="line">  <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> myapp</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">1</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">app</span><span class="token punctuation">:</span> myapp</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">app</span><span class="token punctuation">:</span> myapp</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> myapp</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine<span class="token punctuation">:</span>latest</span>
<span class="line">          <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sh&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;-c&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;while true; do echo &quot;logging&quot; &gt;&gt; /opt/logs.txt; sleep 1; done&#39;</span><span class="token punctuation">]</span></span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data</span>
<span class="line">              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /opt</span>
<span class="line">      <span class="token key atrule">initContainers</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> logshipper</span>
<span class="line">          <span class="token key atrule">image</span><span class="token punctuation">:</span> alpine<span class="token punctuation">:</span>latest</span>
<span class="line">          <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Always</span>
<span class="line">          <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&#39;sh&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;-c&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;tail -F /opt/logs.txt&#39;</span><span class="token punctuation">]</span></span>
<span class="line">          <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data</span>
<span class="line">              <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /opt</span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> data</span>
<span class="line">          <span class="token key atrule">emptyDir</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="sidecar-containers-and-pod-lifecycle" tabindex="-1"><a class="header-anchor" href="#sidecar-containers-and-pod-lifecycle"><span>Sidecar containers and Pod lifecycle</span></a></h5><p>If an init container is created with its <code>restartPolicy</code> set to <code>Always</code>, it will start and remain running during the entire life of the Pod.</p><p>After a sidecar-style init container is running (the kubelet has set the <code>started</code> status for that init container to true), the kubelet then starts the next init container from the ordered <code>.spec.initContainers</code> list.</p><blockquote><p>具体来说，它的完整路径是 pod.status.initContainerStatuses[].started。 sidecar container启动后可能会一直处于running状态[而不是正常的init container中的等前一个init container的state变为terminated再启动下一个]，但它不会影响下一个init container的正常启动，因为这个<code>....started</code>的状态值为true</p><p>这不是一个你会用 kubectl get pods 直接看到的顶层状态，而是需要查看 Pod 的详细 YAML 或 JSON 描述才能找到的内部状态.</p></blockquote><h6 id="jobs-with-sidecar-containers" tabindex="-1"><a class="header-anchor" href="#jobs-with-sidecar-containers"><span>Jobs with sidecar containers</span></a></h6><p>If you define a Job that uses sidecar using Kubernetes-style init containers, the sidecar container in each Pod does not prevent the Job from completing after the main container has finished.</p><blockquote><p>Sidecar 容器本身将不再成为判断 Pod 是否成功完成的阻碍。</p></blockquote><h5 id="differences-from-application-containers" tabindex="-1"><a class="header-anchor" href="#differences-from-application-containers"><span>Differences from application containers</span></a></h5><p>So exit codes different from <code>0</code> (<code>0</code> indicates successful exit), for sidecar containers are normal on Pod termination and should be generally ignored by the external tooling.</p><h5 id="differences-from-init-containers" tabindex="-1"><a class="header-anchor" href="#differences-from-init-containers"><span>Differences from init containers</span></a></h5><p>Sidecar containers run concurrently with the main application container.</p><p>Sidecar containers can interact directly with the main application containers, because like init containers they always share the same network, and can optionally also share volumes (filesystems).</p><p>Init containers stop before the main containers start up, so init containers cannot exchange messages with the app container in a Pod. Any data passing is one-way (for example, an init container can put information inside an <code>emptyDir</code> volume).</p><h4 id="ephemeral-containers" tabindex="-1"><a class="header-anchor" href="#ephemeral-containers"><span>Ephemeral Containers</span></a></h4><p>A special type of container that runs temporarily in an existing <a href="https://kubernetes.io/docs/concepts/workloads/pods/" target="_blank" rel="noopener noreferrer">Pod</a> to accomplish user-initiated actions such as troubleshooting. You use ephemeral containers to inspect services rather than to build applications.</p><blockquote><h4 id="note" tabindex="-1"><a class="header-anchor" href="#note"><span>Note:</span></a></h4><p>Ephemeral containers are not supported by <a href="https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/" target="_blank" rel="noopener noreferrer">static pods</a>.</p></blockquote><p>Ephemeral containers are created using a special <code>ephemeralcontainers</code> handler in the API rather than by adding them directly to <code>pod.spec</code>, so it&#39;s not possible to add an ephemeral container using <code>kubectl edit</code>.</p><p>可以使用 <code>kubectl debug</code> 命令来附加一个临时容器：</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># 语法: kubectl debug -it &lt;pod-name&gt; --image=&lt;debug-image&gt; --target=&lt;app-container-name&gt; -- &lt;command&gt;</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 附加一个 busybox 容器，并启动一个交互式的 shell</span></span>
<span class="line">kubectl debug <span class="token parameter variable">-it</span> my-app-pod <span class="token parameter variable">--image</span><span class="token operator">=</span>busybox <span class="token parameter variable">--target</span><span class="token operator">=</span>my-app-container</span>
<span class="line"></span>
<span class="line"><span class="token comment"># 进入 shell 后，你就位于 my-app-pod 的网络环境中了</span></span>
<span class="line"><span class="token comment"># 你可以...</span></span>
<span class="line"><span class="token comment"># 检查网络连接</span></span>
<span class="line">/ <span class="token comment"># ping google.com</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 检查应用容器的端口是否在监听 (假设应用跑在 80 端口)</span></span>
<span class="line">/ <span class="token comment"># wget -qO- localhost:80</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 查看所有进程 (如果开启了进程共享)</span></span>
<span class="line">/ <span class="token comment"># ps aux</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>当你执行 <code>kubectl debug</code> 后，如果你去查看 Pod 的 YAML 定义，你会发现多了一个 <code>ephemeralContainers</code> 字段，里面描述了你刚刚添加的 <code>busybox</code> 容器。</p><h4 id="disruptions" tabindex="-1"><a class="header-anchor" href="#disruptions"><span>Disruptions</span></a></h4><p>This guide is for application owners who want to build highly available applications, and thus need to understand what types of disruptions can happen to Pods.</p><h5 id="voluntary-and-involuntary-disruptions" tabindex="-1"><a class="header-anchor" href="#voluntary-and-involuntary-disruptions"><span>Voluntary and involuntary disruptions</span></a></h5><p>We call these unavoidable cases <em>involuntary disruptions</em> to an application. Examples are:</p><ul><li>a hardware failure of the physical machine backing the node</li><li>cluster administrator deletes VM (instance) by mistake</li><li>cloud provider or hypervisor failure makes VM disappear</li><li>a kernel panic</li><li>the node disappears from the cluster due to cluster network partition</li><li>eviction of a pod due to the node being <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank" rel="noopener noreferrer">out-of-resources</a>.</li></ul><p>We call other cases <em>voluntary disruptions</em>. These include both actions initiated by the application owner and those initiated by a Cluster Administrator. Typical application owner actions include:</p><ul><li>deleting the deployment or other controller that manages the pod</li><li>updating a deployment&#39;s pod template causing a restart</li><li>directly deleting a pod (e.g. by accident)</li></ul><p>Cluster administrator actions include:</p><ul><li><a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/" target="_blank" rel="noopener noreferrer">Draining a node</a> for repair or upgrade.</li><li>Draining a node from a cluster to scale the cluster down (learn about <a href="https://kubernetes.io/docs/concepts/cluster-administration/node-autoscaling/" target="_blank" rel="noopener noreferrer">Node Autoscaling</a>).</li><li>Removing a pod from a node to permit something else to fit on that node.</li></ul><h5 id="dealing-with-disruptions" tabindex="-1"><a class="header-anchor" href="#dealing-with-disruptions"><span>Dealing with disruptions</span></a></h5><p>Here are some ways to mitigate involuntary disruptions:</p><ul><li>Ensure your pod <a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" rel="noopener noreferrer">requests the resources</a> it needs.</li><li>Replicate your application if you need higher availability. (Learn about running replicated <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/" target="_blank" rel="noopener noreferrer">stateless</a> and <a href="https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/" target="_blank" rel="noopener noreferrer">stateful</a> applications.)</li><li>For even higher availability when running replicated applications, spread applications across racks (using <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity" target="_blank" rel="noopener noreferrer">anti-affinity</a>) or across zones (if using a <a href="https://kubernetes.io/docs/setup/multiple-zones" target="_blank" rel="noopener noreferrer">multi-zone cluster</a>.)</li></ul><h5 id="pod-disruption-budgets" tabindex="-1"><a class="header-anchor" href="#pod-disruption-budgets"><span>Pod disruption budgets</span></a></h5><p>As an application owner, you can create a PodDisruptionBudget (PDB) for each application. A PDB limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions.</p><p>A PDB specifies the number of replicas that an application can tolerate having, relative to how many it is intended to have. For example, a Deployment which has a <code>.spec.replicas: 5</code> is supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.</p><p>It is recommended to set <code>AlwaysAllow</code> <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy" target="_blank" rel="noopener noreferrer">Unhealthy Pod Eviction Policy</a> to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod" target="_blank" rel="noopener noreferrer">healthy</a> before the drain can proceed.</p><p>example:</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> policy/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> PodDisruptionBudget</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>app<span class="token punctuation">-</span>pdb</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">minAvailable</span><span class="token punctuation">:</span> <span class="token number">2</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">app</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>app</span>
<span class="line">  <span class="token comment"># 关键配置在这里！</span></span>
<span class="line">  <span class="token key atrule">unhealthyPodEvictionPolicy</span><span class="token punctuation">:</span> AlwaysAllow</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><table><thead><tr><th>策略</th><th><code>IfHealthyBudget</code> (默认)</th><th><code>AlwaysAllow</code></th></tr></thead><tbody><tr><td><strong>优点</strong></td><td>对应用更“安全”，尽最大努力保证健康实例的数量。</td><td><strong>优先保障集群运维</strong>，不会因为单个应用的故障而阻塞节点维护、升级等重要操作。</td></tr><tr><td><strong>缺点</strong></td><td><strong>可能阻塞节点排空</strong>，导致集群运维工作无法进行。</td><td>如果应用的大多数实例都不健康，<code>drain</code> 操作可能会驱逐掉最后几个健康的实例，可能导致服务短暂中断。</td></tr><tr><td><strong>适用场景</strong></td><td>极少数情况下，如果应用的健康比集群的可维护性更重要，且应用本身非常稳定。</td><td><strong>绝大多数场景的推荐做法</strong>。它遵循一个重要的运维理念：应用的故障应该由应用自身解决（例如通过控制器重建），而不应该影响到整个基础设施的管理。</td></tr></tbody></table><h5 id="pod-disruption-conditions" tabindex="-1"><a class="header-anchor" href="#pod-disruption-conditions"><span>Pod disruption conditions</span></a></h5><p>[与 Pod conditions 关联](#Pod conditions)</p><p>A dedicated Pod <code>DisruptionTarget</code> <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions" target="_blank" rel="noopener noreferrer">condition</a> is added to indicate that the Pod is about to be deleted due to a <a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/" target="_blank" rel="noopener noreferrer">disruption</a>. The <code>reason</code> field of the condition additionally indicates one of the following reasons for the Pod termination:</p><ul><li><p><code>PreemptionByScheduler</code></p><p>Pod is due to be <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank" rel="noopener noreferrer">preempted</a> by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/" target="_blank" rel="noopener noreferrer">Pod priority preemption</a>.</p></li><li><p><code>DeletionByTaintManager</code></p><p>Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within <code>kube-controller-manager</code>) due to a <code>NoExecute</code> taint that the Pod does not tolerate; see <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" rel="noopener noreferrer">taint</a>-based evictions.</p></li><li><p><code>EvictionByEvictionAPI</code></p><p>Pod has been marked for <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/api-eviction/" target="_blank" rel="noopener noreferrer">eviction using the Kubernetes API</a> .</p></li><li><p><code>DeletionByPodGC</code></p><p>Pod, that is bound to a no longer existing Node, is due to be deleted by <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection" target="_blank" rel="noopener noreferrer">Pod garbage collection</a>.</p></li><li><p><code>TerminationByKubelet</code></p><p>Pod has been terminated by the kubelet, because of either <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank" rel="noopener noreferrer">node pressure eviction</a>, the <a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown" target="_blank" rel="noopener noreferrer">graceful node shutdown</a>, or preemption for <a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/" target="_blank" rel="noopener noreferrer">system critical pods</a></p></li></ul><p>When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job&#39;s <a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-failure-policy" target="_blank" rel="noopener noreferrer">Pod failure policy</a>.</p><h4 id="pod-hostname" tabindex="-1"><a class="header-anchor" href="#pod-hostname"><span>Pod hostname</span></a></h4><h5 id="default-pod-hostname" tabindex="-1"><a class="header-anchor" href="#default-pod-hostname"><span>Default Pod hostname</span></a></h5><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> busybox<span class="token punctuation">-</span><span class="token number">1</span></span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox<span class="token punctuation">:</span><span class="token number">1.28</span></span>
<span class="line">    <span class="token key atrule">command</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> sleep</span>
<span class="line">      <span class="token punctuation">-</span> <span class="token string">&quot;3600&quot;</span></span>
<span class="line">    <span class="token key atrule">name</span><span class="token punctuation">:</span> busybox</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>The Pod created by this manifest will have its hostname and fully qualified domain name (FQDN) set to <code>busybox-1</code>.</p><h5 id="hostname-with-pod-s-hostname-and-subdomain-fields" tabindex="-1"><a class="header-anchor" href="#hostname-with-pod-s-hostname-and-subdomain-fields"><span>Hostname with pod&#39;s hostname and subdomain fields</span></a></h5><p>The Pod spec includes an optional <code>hostname</code> field. When set, this value takes precedence over the Pod&#39;s <code>metadata.name</code> as the hostname (observed from within the Pod).</p><p>When both hostname and subdomain are set, the cluster&#39;s DNS server will create A and/or AAAA records based on these fields.</p><h5 id="hostname-with-pod-s-sethostnameasfqdn-fields" tabindex="-1"><a class="header-anchor" href="#hostname-with-pod-s-sethostnameasfqdn-fields"><span>Hostname with pod&#39;s setHostnameAsFQDN fields</span></a></h5><p>When both <code>setHostnameAsFQDN: true</code> and the subdomain field is set in the Pod spec, the kubelet writes the Pod&#39;s FQDN into the hostname for that Pod&#39;s namespace. In this case, both <code>hostname</code> and <code>hostname --fqdn</code> return the Pod&#39;s FQDN.</p><blockquote><p>Note: In Linux, the hostname field of the kernel (the nodename field of struct utsname) is limited to 64 characters.</p><p>If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in Pending status (ContainerCreating as seen by kubectl) generating error events, such as &quot;Failed to construct FQDN from Pod hostname and cluster domain&quot;.</p><p>This means that when using this field, you must ensure the combined length of the Pod&#39;s metadata.name (or spec.hostname) and spec.subdomain fields results in an FQDN that does not exceed 64 characters.</p></blockquote><h4 id="pod-qos-classes" tabindex="-1"><a class="header-anchor" href="#pod-qos-classes"><span>Pod QoS classes</span></a></h4><h5 id="quality-of-service-classes" tabindex="-1"><a class="header-anchor" href="#quality-of-service-classes"><span>Quality of Service classes</span></a></h5><p>Kubernetes assigns every Pod a QoS class based on the resource requests and limits of its component Containers. QoS classes are used by Kubernetes to decide which Pods to evict from a Node experiencing <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank" rel="noopener noreferrer">Node Pressure</a>. The possible QoS classes are <code>Guaranteed</code>, <code>Burstable</code>, and <code>BestEffort</code>. When a Node runs out of resources, Kubernetes will first evict <code>BestEffort</code> Pods running on that Node, followed by <code>Burstable</code> and finally <code>Guaranteed</code> Pods.</p><ol><li><strong><code>Guaranteed</code> (最高优先级)</strong><ul><li><strong>条件</strong>: Pod 中的<strong>每一个</strong>容器都必须同时设置了 CPU 和内存的 <code>requests</code> 和 <code>limits</code>，并且对于每一种资源，<code>requests</code> 的值必须<strong>严格等于</strong> <code>limits</code> 的值。</li><li><strong>特点</strong>: 这些 Pod 的资源需求是完全可预测的。只要不超过 <code>limits</code>，它们就能获得所请求的资源。在节点资源紧张时，这类 Pod 最后才会被驱逐。</li></ul></li><li><strong><code>BestEffort</code> (最低优先级)</strong><ul><li><strong>条件</strong>: Pod 中的任何一个容器都没有设置任何 <code>requests</code> 或 <code>limits</code>。</li><li><strong>特点</strong>: 这些 Pod 没有任何资源保障，它们会使用节点上一切可用的空闲资源。当节点资源紧张时，这类 Pod 是<strong>最先被驱逐</strong>的。</li></ul></li><li><strong><code>Burstable</code> (中等优先级)</strong><ul><li><strong>条件</strong>: Pod 不满足 <code>Guaranteed</code> 和 <code>BestEffort</code> 的任何一个条件。换句话说，只要 Pod 中至少有一个容器设置了 <code>requests</code>，但又不完全满足 <code>Guaranteed</code> 的严格要求，它就是 <code>Burstable</code>。</li><li><strong>特点</strong>: 这类 Pod 获得了一定程度的资源保障（由 <code>requests</code> 保证），同时允许在节点资源有富余时，使用超过其 <code>requests</code> 的资源，最多不能超过其 <code>limits</code>（如果设置了的话）。在资源紧张时，它们的驱逐优先级介于 <code>Guaranteed</code> 和 <code>BestEffort</code> 之间。</li></ul></li></ol><h5 id="some-behavior-is-independent-of-qos-class" tabindex="-1"><a class="header-anchor" href="#some-behavior-is-independent-of-qos-class"><span>Some behavior is independent of QoS class</span></a></h5><p>For example:</p><ul><li>Any Container exceeding a resource limit will be killed and restarted by the kubelet without affecting other Containers in that Pod.</li><li>If a Container exceeds its resource request and the node it runs on faces resource pressure, the Pod it is in becomes a candidate for <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/" target="_blank" rel="noopener noreferrer">eviction</a>. If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a replacement Pod, usually on a different node.</li><li>The resource request of a Pod is equal to the sum of the resource requests of its component Containers, and the resource limit of a Pod is equal to the sum of the resource limits of its component Containers.</li><li>The kube-scheduler does not consider QoS class when selecting which Pods to <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption" target="_blank" rel="noopener noreferrer">preempt</a>. Preemption can occur when a cluster does not have enough resources to run all the Pods you defined.</li></ul><h4 id="downward-api" tabindex="-1"><a class="header-anchor" href="#downward-api"><span>Downward API</span></a></h4><p>There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API.</p><h5 id="available-fields" tabindex="-1"><a class="header-anchor" href="#available-fields"><span>Available fields</span></a></h5><p>You can pass information from available Pod-level fields using <code>fieldRef</code>. <a href="https://kubernetes.io/docs/concepts/workloads/pods/downward-api/#downwardapi-fieldRef" target="_blank" rel="noopener noreferrer">fieldRef</a></p><p>You can pass information from available Container-level fields using <code>resourceFieldRef</code>.</p><h3 id="workload-management" tabindex="-1"><a class="header-anchor" href="#workload-management"><span>Workload management</span></a></h3><h4 id="deployments" tabindex="-1"><a class="header-anchor" href="#deployments"><span>Deployments</span></a></h4><h5 id="creating-a-deployment" tabindex="-1"><a class="header-anchor" href="#creating-a-deployment"><span>Creating a Deployment</span></a></h5><ul><li><p>Do not manage ReplicaSets owned by a Deployment.</p></li><li><p>Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets).</p><blockquote><p>不要让不同的控制器[[实例]（Controller）使用可以匹配到同一批 Pod 的选择器（Selector）</p></blockquote></li><li><p>The <code>pod-template-hash</code> label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.</p></li></ul><h5 id="updating-a-deployment" tabindex="-1"><a class="header-anchor" href="#updating-a-deployment"><span>Updating a Deployment</span></a></h5><p>If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match <code>.spec.selector</code> but whose template does not match <code>.spec.template</code> is scaled down.</p><blockquote><p>ReplicaSet去控制 selector没有变的; .spec.template变化了</p></blockquote><h6 id="label-selector-updates" tabindex="-1"><a class="header-anchor" href="#label-selector-updates"><span>Label selector updates</span></a></h6><ul><li><p>Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too, otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet.</p><blockquote><p>如果selctor增加了新的label, 对应的<code>spec.template.metadata.labels</code>也要加上这个新的label. 更新后旧的ReplicaSets并不会自动被清除</p></blockquote></li><li><p>Selector updates changes the existing value in a selector key -- result in the same behavior as additions.</p></li><li><p>Selector removals removes an existing key from the Deployment selector -- do not require any changes in the Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the removed label still exists in any existing Pods and ReplicaSets.</p></li></ul><h5 id="rolling-back-a-deployment" tabindex="-1"><a class="header-anchor" href="#rolling-back-a-deployment"><span>Rolling Back a Deployment</span></a></h5><p>when you roll back to an earlier revision, only the Deployment&#39;s Pod template part is rolled back.</p><blockquote><p>不会改变的: 你手动设置的副本数 replicas: 5 不会回滚到初始的 3。因为 replicas 字段不属于 .spec.template (Pod 模板) 的一部分。它属于 Deployment 的控制器策略。</p></blockquote><h6 id="checking-rollout-history-of-a-deployment" tabindex="-1"><a class="header-anchor" href="#checking-rollout-history-of-a-deployment"><span>Checking Rollout History of a Deployment</span></a></h6><p>check the revisions of this Deployment:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl rollout <span class="token function">history</span> deployment/nginx-deployment</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>To see the details of each revision, run:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl rollout <span class="token function">history</span> deployment/nginx-deployment <span class="token parameter variable">--revision</span><span class="token operator">=</span><span class="token number">2</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h6 id="rolling-back-to-a-previous-revision" tabindex="-1"><a class="header-anchor" href="#rolling-back-to-a-previous-revision"><span>Rolling Back to a Previous Revision</span></a></h6><p>decided to undo the current rollout and rollback to the previous revision:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl rollout undo deployment/nginx-deployment</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>you can rollback to a specific revision by specifying it with <code>--to-revision</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl rollout undo deployment/nginx-deployment --to-revision<span class="token operator">=</span><span class="token number">2</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h5 id="scaling-a-deployment" tabindex="-1"><a class="header-anchor" href="#scaling-a-deployment"><span>Scaling a Deployment</span></a></h5><h6 id="proportional-scaling" tabindex="-1"><a class="header-anchor" href="#proportional-scaling"><span>Proportional scaling</span></a></h6><p>比例扩容是指，当一个正在进行版本更新的 Deployment 需要扩容时，Kubernetes 不会把所有的新增 Pod 都创建成新版本，而是会按照当前新旧版本的 Pod 数量比例，来分配这些新增的 Pod。 按比例新增完以后，再升级到新的版本</p><ul><li>只有在滚动更新的特定窗口期内，比例扩容机制才会被激活和使用</li></ul><h5 id="pausing-and-resuming-a-rollout-of-a-deployment" tabindex="-1"><a class="header-anchor" href="#pausing-and-resuming-a-rollout-of-a-deployment"><span>Pausing and Resuming a rollout of a Deployment</span></a></h5><p>当一个 Deployment 的发布（rollout）被暂停（paused）后，你对其模板（template）所做的任何后续更改，例如使用 kubectl set image 更新镜像，都仅仅是更新了 Deployment 这个对象在 Kubernetes API Server 中的定义（Spec）。然而，Deployment Controller（控制器）因为收到了“暂停”指令，所以它不会触发任何实际的滚动更新操作。也就是说，它不会去创建新的 ReplicaSet，也不会用新镜像去创建新的 Pod。</p><p>只有当你执行 kubectl rollout resume 命令后，Deployment Controller 才会解除暂停状态，然后它会去比较当前的活动状态和你在暂停期间所做的全部修改后的期望状态（Desired State）</p><ul><li>这种“暂停-修改-恢复”的机制非常有用，它为我们提供了一个窗口期，让我们可以在一个发布周期内安全地应用多个变更，而不是每做一个小改动就触发一次滚动更新。</li></ul><h4 id="statefulsets" tabindex="-1"><a class="header-anchor" href="#statefulsets"><span>StatefulSets</span></a></h4><h5 id="using-statefulsets" tabindex="-1"><a class="header-anchor" href="#using-statefulsets"><span>Using StatefulSets</span></a></h5><p>StatefulSets are valuable for applications that require one or more of the following.</p><ul><li>Stable, unique network identifiers.</li><li>Stable, persistent storage.</li><li>Ordered, graceful deployment and scaling.</li><li>Ordered, automated rolling updates.</li></ul><p>In the above, stable is synonymous with persistence across Pod (re)scheduling.</p><h5 id="limitations" tabindex="-1"><a class="header-anchor" href="#limitations"><span>Limitations</span></a></h5><ul><li><p>Deleting and/or scaling a StatefulSet down will <em>not</em> delete the volumes associated with the StatefulSet.</p></li><li><p>StatefulSets currently require a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services" target="_blank" rel="noopener noreferrer">Headless Service</a> to be responsible for the network identity of the Pods. You are responsible for creating this Service.</p></li><li><p>StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.</p><blockquote><p>Kubernetes 会立即开始清理其所属的 Pods，但这个过程不保证顺序，也不保证 Pods 能优雅地关闭</p></blockquote></li><li><p>StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.</p></li></ul><h5 id="example" tabindex="-1"><a class="header-anchor" href="#example"><span>Example</span></a></h5><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Service</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx</span>
<span class="line">  <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token punctuation">-</span> <span class="token key atrule">port</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line">    <span class="token key atrule">name</span><span class="token punctuation">:</span> web</span>
<span class="line">  <span class="token key atrule">clusterIP</span><span class="token punctuation">:</span> None</span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx</span>
<span class="line"><span class="token punctuation">---</span></span>
<span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> web</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx <span class="token comment"># has to match .spec.template.metadata.labels</span></span>
<span class="line">  <span class="token key atrule">serviceName</span><span class="token punctuation">:</span> <span class="token string">&quot;nginx&quot;</span></span>
<span class="line">  <span class="token key atrule">replicas</span><span class="token punctuation">:</span> <span class="token number">3</span> <span class="token comment"># by default is 1</span></span>
<span class="line">  <span class="token key atrule">minReadySeconds</span><span class="token punctuation">:</span> <span class="token number">10</span> <span class="token comment"># by default is 0</span></span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">app</span><span class="token punctuation">:</span> nginx <span class="token comment"># has to match .spec.selector.matchLabels</span></span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">10</span></span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> nginx</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> registry.k8s.io/nginx<span class="token punctuation">-</span>slim<span class="token punctuation">:</span><span class="token number">0.24</span></span>
<span class="line">        <span class="token key atrule">ports</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">containerPort</span><span class="token punctuation">:</span> <span class="token number">80</span></span>
<span class="line">          <span class="token key atrule">name</span><span class="token punctuation">:</span> web</span>
<span class="line">        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> www</span>
<span class="line">          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /usr/share/nginx/html</span>
<span class="line">  <span class="token key atrule">volumeClaimTemplates</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token punctuation">-</span> <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> www</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">accessModes</span><span class="token punctuation">:</span> <span class="token punctuation">[</span> <span class="token string">&quot;ReadWriteOnce&quot;</span> <span class="token punctuation">]</span></span>
<span class="line">      <span class="token key atrule">storageClassName</span><span class="token punctuation">:</span> <span class="token string">&quot;my-storage-class&quot;</span></span>
<span class="line">      <span class="token key atrule">resources</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">requests</span><span class="token punctuation">:</span></span>
<span class="line">          <span class="token key atrule">storage</span><span class="token punctuation">:</span> 1Gi</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="pod-identity" tabindex="-1"><a class="header-anchor" href="#pod-identity"><span>Pod Identity</span></a></h5><p>StatefulSet Pods have a unique identity that consists of an ordinal, a stable network identity, and stable storage. The identity sticks to the Pod, regardless of which node it&#39;s (re)scheduled on.</p><h6 id="ordinal-index" tabindex="-1"><a class="header-anchor" href="#ordinal-index"><span>Ordinal Index</span></a></h6><p>For a StatefulSet with N <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#replicas" target="_blank" rel="noopener noreferrer">replicas</a>, each Pod in the StatefulSet will be assigned an integer ordinal, that is unique over the Set. By default, pods will be assigned ordinals from 0 up through N-1. The StatefulSet controller will also add a pod label with this index: <code>apps.kubernetes.io/pod-index</code>.</p><h6 id="start-ordinal" tabindex="-1"><a class="header-anchor" href="#start-ordinal"><span>Start ordinal</span></a></h6><ul><li><code>.spec.ordinals.start</code>: If the <code>.spec.ordinals.start</code> field is set, Pods will be assigned ordinals from <code>.spec.ordinals.start</code> up through <code>.spec.ordinals.start + .spec.replicas - 1</code>.</li></ul><h6 id="stable-network-id" tabindex="-1"><a class="header-anchor" href="#stable-network-id"><span>Stable Network ID</span></a></h6><p>how that affects the DNS names for the StatefulSet&#39;s Pods.</p><table><thead><tr><th>Cluster Domain</th><th>Service (ns/name)</th><th>StatefulSet (ns/name)</th><th>StatefulSet Domain</th><th>Pod DNS</th><th>Pod Hostname</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><h6 id="stable-storage" tabindex="-1"><a class="header-anchor" href="#stable-storage"><span>Stable Storage</span></a></h6><p>Pod 的调度位置不会影响 StatefulSet 创建的 PV 数量。只要 replicas: 3 并且定义了 volumeClaimTemplates，就一定会创建 3 个 PVC，进而触发创建 3 个 PV，无论这些 Pod 最终在哪里运行。</p><p>Note that, the PersistentVolumes associated with the Pods&#39; PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be done manually.</p><h6 id="pod-name-label" tabindex="-1"><a class="header-anchor" href="#pod-name-label"><span>Pod Name Label</span></a></h6><p>When the StatefulSet <a href="https://kubernetes.io/docs/concepts/architecture/controller/" target="_blank" rel="noopener noreferrer">controller</a> creates a Pod, it adds a label, <code>statefulset.kubernetes.io/pod-name</code>, that is set to the name of the Pod. This label allows you to attach a Service to a specific Pod in the StatefulSet.</p><h5 id="deployment-and-scaling-guarantees" tabindex="-1"><a class="header-anchor" href="#deployment-and-scaling-guarantees"><span>Deployment and Scaling Guarantees</span></a></h5><ul><li>For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.</li><li>When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.</li><li>Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.</li><li>Before a Pod is terminated, all of its successors must be completely shutdown.</li></ul><p>The StatefulSet should not specify a <code>pod.Spec.TerminationGracePeriodSeconds</code> of 0.</p><h5 id="pod-management-policies" tabindex="-1"><a class="header-anchor" href="#pod-management-policies"><span>Pod Management Policies</span></a></h5><p>via its <code>.spec.podManagementPolicy</code> field.</p><ul><li><code>OrderedReady</code> pod management is the default for StatefulSets.</li><li><code>Parallel</code> pod management tells the StatefulSet controller to launch or terminate all Pods in parallel</li></ul><h5 id="update-strategies" tabindex="-1"><a class="header-anchor" href="#update-strategies"><span>Update strategies</span></a></h5><p>There are two possible values for a StatefulSet&#39;s <code>.spec.updateStrategy</code> field.</p><ul><li><p><code>OnDelete</code></p><p>When a StatefulSet&#39;s <code>.spec.updateStrategy.type</code> is set to <code>OnDelete</code>, the StatefulSet controller will not automatically update the Pods in a StatefulSet. Users must manually delete Pods to cause the controller to create new Pods that reflect modifications made to a StatefulSet&#39;s <code>.spec.template</code>.</p></li><li><p><code>RollingUpdate</code></p><p>The <code>RollingUpdate</code> update strategy implements automated, rolling updates for the Pods in a StatefulSet. This is the default update strategy.</p></li></ul><h5 id="rolling-updates" tabindex="-1"><a class="header-anchor" href="#rolling-updates"><span>Rolling Updates</span></a></h5><p>When a StatefulSet&#39;s <code>.spec.updateStrategy.type</code> is set to <code>RollingUpdate</code>, the StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod termination (from the largest ordinal to the smallest), updating each Pod one at a time.</p><h6 id="partitioned-rolling-updates" tabindex="-1"><a class="header-anchor" href="#partitioned-rolling-updates"><span>Partitioned rolling updates</span></a></h6><p>The <code>RollingUpdate</code> update strategy can be partitioned, by specifying a <code>.spec.updateStrategy.rollingUpdate.partition</code>.</p><p><code>partition</code>（分区）是Kubernetes StatefulSet中<code>RollingUpdate</code>（滚动更新）策略的一个核心属性。它允许你对有状态应用（如数据库）进行<strong>部分更新</strong>或<strong>阶段性发布</strong>。</p><p>它的工作原理是充当一个“分界线”，将所有Pod副本根据其序号（ordinal，即<code>pod-0</code>, <code>pod-1</code>...）分为两个集合：</p><ol><li><strong>“更新区” (Update Zone):</strong><ul><li><strong>规则：</strong> 序号 **大于或等于 ** <code>partition</code> 值的Pod。</li><li><strong>行为：</strong> 当你更新StatefulSet的Pod模板（<code>.spec.template</code>，例如更换镜像）时，这个区域的Pod<strong>会被</strong>自动滚动更新到新版本。</li></ul></li><li><strong>“锁定区” (Locked Zone):</strong><ul><li><strong>规则：</strong> 序号 <strong>小于</strong> <code>partition</code> 值的Pod。</li><li><strong>行为：</strong> 这个区域的Pod<strong>不会</strong>被更新，它们会被“锁定”在当前（旧的）版本。</li><li><strong>关键保护机制：</strong> 即使你手动删除一个“锁定区”的Pod，Kubernetes也会<strong>使用旧版本的模板</strong>来重建它，以防止意外升级。</li></ul></li></ol><h5 id="revision-history" tabindex="-1"><a class="header-anchor" href="#revision-history"><span>Revision history</span></a></h5><p>Control retained revisions with <code>.spec.revisionHistoryLimit</code>:</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> StatefulSet</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> webapp</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">revisionHistoryLimit</span><span class="token punctuation">:</span> <span class="token number">5</span>  <span class="token comment"># Keep last 5 revisions</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can revert to a previous configuration using:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># View revision history</span></span>
<span class="line">kubectl rollout <span class="token function">history</span> statefulset/webapp</span>
<span class="line"></span>
<span class="line"><span class="token comment"># Rollback to a specific revision</span></span>
<span class="line">kubectl rollout undo statefulset/webapp --to-revision<span class="token operator">=</span><span class="token number">3</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>To view associated ControllerRevisions:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment"># List all revisions for the StatefulSet</span></span>
<span class="line">kubectl get controllerrevisions <span class="token parameter variable">-l</span> app.kubernetes.io/name<span class="token operator">=</span>webapp</span>
<span class="line"></span>
<span class="line"><span class="token comment"># View detailed configuration of a specific revision</span></span>
<span class="line">kubectl get controllerrevision/webapp-3 <span class="token parameter variable">-o</span> yaml</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="persistentvolumeclaim-retention" tabindex="-1"><a class="header-anchor" href="#persistentvolumeclaim-retention"><span>PersistentVolumeClaim retention</span></a></h5><p>The optional <code>.spec.persistentVolumeClaimRetentionPolicy</code> field controls if and how PVCs are deleted during the lifecycle of a StatefulSet.</p><p>这个策略下有两个子策略，它们控制着不同场景下的 PVC 行为：</p><ol><li><strong><code>whenDeleted</code></strong><ul><li><strong>触发时机：</strong> 当整个 StatefulSet 资源<strong>被删除</strong>时（例如，你执行了 <code>kubectl delete statefulset my-app</code>）。</li><li><strong>控制对象：</strong> <em>所有</em>由这个 StatefulSet 创建的 PVC。</li></ul></li><li><strong><code>whenScaled</code></strong><ul><li><strong>触发时机：</strong> 当 StatefulSet 的副本数（<code>replicas</code>）<strong>被调小</strong>时（即“缩容”，例如从 5 个副本缩减到 3 个副本）。</li><li><strong>控制对象：</strong> <em>仅仅</em>那些因缩容而被删除的 Pod 所对应的 PVC（在上面的例子中，就是 <code>my-app-4</code> 和 <code>my-app-3</code> 对应的 PVC）。</li></ul></li></ol><p>策略的两个选项 对于上述的每一种场景，你都有两种行为选项：</p><ol><li><strong><code>Retain</code> (默认值)</strong><ul><li><strong>含义：</strong> 保留。</li><li><strong>行为：</strong> 这就是 Kubernetes 的经典行为。即使 Pod 被删除，PVC 也会被保留在集群中。</li><li><strong>适用场景：</strong> 生产环境的数据库、关键数据存储。<strong>数据的安全性是第一位的。</strong></li></ul></li><li><strong><code>Delete</code></strong><ul><li><strong>含义：</strong> 删除。</li><li><strong>行为：</strong> 当关联的 Pod 被终止<em>之后</em>，Kubernetes 会自动删除该 PVC。</li><li><strong>适用场景：</strong><ul><li>开发/测试环境：快速清理资源，避免垃圾堆积。</li><li>数据可再生应用：例如一个分布式缓存集群，缓存数据丢失后可以重新生成。</li><li>临时数据处理：Pod 只是用 PVC 做临时落地，Pod 没了数据也就不需要了。</li></ul></li></ul></li></ol><h4 id="daemonset" tabindex="-1"><a class="header-anchor" href="#daemonset"><span>DaemonSet</span></a></h4><h5 id="writing-a-daemonset-spec" tabindex="-1"><a class="header-anchor" href="#writing-a-daemonset-spec"><span>Writing a DaemonSet Spec</span></a></h5><h6 id="create-a-daemonset" tabindex="-1"><a class="header-anchor" href="#create-a-daemonset"><span>Create a DaemonSet</span></a></h6><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch</span>
<span class="line">  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system</span>
<span class="line">  <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">k8s-app</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>logging</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">selector</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch</span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">labels</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch</span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token comment"># these tolerations are to have the daemonset runnable on control plane nodes</span></span>
<span class="line">      <span class="token comment"># remove them if your control plane nodes should not run pods</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/control<span class="token punctuation">-</span>plane</span>
<span class="line">        <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists</span>
<span class="line">        <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule</span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> node<span class="token punctuation">-</span>role.kubernetes.io/master</span>
<span class="line">        <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists</span>
<span class="line">        <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> fluentd<span class="token punctuation">-</span>elasticsearch</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> quay.io/fluentd_elasticsearch/fluentd<span class="token punctuation">:</span>v5.0.1</span>
<span class="line">        <span class="token key atrule">resources</span><span class="token punctuation">:</span></span>
<span class="line">          <span class="token key atrule">limits</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">memory</span><span class="token punctuation">:</span> 200Mi</span>
<span class="line">          <span class="token key atrule">requests</span><span class="token punctuation">:</span></span>
<span class="line">            <span class="token key atrule">cpu</span><span class="token punctuation">:</span> 100m</span>
<span class="line">            <span class="token key atrule">memory</span><span class="token punctuation">:</span> 200Mi</span>
<span class="line">        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog</span>
<span class="line">          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/log</span>
<span class="line">      <span class="token comment"># it may be desirable to set a high priority class to ensure that a DaemonSet Pod</span></span>
<span class="line">      <span class="token comment"># preempts running Pods</span></span>
<span class="line">      <span class="token comment"># priorityClassName: important</span></span>
<span class="line">      <span class="token key atrule">terminationGracePeriodSeconds</span><span class="token punctuation">:</span> <span class="token number">30</span></span>
<span class="line">      <span class="token key atrule">volumes</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> varlog</span>
<span class="line">        <span class="token key atrule">hostPath</span><span class="token punctuation">:</span></span>
<span class="line">          <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/log</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h6 id="running-pods-on-select-nodes" tabindex="-1"><a class="header-anchor" href="#running-pods-on-select-nodes"><span>Running Pods on select Nodes</span></a></h6><p>if you specify a <code>.spec.template.spec.affinity</code>, then DaemonSet controller will create Pods on nodes which match that <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/" target="_blank" rel="noopener noreferrer">node affinity</a>. If you do not specify either, then the DaemonSet controller will create Pods on all nodes.</p><h5 id="how-daemon-pods-are-scheduled" tabindex="-1"><a class="header-anchor" href="#how-daemon-pods-are-scheduled"><span>How Daemon Pods are scheduled</span></a></h5><p>A DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod. The DaemonSet controller creates a Pod for each eligible node and adds the <code>spec.affinity.nodeAffinity</code> field of the Pod to match the target host. After the Pod is created, the default scheduler typically takes over and then binds the Pod to the target host by setting the <code>.spec.nodeName</code> field. If the new Pod cannot fit on the node, the default scheduler may preempt (evict) some of the existing Pods based on the <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority" target="_blank" rel="noopener noreferrer">priority</a> of the new Pod.</p><blockquote><p>抢占行为完全是基于优先级 (Priority) 的，而优先级是通过 PriorityClass 来定义的</p></blockquote><h6 id="taints-and-tolerations" tabindex="-1"><a class="header-anchor" href="#taints-and-tolerations"><span>Taints and tolerations</span></a></h6><p>The DaemonSet controller automatically adds a set of <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/" target="_blank" rel="noopener noreferrer">tolerations</a> to DaemonSet Pods:</p><table><thead><tr><th>Toleration key</th><th>Effect</th><th>Details</th></tr></thead><tbody><tr><td><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-not-ready" target="_blank" rel="noopener noreferrer"><code>node.kubernetes.io/not-ready</code></a></td><td><code>NoExecute</code></td><td>DaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted.</td></tr><tr><td><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-unreachable" target="_blank" rel="noopener noreferrer"><code>node.kubernetes.io/unreachable</code></a></td><td><code>NoExecute</code></td><td>DaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted.</td></tr><tr><td><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-disk-pressure" target="_blank" rel="noopener noreferrer"><code>node.kubernetes.io/disk-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with disk pressure issues.</td></tr><tr><td><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-memory-pressure" target="_blank" rel="noopener noreferrer"><code>node.kubernetes.io/memory-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with memory pressure issues.</td></tr><tr><td><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-pid-pressure" target="_blank" rel="noopener noreferrer"><code>node.kubernetes.io/pid-pressure</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes with process pressure issues.</td></tr><tr><td><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-unschedulable" target="_blank" rel="noopener noreferrer"><code>node.kubernetes.io/unschedulable</code></a></td><td><code>NoSchedule</code></td><td>DaemonSet Pods can be scheduled onto nodes that are unschedulable.</td></tr><tr><td><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#node-kubernetes-io-network-unavailable" target="_blank" rel="noopener noreferrer"><code>node.kubernetes.io/network-unavailable</code></a></td><td><code>NoSchedule</code></td><td><strong>Only added for DaemonSet Pods that request host networking</strong>, i.e., Pods having <code>spec.hostNetwork: true</code>. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.</td></tr></tbody></table><p>You can add your own tolerations to the Pods of a DaemonSet as well, by defining these in the Pod template of the DaemonSet.</p><blockquote><p>📝 Taint Effect: <code>NoExecute</code> 核心笔记</p><p><code>NoExecute</code> 是 Taint（污点）三种效果中最“强力”的一种，它的核心是<strong>驱逐（Eviction）</strong>。</p><ol><li>核心功能：驱逐正在运行的 Pod</li></ol><ul><li><strong>触发条件</strong>：当一个 Node 被添加 <code>effect: NoExecute</code> 的 Taint。</li><li><strong>立即执行</strong>：K8s 会<strong>立即</strong>检查该 Node 上所有正在运行的 Pod。</li><li><strong>驱逐对象</strong>：所有<strong>没有</strong>匹配此 Taint 的 <code>toleration</code>（容忍）的 Pod，都会<strong>立刻</strong>被启动驱逐流程。</li><li><strong>新 Pod 调度</strong>：<code>NoExecute</code> 效果也包含了 <code>NoSchedule</code> 的功能，即新的 Pod 也无法调度到这个 Node 上（除非它们有匹配的 Toleration）。</li></ul><ol start="2"><li>驱逐过程：优雅终止 (Graceful)</li></ol><p>驱逐并非瞬时的强制杀死（<code>kill -9</code>）：</p><ul><li>Pod 状态变为 <code>Terminating</code>。</li><li>Kubelet 开始执行 Pod 的<strong>终止宽限期</strong>（<code>terminationGracePeriodSeconds</code>，默认 30s）。</li><li>Pod 进程收到 <code>SIGTERM</code> 信号，有机会“优雅地”关闭。</li><li>宽限期结束后，如果 Pod 仍未退出，才会被 <code>SIGKILL</code> 强制终止。</li><li>与此同时，Deployment 等控制器会在其他可用节点上创建新的替代 Pod。</li></ul><ol start="3"><li>关键配置：<code>tolerationSeconds</code></li></ol><p><code>tolerationSeconds</code> 是 <code>NoExecute</code> 容忍中一个<strong>极其重要</strong>的可选配置。</p><ul><li><strong>目的</strong>：允许 Pod &quot;临时容忍&quot;一个 <code>NoExecute</code> Taint 一段时间，而不是立即被驱逐。</li><li><strong>工作方式</strong>： <ul><li>Pod 必须<strong>有</strong>匹配的 <code>toleration</code> 才能使用此配置。</li><li>当 Node 出现 <code>NoExecute</code> Taint 时（例如 <code>node.kubernetes.io/unreachable</code>），计时开始。</li><li>Pod 会继续运行 <code>tolerationSeconds</code> 所指定的秒数。</li><li><strong>Taint 消失</strong>：如果 Taint 在倒计时结束前被移除（如 Node 恢复），Pod 会继续正常运行，不会被驱逐。</li><li><strong>Taint 持续</strong>：如果倒计时结束，Taint 仍然存在，Pod 将被启动驱逐流程。</li></ul></li><li><strong>典型用例</strong>：常用于 <code>StatefulSet</code>，防止因短暂的网络分区（Node 暂时 <code>NotReady</code>）导致 Pod 被立即驱逐，从而避免有状态应用的数据和服务中断。</li></ul></blockquote><h5 id="updating-a-daemonset" tabindex="-1"><a class="header-anchor" href="#updating-a-daemonset"><span>Updating a DaemonSet</span></a></h5><p>If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete Pods from newly not-matching nodes.</p><p>You can delete a DaemonSet. If you specify <code>--cascade=orphan</code> with <code>kubectl</code>, then the Pods will be left on the nodes. If you subsequently create a new DaemonSet with the same selector, the new DaemonSet adopts the existing Pods.</p><h4 id="jobs" tabindex="-1"><a class="header-anchor" href="#jobs"><span>Jobs</span></a></h4><h5 id="running-an-example-job" tabindex="-1"><a class="header-anchor" href="#running-an-example-job"><span>Running an example Job</span></a></h5><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Job</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> pi</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">ttlSecondsAfterFinished</span><span class="token punctuation">:</span> <span class="token number">60</span>  <span class="token comment"># 任务完成后 60 秒自动删除 Job 和 Pod</span></span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> pi</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> perl<span class="token punctuation">:</span>5.34.0</span>
<span class="line">        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;perl&quot;</span><span class="token punctuation">,</span>  <span class="token string">&quot;-Mbignum=bpi&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-wle&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;print bpi(2000)&quot;</span><span class="token punctuation">]</span></span>
<span class="line">      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never</span>
<span class="line">  <span class="token key atrule">backoffLimit</span><span class="token punctuation">:</span> <span class="token number">4</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Check on the status of the Job with <code>kubectl</code>:</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">kubectl describe job pi</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h5 id="writing-a-job-spec" tabindex="-1"><a class="header-anchor" href="#writing-a-job-spec"><span>Writing a Job spec</span></a></h5><h6 id="job-labels" tabindex="-1"><a class="header-anchor" href="#job-labels"><span>Job Labels</span></a></h6><p>Job labels will have <code>batch.kubernetes.io/</code> prefix for <code>job-name</code> and <code>controller-uid</code>.</p><h6 id="pod-template" tabindex="-1"><a class="header-anchor" href="#pod-template"><span>Pod Template</span></a></h6><p>Only a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy" target="_blank" rel="noopener noreferrer"><code>RestartPolicy</code></a> equal to <code>Never</code> or <code>OnFailure</code> is allowed.</p><h6 id="pod-selector" tabindex="-1"><a class="header-anchor" href="#pod-selector"><span>Pod selector</span></a></h6><p>默认情况下不应指定 (.spec.selector)</p><blockquote><p>如果用户决定覆盖默认逻辑并自定义 Pod 选择器，必须非常谨慎。以下风险：</p><p>• <strong>标签冲突</strong>：如果你指定的标签选择器不是唯一的，且匹配到了其他 Job 的 Pod，可能会导致非预期行为。</p><p>• <strong>错误的 Pod 管理</strong>：</p><p>◦ 属于其他 Job 的 Pod 可能会被误删。</p><p>◦ 该 Job 可能会将其他不相关的 Pod 计入自己的完成计数。</p><p>◦ 一个或多个 Job 可能会因此拒绝创建 Pod 或无法运行至完成。</p><p>• <strong>级联影响</strong>：非唯一的选择器还可能导致其他控制器（如 ReplicationController）及其 Pod 表现出不可预测的行为。</p><p>• <strong>缺乏拦截</strong>：Kubernetes <strong>不会阻止</strong>用户在指定 <code>.spec.selector</code> 时犯错</p></blockquote><h6 id="parallel-execution-for-jobs" tabindex="-1"><a class="header-anchor" href="#parallel-execution-for-jobs"><span>Parallel execution for Jobs</span></a></h6><ol><li><strong>非并行 Job (Non-parallel Job)</strong></li></ol><p>这是最简单、最常见的 Job 类型，通常用于执行一次性的运维任务。</p><ul><li><strong>核心特征</strong>： <ul><li>通常只启动 <strong>一个</strong> Pod。</li><li>只要这个 Pod <strong>成功终止</strong>（Exit Code 0），整个 Job 就视为完成。</li></ul></li><li><strong>配置方式</strong>： <ul><li><code>.spec.completions</code>：<strong>不设置</strong>（默认为 1）。</li><li><code>.spec.parallelism</code>：<strong>不设置</strong>（默认为 1）。</li></ul></li><li><strong>适用场景</strong>： <ul><li>数据库迁移脚本 (Database Migration)。</li><li>一次性的备份任务。</li></ul></li></ul><ol start="2"><li><strong>固定完成计数 (Fixed Completion Count)</strong></li></ol><p>当你有一堆任务需要处理，并且你明确知道任务的总量时使用此模式。</p><ul><li><strong>核心特征</strong>： <ul><li>需要设置一个非零的正整数作为目标。</li><li>Job Controller 会不断创建 Pod，直到累计有指定数量（<code>.spec.completions</code>）的 Pod 成功退出。</li></ul></li><li><strong>配置方式</strong>： <ul><li><code>.spec.completions</code>：设置为 <strong>N</strong> (非零正值)。</li><li><code>parallelism</code> : 设置每次允许 N 个 Pod 并行跑</li></ul></li><li><strong>完成模式 (Completion Mode)</strong> - <em>K8s v1.21+</em>： <ul><li><strong>NonIndexed (默认)</strong>： <ul><li>Pod 的完成是同质的（Homogenous）。也就是说，Pod A 完成和 Pod B 完成没有区别，只是计数器 +1。</li></ul></li><li><strong>Indexed (索引模式)</strong>： <ul><li><strong>核心概念</strong>：每个 Pod 会获得一个从 <code>0</code> 到 <code>N-1</code> 的唯一索引。</li><li><strong>获取索引方式</strong>：程序可以通过 Pod 的 Annotation、Label、Hostname 或 环境变量 (<code>JOB_COMPLETION_INDEX</code>) 获取当前处理的是第几个任务。</li><li><strong>完成条件</strong>：每个索引（0, 1, ... N-1）都必须有一个对应的 Pod 成功完成。</li><li><strong>适用场景</strong>：静态分片处理。例如，有 10 个大文件需要转码，你可以启动 10 个 Job Pod，Pod-0 处理 <code>file-0.mp4</code>，Pod-1 处理 <code>file-1.mp4</code>。</li></ul></li></ul></li></ul><ol start="3"><li>工作队列 (Work Queue)</li></ol><p>这种模式通常用于并行处理，但任务总数不固定，或者由工作队列（如 RabbitMQ, Redis）来决定何时结束。</p><ul><li><strong>核心特征</strong>： <ul><li>Pod 必须能够通过外部服务（队列）或者相互协调来判断是否还有工作要做。</li><li><strong>关键终止逻辑</strong>：一旦 <strong>任意一个 Pod 成功终止</strong>，Job Controller 就认为整个任务队列已经空了（工作完成）。</li><li>此时，Job 会立即停止创建新 Pod，并开始终止其他还在运行的 Pod。</li></ul></li><li><strong>配置方式</strong>： <ul><li><code>.spec.completions</code>：<strong>不设置</strong>。</li><li><code>.spec.parallelism</code>：设置为大于 1 的整数（启用并行）。</li></ul></li><li><strong>适用场景</strong>： <ul><li>多个 Worker 消费者从 RabbitMQ 中抢任务，当队列为空时，Worker 正常退出。</li></ul></li></ul><h6 id="controlling-parallelism" tabindex="-1"><a class="header-anchor" href="#controlling-parallelism"><span>Controlling parallelism</span></a></h6><p>The requested parallelism (<code>.spec.parallelism</code>) can be set to any non-negative value. If it is unspecified, it defaults to 1. If it is specified as 0, then the Job is effectively paused until it is increased.</p><h5 id="pod-failure-policy" tabindex="-1"><a class="header-anchor" href="#pod-failure-policy"><span>Pod failure policy</span></a></h5><p>在 Kubernetes 批处理任务（Jobs）的框架下，<strong>故障处理与终止策略</strong>是确保任务可靠性、资源利用率以及异常情况自愈的核心机制。根据提供的资料，这些策略可以从失败重试、精细化策略控制、时间限制以及完成后的自动清理四个维度进行深入探讨。</p><ol><li>基础故障处理：重试与退避机制</li></ol><p>Job 的基本职能是确保指定数量的 Pod 成功终止。当故障发生时，系统采用以下机制：</p><p>• <strong>重启策略（Restart Policy）：</strong> Job 仅支持 <code>Never</code> 或 <code>OnFailure</code>。若设为 <code>OnFailure</code>，容器失败时会在原 Pod 内重启；若为 <code>Never</code>，Pod 失败时 Job 控制器会创建新 Pod。</p><p>• <strong>回退限制（backoffLimit）：</strong> 字段 <code>.spec.backoffLimit</code> 定义了在将 Job 标记为失败前的最大重试次数，默认值为 6。</p><p>• <strong>指数退避延迟：</strong> 失败的 Pod 会以指数级的延迟（10s, 20s, 40s...）重新创建，延迟上限为 6 分钟。</p><p>• <strong>按索引退避（backoffLimitPerIndex）：</strong> 在索引 Job 中，可以为每个索引独立设置重试上限，某个索引的失败不会中断其他索引的执行。</p><ol start="2"><li>精细化失败策略（Pod Failure Policy）</li></ol><p>为了更灵活地处理不同类型的失败，Kubernetes 提供了 <code>.spec.podFailurePolicy</code>：</p><p>• <strong>基于规则的动作：</strong> 可以根据容器的<strong>退出码</strong>或 <strong>Pod 条件</strong>（如 <code>DisruptionTarget</code> 节点干扰）采取不同行动。</p><p>• <strong>可选动作：</strong></p><p>◦ <strong>FailJob：</strong> 一旦匹配（如发现软件逻辑漏洞的特定退出码），立即停止整个 Job 并终止所有运行中的 Pod。</p><p>◦ <strong>Ignore：</strong> 忽略此类失败（如因抢占导致的 Pod 终止），不计入 <code>backoffLimit</code> 计数。</p><p>◦ <strong>Count/FailIndex：</strong> 按照默认方式计数或仅标记当前索引失败。</p><ol start="3"><li>主动终止与成功判定策略</li></ol><p>除了被动等待 Pod 运行，Job 还可以主动控制任务的生命周期：</p><p>• <strong>主动截止时间（activeDeadlineSeconds）：</strong> 该字段设置 Job 的运行时间上限。它具有<strong>最高优先级</strong>，一旦超时，无论 <code>backoffLimit</code> 是否达到，系统都会终止所有 Pod 并标记 Job 为失败。</p><p>• <strong>成功策略（Success Policy）：</strong> 允许用户定义 Job 何时被视为成功。例如在模拟计算中，只要特定比例的索引成功，或者指定的“领导者”索引成功，即可宣布 Job 成功并终止剩余 Pod。</p><ol start="4"><li>终止后的状态管理与清理</li></ol><p>Job 达到终态（<code>Complete</code> 或 <code>Failed</code>）后的处理同样重要：</p><p>• <strong>延迟终端状态确认：</strong> 自 v1.31 起，Job 控制器会等待所有 Pod 彻底终止后，才添加最终的 <code>Complete</code> 或 <code>Failed</code> 状态标签。</p><p>• <strong>自动清理（TTL-after-finished）：</strong> 通过 <code>.spec.ttlSecondsAfterFinished</code> 字段，控制器会在 Job 完成后的指定秒数内执行级联删除，清理 Job 对象及其关联的 Pod，以减轻 API 服务器的压力。</p><p>• <strong>手动清理：</strong> 默认情况下，完成的 Job 及其日志会保留在 API 中供诊断使用，直到用户手动删除。</p><p>--------------------------------------------------------------------------------</p><p><strong>比喻理解：</strong> 可以将 <strong>Kubernetes Job 的故障处理</strong>想象成一场有严格规章的<strong>科学实验</strong>。<code>backoffLimit</code> 是允许实验失败重启的次数；<code>podFailurePolicy</code> 就像实验室准则，规定了如果是因为“仪器坏了（节点干扰）”就重新再做一次且不扣分，但如果是“实验逻辑错误（特定退出码）”就直接终止整个项目。而 <code>activeDeadlineSeconds</code> 则是实验室的下班铃声，铃声一响，无论实验进度如何都必须强行停止。最后，<code>TTL-after-finished</code> 就像是自动清洁机器人，在实验结束并留出足够时间让你记录数据后，自动把实验室打扫干净。</p><p>example:</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Job</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> robust<span class="token punctuation">-</span>job<span class="token punctuation">-</span>demo</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token comment"># 1. 全局重试限制 (默认是6，这里设为4)</span></span>
<span class="line">  <span class="token key atrule">backoffLimit</span><span class="token punctuation">:</span> <span class="token number">4</span></span>
<span class="line">  </span>
<span class="line">  <span class="token comment"># 2. 整个 Job 的硬性超时时间 (10分钟)</span></span>
<span class="line">  <span class="token key atrule">activeDeadlineSeconds</span><span class="token punctuation">:</span> <span class="token number">600</span></span>
<span class="line">  </span>
<span class="line">  <span class="token comment"># 3. 智能失败策略 (v1.26+ 稳定特性)</span></span>
<span class="line">  <span class="token key atrule">podFailurePolicy</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">rules</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">action</span><span class="token punctuation">:</span> Ignore             <span class="token comment"># 如果是因为 Disruption (如节点被删) 导致的失败，不计入重试次数</span></span>
<span class="line">      <span class="token key atrule">onPodConditions</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">type</span><span class="token punctuation">:</span> DisruptionTarget</span>
<span class="line">    <span class="token punctuation">-</span> <span class="token key atrule">action</span><span class="token punctuation">:</span> FailJob            <span class="token comment"># 如果容器返回 42 号错误码，直接让 Job 失败，别重试了</span></span>
<span class="line">      <span class="token key atrule">onExitCodes</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token key atrule">containerName</span><span class="token punctuation">:</span> main      <span class="token comment"># 指定容器名</span></span>
<span class="line">        <span class="token key atrule">operator</span><span class="token punctuation">:</span> In</span>
<span class="line">        <span class="token key atrule">values</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">42</span><span class="token punctuation">]</span></span>
<span class="line"></span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never       <span class="token comment"># 配合 podFailurePolicy 推荐使用 Never</span></span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">        <span class="token comment"># 模拟：随机失败，或者休眠</span></span>
<span class="line">        <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;sh&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;-c&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;echo &#39;Processing...&#39;; sleep 5; exit 1&quot;</span><span class="token punctuation">]</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="success-policy" tabindex="-1"><a class="header-anchor" href="#success-policy"><span>Success policy</span></a></h5><p>在 Kubernetes 批处理任务（Jobs）的背景下，**成功策略（Success Policy）**定义了 Job 何时可以被宣告为“执行成功”。这一机制在 v1.31 及更高版本中提供了比默认计数更精细的控制手段。</p><p>以下是根据来源对 Job 成功策略及其核心机制的详细讨论：</p><ol><li>默认的成功判定标准</li></ol><p>在没有额外配置的情况下，Job 的成功判定遵循简单的计数逻辑：</p><p>• <strong>非并行任务</strong>：一旦唯一的 Pod 成功终止，任务即宣告成功。</p><p>• <strong>固定完成计数并行任务</strong>：当成功终止的 Pod 数量达到 <strong>.spec.completions</strong> 指定的数值时，Job 才被视为完成。</p><p>• <strong>工作队列模式</strong>：只要有<strong>任何一个</strong> Pod 成功终止，且所有已启动的 Pod 都已停止，Job 就算成功。</p><p>• <strong>索引模式（Indexed Job）</strong>：默认要求从 0 到 <code>completions-1</code> 的<strong>每一个索引</strong>都至少有一个成功的 Pod。</p><ol start="2"><li>精细化成功策略 (<code>.spec.successPolicy</code>)</li></ol><p>为了应对更复杂的业务需求，Kubernetes 引入了 <code>.spec.successPolicy</code>（主要针对<strong>索引任务</strong>），允许用户在不等待所有索引成功的情况下宣告任务成功。</p><p><strong>核心应用场景：</strong></p><p>• <strong>模拟实验</strong>：在运行带有不同参数的模拟任务时，可能并不需要所有参数的计算都成功，只要得到部分结果即可视为整体作业成功。</p><p>• <strong>领导者-工作者模式</strong>：例如 MPI 或 PyTorch 框架，通常只有“领导者（Leader）”节点的成功才真正决定了整个 Job 的成败。</p><ol start="3"><li>成功策略的规则配置</li></ol><p>成功策略通过一组规则（Rules）定义，这些规则按<strong>顺序评估</strong>，一旦满足其中一条，后续规则将被忽略。规则主要包含两个维度：</p><p>• <strong>succeededIndexes</strong>**（成功索引集）**：指定必须成功的特定索引范围（如 <code>0, 2-3</code>）。</p><p>• <strong>succeededCount</strong>**（成功计数）**：指定需要成功的最小索引数量。</p><p><strong>配置组合方式：</strong></p><ol><li><strong>仅指定索引集</strong>：当该集合中的<strong>所有</strong>索引都成功时，Job 成功。</li><li><strong>仅指定计数</strong>：当成功的索引<strong>总数</strong>达到该值时，Job 成功。</li><li><strong>两者结合</strong>：当指定的索引子集中成功的数量达到 <code>succeededCount</code> 时，Job 即刻宣告成功。</li><li>优先级与终止流程</li></ol><p>• <strong>策略优先级</strong>：来源特别指出，如果 Job 同时定义了成功策略和终止策略（如 <code>.spec.backoffLimit</code> 或 <code>.spec.podFailurePolicy</code>），一旦触发了终止策略（判定为失败），系统将<strong>优先遵守终止策略</strong>并忽略成功策略。</p><p>• <strong>清理存余 Pod</strong>：一旦 Job 满足了成功策略，控制器会立即标记 Job 满足成功准则（添加 <code>SuccessCriteriaMet</code> 条件），并主动<strong>终止所有仍在运行的残余 Pod</strong>。</p><p>• <strong>状态转变</strong>：在所有 Pod 彻底终止后，Job 的最终状态才会转变为 <code>Complete</code>。</p><ol start="5"><li>状态跟踪机制</li></ol><p>在 Job 满足成功条件后，其状态会发生细微变化：</p><p>• <strong>SuccessCriteriaMet</strong>：这是触发终止流程的信号。用户可以通过该条件提前判断任务是否已经达成目标，而无需等待所有 Pod 彻底关闭。</p><p>• <strong>CompletedIndexes</strong>：无论是否设置了成功策略，系统都会在状态中记录下所有已成功的索引。</p><p>--------------------------------------------------------------------------------</p><p><strong>比喻理解：</strong> 可以将 <strong>Job 成功策略</strong>想象成一场“<strong>选拔赛</strong>”。</p><p>• <strong>默认模式</strong>：相当于“全员达标”，必须所有的运动员（索引）都通过考核，整个代表队才算合格。</p><p>• <strong>成功策略模式</strong>：则提供了更灵活的规则。比如“核心成员达标制”，只要 1 号种子选手（Leader 索引）赢了，或者预选名单里有任意 3 个人（<code>succeededCount</code>）出线，整个队伍就可以提前宣告胜利并收工回家，剩下的选拔流程（仍在运行的 Pod）会被直接取消。</p><p>example：</p><div class="language-yaml line-numbers-mode" data-highlighter="prismjs" data-ext="yml"><pre><code class="language-yaml"><span class="line"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1</span>
<span class="line"><span class="token key atrule">kind</span><span class="token punctuation">:</span> Job</span>
<span class="line"><span class="token key atrule">metadata</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">name</span><span class="token punctuation">:</span> success<span class="token punctuation">-</span>policy<span class="token punctuation">-</span>demo</span>
<span class="line"><span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">  <span class="token key atrule">completions</span><span class="token punctuation">:</span> <span class="token number">5</span></span>
<span class="line">  <span class="token key atrule">parallelism</span><span class="token punctuation">:</span> <span class="token number">5</span></span>
<span class="line">  <span class="token key atrule">completionMode</span><span class="token punctuation">:</span> Indexed  <span class="token comment"># 必须开启 Indexed 模式</span></span>
<span class="line">  </span>
<span class="line">  <span class="token comment"># 核心配置：成功策略</span></span>
<span class="line">  <span class="token key atrule">successPolicy</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">rules</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">succeededIndexes</span><span class="token punctuation">:</span> <span class="token string">&quot;0&quot;</span>  <span class="token comment"># 规则1: 索引 0 必须成功</span></span>
<span class="line">        <span class="token key atrule">succeededCount</span><span class="token punctuation">:</span> <span class="token number">1</span>      <span class="token comment"># 这里的 Count 是指命中的索引数量</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">succeededIndexes</span><span class="token punctuation">:</span> <span class="token string">&quot;1-4&quot;</span> <span class="token comment"># 规则2: 在索引 1到4 中</span></span>
<span class="line">        <span class="token key atrule">succeededCount</span><span class="token punctuation">:</span> <span class="token number">2</span>       <span class="token comment"># 只要有任意 2 个成功即可</span></span>
<span class="line"></span>
<span class="line">  <span class="token key atrule">template</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token key atrule">spec</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token key atrule">restartPolicy</span><span class="token punctuation">:</span> Never</span>
<span class="line">      <span class="token key atrule">containers</span><span class="token punctuation">:</span></span>
<span class="line">      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> worker</span>
<span class="line">        <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox</span>
<span class="line">        <span class="token comment"># 模拟逻辑：打印自己的 Index。</span></span>
<span class="line">        <span class="token comment"># $JOB_COMPLETION_INDEX 是 Indexed Job 注入的环境变量</span></span>
<span class="line">        <span class="token key atrule">command</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token punctuation">-</span> sh</span>
<span class="line">        <span class="token punctuation">-</span> <span class="token punctuation">-</span>c</span>
<span class="line">        <span class="token punctuation">-</span> <span class="token punctuation">|</span><span class="token scalar string"></span>
<span class="line">          echo &quot;我是 Worker $JOB_COMPLETION_INDEX&quot;</span>
<span class="line">          sleep 10</span>
<span class="line">          exit 0</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="job-termination-and-cleanup" tabindex="-1"><a class="header-anchor" href="#job-termination-and-cleanup"><span>Job termination and cleanup</span></a></h5><p>在 Kubernetes 批处理任务（Jobs）的框架下，<strong>生命周期管理与清理</strong>是一个从任务启动、状态监控到最终资源回收的完整闭环。根据提供的资料，这一过程不仅涉及任务的成败判定，还包括为了减轻 API 服务器压力而设计的多种自动和手动清理机制。</p><p>以下是根据来源对 Job 生命周期管理与清理的详细讨论：</p><ol><li>Job 的生命周期阶段与终端状态</li></ol><p>Job 旨在运行一次性任务直至成功完成。其生命周期包含以下关键节点：</p><p>• <strong>重试与追踪</strong>：Job 会持续重试执行 Pod，直到达到指定的成功完成次数。在此期间，控制平面使用 <code>batch.kubernetes.io/job-tracking</code> **终止器（Finalizer）**来追踪所属 Pod，确保 Pod 在被计入状态之前不会被彻底移除。</p><p>• <strong>终端状态</strong>：Job 最终会进入两个终端状态之一：**Succeeded（成功）**或 <strong>Failed（失败）</strong>。</p><p>• <strong>状态转变的延迟处理</strong>：在 Kubernetes v1.31 及更高版本中，控制器会<strong>延迟添加</strong>终端条件（<code>Complete</code> 或 <code>Failed</code>），直到该 Job 的所有 Pod 都已彻底终止。在此之前，系统会先通过 <code>SuccessCriteriaMet</code> 或 <code>FailureTarget</code> 条件来触发 Pod 的终止流程。</p><ol start="2"><li>手动清理与诊断保留</li></ol><p>默认情况下，Job 完成后其对象和 Pod <strong>不会被自动删除</strong>。</p><p>• <strong>保留目的</strong>：保留已完成的 Pod 允许用户查看标准输出日志、警告或其他诊断信息。</p><p>• <strong>手动操作</strong>：用户需要通过 <code>kubectl delete</code> 手动删除旧 Job。删除 Job 对象时，它所创建的所有 Pod 也会被一并清理。</p><ol start="3"><li>核心自动化清理机制：TTL-after-finished</li></ol><p>为了避免已完成任务在 API 服务器中无限堆积，Kubernetes 提供了 <strong>TTL（生存时间）机制</strong>。</p><p>• <strong>工作原理</strong>：通过在 Job 中指定 <strong>.spec.ttlSecondsAfterFinished</strong> 字段，设置任务完成后的保留秒数。</p><p>• <strong>级联删除</strong>：当 TTL 过期时，TTL 控制器会执行<strong>级联删除</strong>，即同时删除 Job 及其所有依赖对象（如 Pods）。</p><p>• <strong>灵活性</strong>：此字段可以随时设置：既可以在创建时指定，也可以在任务完成后手动更新，甚至可以通过 <strong>Mutating Admission Webhook</strong> 动态注入（例如根据成功或失败状态设置不同的保留时间）。</p><p>• <strong>注意事项</strong>：该机制对集群内的**时间偏差（Time Skew）**非常敏感，可能导致在错误的时间触发清理。</p><ol start="4"><li>其他生命周期控制手段</li></ol><p>• **主动截止时间 ( activeDeadlineSeconds **)：这是一种基于时间的终止策略。一旦达到设定的时间上限，Job 会终止所有运行中的 Pod 并转为失败状态。</p><p>• <strong>CronJob 管理</strong>：如果 Job 由 CronJob 管理，系统会根据 CronJob 定义的<strong>基于容量的清理策略</strong>（Capacity-based cleanup policy）自动清理历史任务。</p><p>• <strong>挂起与恢复</strong>：通过设置 <code>.spec.suspend: true</code>，可以临时停止 Job 的执行并终止其活跃 Pod，直到重新恢复（此时 <code>activeDeadlineSeconds</code> 计时器会重置）。</p><ol start="5"><li>生命周期管理的最佳实践</li></ol><p>资料建议，对于直接创建的非托管 Job（Unmanaged Jobs），<strong>强烈推荐设置 TTL 字段</strong>。因为这些 Job 默认的删除策略可能会导致 Pod 在 Job 删除后变成“孤儿（Orphan）”，虽然系统最终会进行垃圾回收，但在此之前大量堆积的 Pod 可能会导致集群性能下降甚至下线。</p><p>--------------------------------------------------------------------------------</p><p><strong>比喻理解</strong>： 可以将 <strong>Job 的生命周期管理</strong>想象成一个“自动化实验室”。</p><p>• <strong>手动清理</strong>就像是实验结束后，实验员（用户）必须亲自进入实验室打扫（<code>kubectl delete</code>），否则实验器材（Pod）和记录单（Job 对象）会一直占用空间。</p><p>• <strong>TTL 机制</strong>则是一个“自动销毁定时器”。实验一结束（完成或失败），定时器开始倒计时；一旦时间到，实验室会自动进行大扫除，把记录单和器材全部清空。</p><p>• **Finalizers（终止器）**就像是在每个器材上贴的“审计标签”，确保在实验室系统确认实验结果之前，没有任何器材会被偷偷扔掉。</p><h5 id="advanced-usage" tabindex="-1"><a class="header-anchor" href="#advanced-usage"><span>Advanced usage</span></a></h5><p>在 Kubernetes Job 的架构中，“<strong>高级用法</strong>”涵盖了从精准的调度控制到生命周期接管的一系列复杂机制，旨在满足大规模、高性能或自定义化的批处理需求。</p><p>以下是根据来源对 Job 高级用法的详细讨论：</p><ol><li>任务挂起与恢复 (Suspending a Job)</li></ol><p>用户可以通过更新 <strong>.spec.suspend</strong> 字段来控制 Job 的执行状态。</p><p>• <strong>灵活控制</strong>：Job 可以创建时即处于挂起状态（<code>true</code>），由自定义控制器决定何时启动；也可以在运行中挂起。</p><p>• <strong>资源清理与重置</strong>：挂起 Job 会导致所有未完成的 Pod 被终止（发送 SIGTERM）。当 Job 恢复（设为 <code>false</code>）时，其 <code>.status.startTime</code> 会重置，这意味着 <strong>activeDeadlineSeconds</strong> <strong>计时器也会随之停止并重置</strong>。</p><ol start="2"><li>可变调度指令 (Mutable Scheduling Directives)</li></ol><p>这是一项针对挂起 Job 的高级特性，允许在任务启动前调整其调度约束。</p><p>• <strong>适用条件</strong>：仅适用于处于挂起状态且<strong>从未被恢复运行过</strong>的 Job。</p><p>• <strong>调度优化</strong>：自定义队列控制器可以在 Job 实际启动前，更新 Pod 模板中的<strong>节点亲和性 (Node Affinity)、节点选择器 (Node Selector)、容忍度 (Tolerations)</strong> 等字段，从而引导 Pod 精确落地到目标节点。</p><ol start="3"><li>自定义 Pod 选择器 (Specifying your own Pod selector)</li></ol><p>虽然系统默认会自动生成唯一的选择器，但在特定运维场景下可以手动干预。</p><p>• <strong>接管现有 Pod</strong>：例如，当需要更新 Job 的 Pod 模板或名称，但又想保留正在运行的 Pod 时，可以使用 <code>kubectl delete --cascade=orphan</code> 删除旧 Job，并创建一个带有相同选择器的新 Job。</p><p>• <strong>配置要求</strong>：在这种情况下，必须显式设置 <strong>manualSelector: true</strong>，以告知系统该选择器不匹配系统自动生成的 UID 是预期行为。</p><ol start="4"><li>弹性索引 Job (Elastic Indexed Jobs)</li></ol><p>对于索引模式的 Job，Kubernetes 支持在线动态缩放。</p><p>• <strong>联动缩放</strong>：用户可以同时修改 <code>.spec.parallelism</code> 和 <code>.spec.completions</code>，只要保持两者相等（<strong>.spec.parallelism == .spec.completions</strong>），即可实现扩缩容。</p><p>• <strong>场景应用</strong>：这在分布式训练（如 MPI、PyTorch、Ray）等需要根据资源情况动态调整工作规模的场景中非常有用。</p><ol start="5"><li>Pod 替换策略 (Pod Replacement Policy)</li></ol><p>用户可以控制系统何时创建替代 Pod，以避免资源冗余。</p><p>• <strong>替换时机</strong>：通过设置 <strong>.spec.podReplacementPolicy: Failed</strong>，Job 控制器将等待旧 Pod 完全达到“失败”阶段后再创建新 Pod，而不是在 Pod 一进入“终止中”状态就立即替换。</p><p>• <strong>防止超量</strong>：这确保了在任何时刻，运行中的 Pod 数量都不会超过并行度限制或每个索引一个 Pod 的限制。</p><ol start="6"><li>委派管理 (Delegation to External Controller)</li></ol><p>通过 <strong>spec.managedBy</strong> 字段，用户可以禁用内置的 Job 控制器，并将该 Job 的调和逻辑完全委派给外部控制器（例如第三方批处理调度器）。</p><p>• <strong>标识符</strong>：只要该字段的值不是 <code>kubernetes.io/job-controller</code>，内置控制器就会忽略该对象。</p><p>• <strong>开发者约束</strong>：外部控制器必须遵循 Job API 规范，且<strong>不得使用</strong>内置控制器预留的追踪终止器（<code>batch.kubernetes.io/job-tracking</code>）。</p><p>--------------------------------------------------------------------------------</p><p><strong>比喻理解</strong>： 如果把普通 Job 比作一台“全自动洗衣机”（按一下就开始，洗完就停），那么这些<strong>高级用法</strong>就像是给洗衣机增加了“中途暂停（挂起）”、“洗前自动识别衣物材质并调整转速（可变调度）”以及“允许外接专业洗涤模块（委派管理）”的功能。这些功能让原本简单的批处理任务能够适应更复杂的工业级作业环境。</p><h4 id="kubernetes-job-自动清理机制" tabindex="-1"><a class="header-anchor" href="#kubernetes-job-自动清理机制"><span>Kubernetes Job 自动清理机制</span></a></h4><p>在 Kubernetes 的批处理任务（Jobs）管理中，<strong>已完成任务的自动清理</strong>是一个至关重要的维护机制。它不仅能保持集群的整洁，还能有效减轻 API 服务器的压力。</p><p>根据提供的来源，以下是对该机制的详细讨论：</p><ol><li>核心机制：TTL-after-finished 控制器</li></ol><p>Kubernetes 提供了一个 <strong>TTL-after-finished 控制器</strong>，专门用于限制已完成 Job 对象的生命周期。</p><p>• <strong>触发条件</strong>：当 Job 的状态条件变为 **<code>Complete</code>（成功）**或 **<code>Failed</code>（失败）**时，计时器开始启动。</p><p>• <strong>清理行为</strong>：一旦设定的 TTL（生存时间）过期，该 Job 就会进入可被清理的状态。控制器会执行<strong>级联删除（Cascading Removal）</strong>，这意味着 Job 对象及其关联的所有依赖对象（如生成的 Pods）将一同被删除。</p><p>• <strong>生命周期保证</strong>：即使触发了清理，Kubernetes 仍会尊重对象的生命周期保证，例如等待终结器（Finalizers）处理完成。</p><ol start="2"><li>关键配置字段：<code>.spec.ttlSecondsAfterFinished</code></li></ol><p>该机制主要通过 Job 的 <strong>.spec.ttlSecondsAfterFinished</strong> 字段进行配置：</p><p>• <strong>立即删除</strong>：如果将该字段设置为 <code>0</code>，Job 在完成后会立即符合被自动删除的条件。</p><p>• <strong>无限保留</strong>：如果该字段未设置，TTL 控制器将不会清理该 Job。</p><p>• <strong>灵活设置方式</strong>：</p><p>◦ 在创建 Job 的 **Manifest（清单）**中静态指定。</p><p>◦ 对已经运行完成的 Job 进行<strong>手动更新</strong>以启动清理。</p><p>◦ 使用**准入插件（Mutating Admission Webhook）**在 Job 创建时动态设置，或者在 Job 完成后根据其状态（成功或失败）设置不同的 TTL 值。</p><p>◦ 通过编写<strong>自定义控制器</strong>，为匹配特定选择器（Selector）的一类 Job 管理清理策略。</p><ol start="3"><li>为什么自动清理至关重要？</li></ol><p>• <strong>减轻 API 服务器压力</strong>：保留大量的已完成任务会占用 API 服务器的存储资源并影响性能。</p><p>• <strong>防止 Pod 孤儿化（Orphan Pods）</strong>：对于非 CronJob 管理的“非托管 Job（Unmanaged Jobs）”，其默认删除策略可能会导致 Pod 在 Job 删除后残留。来源强烈建议为这类 Job 设置 TTL 字段，因为大量积压的残留 Pod 可能导致集群性能下降甚至下线。</p><p>• <strong>CronJob 的替代方案</strong>：如果 Job 是由 CronJob 管理的，则通常遵循 CronJob 定义的<strong>基于容量的清理策略</strong>（Capacity-based cleanup policy）。</p><ol start="4"><li>使用限制与注意事项</li></ol><p>在实施自动清理时，需要警惕以下风险：</p><p>• <strong>时间偏差（Time Skew）风险</strong>：TTL 控制器依赖存储在 Job 对象中的时间戳。如果集群内各节点的时钟不一致，可能导致控制器在<strong>错误的时间</strong>（过早或过晚）清理 Job。</p><p>• <strong>修改过期 TTL 无效</strong>：如果在现有的 TTL 已过期后才尝试更新并延长该字段的值，Kubernetes <strong>无法保证</strong>会保留该 Job，即使 API 请求返回成功。</p><p>• <strong>诊断数据丢失</strong>：一旦 Job 被清理，其关联的 <strong>Pod 及其日志</strong>也将一并消失。因此，通常需要保留一段时间以便用户检查错误、警告或诊断输出。</p><p>--------------------------------------------------------------------------------</p><p><strong>比喻理解：</strong> 你可以将已完成的 Job 想象成一张“<strong>餐厅结账单</strong>”。默认情况下，账单会留在桌上，直到服务员（用户）手动收走。设置 <code>.spec.ttlSecondsAfterFinished</code> 就像是给账单装了一个“<strong>自动碎纸机</strong>”：它允许你在客人离开（任务完成）后的几分钟内查看消费明细（日志）；一旦倒计时结束，碎纸机就会自动把账单和桌上的餐具（Pod）全部清理干净，腾出位置给下一位客人。</p></div><!--[--><!--]--></div><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">Last Updated: </span><time class="meta-item-info" datetime="2025-12-31T08:48:58.000Z" data-allow-mismatch>12/31/25, 8:48 AM</time></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: 1308811723@qq.com">RuanCong</span><!--[-->, <!--]--><!--]--><!--[--><span class="contributor" title="email: ruancong130@gmail.com">Leite-home</span><!----><!--]--><!--]--></span></div></div></footer><!----><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-5bSHB77E.js" defer></script>
  </body>
</html>
